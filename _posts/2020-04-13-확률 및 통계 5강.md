---
title: '확률 및 통계 5강'
date: 2020-04-13
permalink: /lecture/prob_stat_05
category:
  - lecture
tags:
  - 확률 및 통계
---

## 3.2 Expectation  
$E[X]=\sum_{i}x_iP(x_i)=\int_{-\infty}^{\infty}xf_X(x)dx$

### 예시 - Poisson Distribution(포아송 분포)
Discrete Random Variable $P_K(k)={\lambda^k \over k!}e^{-\lambda}$(k = 0, 1, 2, 3, ...)

이렇게 생긴 것을 Poisson Distribution이라고 한다.

언제 쓸까?  
은행이나 고객들이 방문한다고 하자. 이 때 단위 시간(time interval) 동안 몇 명의 고객이 방문했는지를 가지고 그 횟수를 확률 변수로 정의한 것이다. 

만약 은행에 한 시간에 평균 10명의 고객이 방문하는데 직원 한 명이 한 시간동안 처리할 수 있는 고객의 수가 5명이라면 두 명의 직원이 고객 응대 일을 해야 한다.

$E[K]=\sum_{k=0}^{\infty}k{\lambda^k \over k!}e^{-\lambda}$  
$=\sum_{k=1}^{\infty}k{\lambda^k \over k!}e^{-\lambda}$  
$=\sum_{k=1}^{\infty}{\lambda^k \over (k-1)!}e^{-\lambda}$  
$=\sum_{k=1}^{\infty}{\lambda \lambda^{k-1} \over (k-1)!}e^{-\lambda}$  
$=\lambda e^{-\lambda} \sum_{k=1}^{\infty}{\lambda^{k-1} \over (k-1)!}$  
$=\lambda e^{-\lambda} \sum_{k=0}^{\infty}{\lambda^k \over k!}$  
$=\lambda$ ($\because \sum_{k=0}^{\infty}{\lambda^k \over k!}=e^\lambda$ by [taylor series](https://blanik00.github.io/posts/taylor_series))

즉, 단위 시간 동안 방문한 고객의 평균 수가 $\lambda$다. 그러므로 은행은 고객이 평균 몇 명 정도 방문하는지 경험적인 데이터로 $\lambda$를 구하고 포아송 분포로 모델링하면 된다. 예를 들어, k가 25 이상이 될 확률이 5% 미만이라면, 그 정도의 고객만 응대할 수 있도록 근무 시간표를 조절하면 된다.

### 예시 - Continuous RV
$f_X(x)=
\begin{cases}
\lambda e^{-\lambda x} \qquad x\ge 0 \\\
0 \qquad \qquad x<0
\end{cases}$

이러한 것을 Exponential Distribution이라고 한다. 이 분포는 시스템의 Lifetime과 관련이 많다. 어느 시스템이 어느 순간에서도 잘 동작하게 될 확률과 관련된 것이다. 시간이 오래될수록 생존 확률이 낮아진다. 즉, 지수 분포로 시스템을 설계하게 되면 "이 시스템은 95%의 Reliability를 가지고 3년 동안은 잘 동작할 것이다"와 같은 분석을 할 수 있게 된다.

$E[X]=\int_{-\infty}^{\infty}xf_X(x)dx$  
$=\int_{0}^{\infty}x\lambda e^{-\lambda x}dx$  
$={1 \over \lambda}$  

**참고**  
예시 3.3, 3.4에서 본 것들은 $\lambda$를 알면 확률분포를 알 수 있다. 이런 것을 Parametric probability density estimation이라고 한다. 앞으로 배울 분포들은 파라미터를 알면 그 분포를 알 수 있는 것과 알 수 없는 것으로 나뉜다.

## 3.4 Moments of RV
- nth-order Moments  
  $E[X^n]=\sum_{i}x_i^nP(x_i)=\int_{-\infty}^{\infty}x^nf_X(x)dx$  
  $n = 1$이라면 $E[X]$는 mean($\mu$)과 같다.  
- Central Moments  
  $E[(X-\mu)^n]=\sum_{i}(x_i-\mu)^nP(x_i)=\int_{-\infty}^{\infty}(x-\mu)^nf_X(x)dx$  
  - $n = 1$  
    $E[(X-\mu)]=\int_{-\infty}^{\infty}(x-\mu)f_X(x)dx=\int_{-\infty}^{\infty}xf_X(x)dx+\mu\int_{-\infty}^{\infty}f_X(x)dx=\mu-\mu\times 1=0$  
    평균을 중심으로 각각의 RV의 차이값을 평균내면 당연히 0이 나온다. 이 예시는 Continous한 경우지만, Discrete한 경우에도 적용된다.
  - $n = 2$  
    $E[(X-\lambda)^2]$. 이는 분산이라고 한다.($=\sigma_X^2$) 우리가 흔히 아는 표준편차의 제곱이다. 분산은 0 이상의 값을 가지며, 모든 값이 같을 때 0의 분산을 가진다.

**예시**  
RV X를 가지고 새로운 함수 $g_1(X), g_2(X)$를 만들었다고 하자. 그렇다면 $E[ag_1(X)+bg_2(X)]$는 어떻게 구할까?

$E[ag_1(X)+bg_2(X)]$  
$=\int (ag_1(x)+bg_2(x))f_X(x)dx$  
$=a\int g_1(x)f_X(x)dx+b\int g_2(x)f_X(x)dx$  
$=aE[g_1(x)]+bE[g_2(x)]$  

이런 것을 선형성(Linearity)을 가지고 있다고 한다.  

아래는 Expectation의 선형성을 활용해 풀 수 있다.  
$\sigma_X^{2}=E[(X-\mu)^2]$  
$=E[X^2-2\mu X+\mu^2]$  
$=E[X^2]-2\mu E[X]+\mu^2$  
$=E[X^2]-\mu^2\quad(\because E[X]=\mu)$  

**예시 - 포아송 분포의 분산**  
$P_K(k)={\lambda^k \over k!}e^{-\lambda}$  
$E[X]=\lambda$  

$E[K^2]=\sum_{k=0}^{\infty}k^2{\lambda^k \over k!}e^{-\lambda}$  
$=e^{-\lambda}\sum_{k=1}^{\infty}k^2{\lambda^k \over k!}$  
$=e^{-\lambda}\sum_{k=1}^{\infty}{k\lambda^k \over (k-1)!}$  
$=e^{-\lambda}\sum_{k=1}^{\infty}{(k-1+1)\lambda^k \over (k-1)!}$  
$=e^{-\lambda}\sum_{k=1}^{\infty}{(k-1)\lambda^k \over (k-1)!}+e^{-\lambda}\sum_{k=1}^{\infty}{1\lambda^k \over (k-1)!}$  
$=e^{-\lambda}\sum_{k=2}^{\infty}{\lambda^k \over (k-2)!}+e^{-\lambda}\sum_{k=1}^{\infty}{\lambda^k \over (k-1)!}$  
$=e^{-\lambda}\sum_{k=2}^{\infty}{\lambda^2 \lambda^{k-2} \over (k-2)!}+e^{-\lambda}\sum_{k=1}^{\infty}{\lambda \lambda^{k-1} \over (k-1)!}$  
$=\lambda^2 e^{-\lambda}\sum_{k=2}^{\infty}{ \lambda^{k-2} \over (k-2)!}+\lambda e^{-\lambda}\sum_{k=1}^{\infty}{\lambda^{k-1} \over (k-1)!}$  
$=\lambda^2 e^{-\lambda}\sum_{k=0}^{\infty}{ \lambda^{k} \over k!}+\lambda e^{-\lambda}\sum_{k=0}^{\infty}{\lambda^{k} \over k!}$  
$=\lambda^2+\lambda$  ($\because \sum_{k=0}^{\infty}{\lambda^k \over k!}=e^\lambda$ by [taylor series](https://blanik00.github.io/posts/taylor_series))  

$\sigma_K^2=E[K^2]-\mu^2=\mu$

즉, 평균과 분산이 $\mu$로 같다.