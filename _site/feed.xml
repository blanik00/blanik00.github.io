<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-30T09:09:48-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JeongUk Jang</title><subtitle>Github Pages template for academic personal websites, forked from mmistakes/minimal-mistakes</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><entry><title type="html">함수(Function)</title><link href="http://localhost:4000/posts/2020/07/31/function" rel="alternate" type="text/html" title="함수(Function)" /><published>2020-07-31T00:00:00-07:00</published><updated>2020-07-31T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/31/function</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/31/function">&lt;h2 id=&quot;함수fxrightarrow-y&quot;&gt;함수($f:X\rightarrow Y$)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;대응 : $X$의 원소 $x$와 $Y$의 원소 $y$를 짝지어주는 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;집합 : 집합 $X$의 원소가 집합 $Y$의 원소가 &lt;strong&gt;하나씩&lt;/strong&gt; 대응되는 관계
    &lt;ul&gt;
      &lt;li&gt;함수인지 아닌지 판단하려면 $X$를 기준으로 생각하면 된다.&lt;/li&gt;
      &lt;li&gt;집합 $X$에서 대응되지 않는 원소가 있다면 함수가 아님&lt;/li&gt;
      &lt;li&gt;집합 $X$에서 두 개 이상에 대응되는 원소가 있다면 함수가 아님&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$X$ : 정의역&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y$ : 공역&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;치역 : 공역 중 실제로 $X$의 원소와 대응되는 것
    &lt;ul&gt;
      &lt;li&gt;치역 $\subset$ 공역&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;함숫값
    &lt;ul&gt;
      &lt;li&gt;$f(1)=a$&lt;/li&gt;
      &lt;li&gt;$f(2)=b$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;서로 같은 함수($f=g$)가 되기 위한 조건
    &lt;ul&gt;
      &lt;li&gt;정의역은 정의역끼리, 공역은 공역끼리 같아야 한다.&lt;/li&gt;
      &lt;li&gt;모든 $x\in X, f(x)=g(x)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;합성함수&quot;&gt;합성함수&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$(g\circ f)(x)=g(f(x))$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;교환법칙이 성립하지 않는다.
    &lt;ul&gt;
      &lt;li&gt;$g\circ f\neq f\circ g$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결합법칙이 성립한다.
    &lt;ul&gt;
      &lt;li&gt;$f\circ(g\circ h)=(f\circ g)\circ h$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;여러가지-함수&quot;&gt;여러가지 함수&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;일대일 함수 : $x_1\neq x_2$이면, $f(x_1)\neq f(x_2)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;일대일 대응&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;치역 = 공역&lt;/li&gt;
      &lt;li&gt;일대일 함수&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;일대일 대응은 &lt;strong&gt;역함수가 존재할 조건&lt;/strong&gt;이다.&lt;/p&gt;

    &lt;p&gt;예를 들어 보자.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x)=
  \begin{cases}
  mx+n &amp; (x&lt; a)\\
  m'x+n' &amp; (x\ge a) 
  \end{cases} %]]&gt;&lt;/script&gt;

    &lt;p&gt;1번 조건을 만족하기 위해 $mx+n=m’x+n’$을 만족해야 한다.(함수가 끊어지지 않고 연결되기 위해)&lt;/p&gt;

    &lt;p&gt;2번 조건을 만족하기 위해 $mm’&amp;gt;0$이어야 한다.(증가함수 혹은 감소함수가 되기 위해)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;상수 함수 : $f(x)=c$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;항등 함수(I)&lt;/strong&gt; : $f(x)=x$
    &lt;ul&gt;
      &lt;li&gt;$f(f(x))=x$라면 $f(x)=?$
        &lt;ul&gt;
          &lt;li&gt;$f(x)=ax+b$로 두자.&lt;/li&gt;
          &lt;li&gt;$f(f(x))=f(ax+b)=a(ax+b)+b=x$&lt;/li&gt;
          &lt;li&gt;$a^2x+ab+b=x$&lt;/li&gt;
          &lt;li&gt;$\therefore a=\pm1$&lt;/li&gt;
          &lt;li&gt;$if\;\;a=1 \rightarrow b=0$&lt;/li&gt;
          &lt;li&gt;$if\;\;a=-1\rightarrow b=any$&lt;/li&gt;
          &lt;li&gt;즉, $f(x)=x$ 또는 $f(x)=-x+k$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;역함수&quot;&gt;&lt;strong&gt;역함수&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$f:X\rightarrow Y$가 일대일 대응이어야 역함수 존재&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$f^{-1}Y\rightarrow X$&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;$x=f^{-1}(y)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;역함수의--성질&quot;&gt;역함수의  성질&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$f^{-1}(f(x))=f^{-1}(y)=x$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$f(f^{-1}(y))=f(x)=y$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$f^{-1}\circ f=f\circ f^{-1}=I$
    &lt;ul&gt;
      &lt;li&gt;즉, 일반적인 함수는 교환법칙이 성립하지 않지만 역함수와의 합성함수는 교환법칙이 성립&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$(f^{-1})^{-1}=f$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$(g\circ f)^{-1}=f^{-1}\circ g^{-1}$
    &lt;ul&gt;
      &lt;li&gt;$(g\circ f)(f^{-1}\circ g^{-1})=g\circ g^{-1}=I$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;역함수-구하기&quot;&gt;역함수 구하기&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$y=f(x)$를 $x=$y에 관한 식으로 바꾼다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$x$ 대신 $y$, $y$ 대신 $x$를 대입한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;역함수의-그래프&quot;&gt;&lt;strong&gt;역함수의 그래프&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;$y=f(x)$의 그래프와 $y=f^{-1}(x)$의 그래프는 직선 $y=x$에 대하여 대칭&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;주어진 구간에서 &lt;strong&gt;증가하는 함수&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;$y=f(x)$의 그래프와 $y=f^{-1}(x)$의 교점은 &lt;strong&gt;$y=x$ 위에 존재&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;그렇기 때문에 $f(x)=f^{-1}(x)$의 실근을 구하라는 문제는 다음 문제와도 같다.
        &lt;ol&gt;
          &lt;li&gt;$f(x)=x$의 실근을 구하라&lt;/li&gt;
          &lt;li&gt;$f^{-1}(x)=x$의 실근을 구하라&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;주어진 구간에서 &lt;strong&gt;감소하는 함수&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;$y=f(x)$의 그래프와 $y=f^{-1}(x)$의 교점은 &lt;strong&gt;$y=x$ 또는 $y=-x+k$ 위에 존재&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;그렇기 때문에 $f(x)=f^{-1}(x)$의 실근을 구하라는 문제는 다음 문제와도 같다.
        &lt;ol&gt;
          &lt;li&gt;$f(x)=x$ 또는 $f(x)=-x+k$의 실근을 구하라&lt;/li&gt;
          &lt;li&gt;$f^{-1}(x)=x$ 또는 $f^{-1}(x)=-x+k$의 실근을 구하라&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;아래 그림은 $f(x)=-x^2+1$, $f^{-1}(x)=\sqrt{-x+1}$의 교점을 나타낸다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88946310-0c4fbb00-d2ca-11ea-967a-6b5efde36285.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">함수($f:X\rightarrow Y$) 대응 : $X$의 원소 $x$와 $Y$의 원소 $y$를 짝지어주는 것</summary></entry><entry><title type="html">MLE, MAP</title><link href="http://localhost:4000/posts/2020/07/30/mle_map" rel="alternate" type="text/html" title="MLE, MAP" /><published>2020-07-30T00:00:00-07:00</published><updated>2020-07-30T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/30/mle_map</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/30/mle_map">&lt;p&gt;MLE와 MAP는 모두 “데이터셋 $X$가 주어졌을 때 $Y$일 확률, 즉, $P(Y|X)$를 구하는 것”을 목표로 한다.&lt;/p&gt;

&lt;p&gt;이런 목표를 달성하기 위해 보통 베이즈 정리를 사용한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underbrace{P(Y|X)}_{\rm posterior} = \frac{\overbrace{P(X|Y)}^{\rm likelihood} \overbrace{P(Y)}^{\rm prior}}{\underbrace{P(X)}_\text{probability of seeing the data}}&lt;/script&gt;

&lt;p&gt;MAP는 위 수식을 충실히 수행한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y|X)\propto P(X|Y)P(Y)&lt;/script&gt;

&lt;p&gt;반면, MLE는 &lt;strong&gt;uniform prior&lt;/strong&gt;를 가정한다. 따라서 위 수식을 아래와 같이 줄일 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y|X)\propto P(X|Y)&lt;/script&gt;

&lt;p&gt;따라서 MLE는 uniform prior를 가정한 MAP의 특수한 케이스다.&lt;/p&gt;

&lt;p&gt;예를 들어보자.&lt;/p&gt;

&lt;p&gt;어떤 마을에서 바닥에 떨어진 머리카락의 길이를 보고 그 머리카락이 남자의 것인지 여자의 것인지 분류하는 문제를 푼다고 하자.&lt;/p&gt;

&lt;p&gt;이 경우, $X$는 머리카락의 길이, $Y$는 성별이다.&lt;/p&gt;

&lt;p&gt;MAP의 경우 likelihood 뿐만 아니라 마을의 남녀 성비도 고려해 분류를 수행한다.&lt;/p&gt;

&lt;p&gt;반면, MLE의 경우 남녀의 성비가 동일하다고 가정하고(uniform prior), likelihood만을 고려해 분류를 수행한다.&lt;/p&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://darkpgmr.tistory.com/62&quot;&gt;베이즈 정리, ML과 MAP, 그리고 영상처리&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.bdhammel.com/mle-map/&quot;&gt;Maximum Likelihood Estimation and Maximum A Posterior Estimation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">MLE와 MAP는 모두 “데이터셋 $X$가 주어졌을 때 $Y$일 확률, 즉, $P(Y|X)$를 구하는 것”을 목표로 한다.</summary></entry><entry><title type="html">명제(Proposition)</title><link href="http://localhost:4000/posts/2020/07/30/proposition" rel="alternate" type="text/html" title="명제(Proposition)" /><published>2020-07-30T00:00:00-07:00</published><updated>2020-07-30T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/30/proposition</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/30/proposition">&lt;h2 id=&quot;명제와-조건&quot;&gt;명제와 조건&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;명제 : 참, 거짓을 판별할 수 있는 문장 또는 식
    &lt;ul&gt;
      &lt;li&gt;2는 홀수이다. (거짓인 명제)&lt;/li&gt;
      &lt;li&gt;3은 홀수이다. (참인 명제)&lt;/li&gt;
      &lt;li&gt;나는 잘생겼다. (명제 아님)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;조건($p$) : 미지수를 포함하는 문장 또는 식이 미지수의 값에 따라 참, 거짓이 결정되는 것
    &lt;ul&gt;
      &lt;li&gt;$x^2=4$의 경우, $x=2$일 때는 참이고, $x=3$일 때는 거짓임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;진리집합($P$) : 조건 $p$를 참이 되게 하는 모든 원소들의 집합&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;명제와-조건의-부정&quot;&gt;명제와 조건의 부정&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$\sim p$ : $p$가 아니다.
    &lt;ul&gt;
      &lt;li&gt;집합으로 나타내면 $p^c$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\sim(\sim p)=p$
    &lt;ul&gt;
      &lt;li&gt;집합으로 나타내면 $(p^c)^c$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아래 목록에 있는 것들은 서로 부정이다.
    &lt;ul&gt;
      &lt;li&gt;$and\Leftrightarrow or$&lt;/li&gt;
      &lt;li&gt;$&amp;gt;\Leftrightarrow \le$&lt;/li&gt;
      &lt;li&gt;$=\Leftrightarrow \neq$&lt;/li&gt;
      &lt;li&gt;$a\le x\le b\Leftrightarrow x &amp;lt; a \;or\; x &amp;gt; b$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prightarrow-q의-참-거짓&quot;&gt;$p\rightarrow q$의 참, 거짓&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;조건 두 개를 합치면 명제가 된다.($p\rightarrow q$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P\subset Q$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$x=2$이면 $x^2=4$이다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;이 명제가 참이 되기 위해서는 $p$의 진리집합이 $q$의 진리집합의 부분집합이어야 한다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;$p$의 진리집합은 $\{2\}$이고, $q$의 진리집합은 $\{2,-2\}$이다.&lt;/li&gt;
      &lt;li&gt;그러므로 이 명제는 참인 명제이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$p$를 “가정”이라고 하고, $q$를 “결론”이라고 한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;필요조건-충분조건&quot;&gt;필요조건, 충분조건&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“$p$이면 $q$이다.”가 참인 명제라면 화살표 두 개를 사용해서 $p\Rightarrow q$라고 표현한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P\subset Q$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이 때 $p$를 &lt;strong&gt;충분조건&lt;/strong&gt;, $q$를 &lt;strong&gt;필요조건&lt;/strong&gt;이라고 한다.
    &lt;ul&gt;
      &lt;li&gt;“$p$는 조건이 충분해서 조건이 필요한 $q$에게 나눠준다.”라고 외우기&lt;/li&gt;
      &lt;li&gt;예를 들어, “6의 배수이면 2의 배수이다.”라는 명제를 생각해보자. 6의 배수는 “2의 배수 &amp;amp; 3의 배수”라는 두 조건을 만족시켜야 하며, 2의 배수는 “2의 배수”만 만족시키면 된다. 6의 배수의 조건이 더 까다로우므로(충분하므로) 6의 배수가 충분조건이 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$p$의 진리집합은 $q$의 진리집합의 부분집합이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;필요충분조건
    &lt;ul&gt;
      &lt;li&gt;$p\Leftrightarrow q$&lt;/li&gt;
      &lt;li&gt;$P=Q$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;명제는 집합으로 포함할 수 있다. 그래서 $p\rightarrow q$라는 명제를 아래와 같은 집합으로 표현하기도 한다.
    &lt;ol&gt;
      &lt;li&gt;$A\cap B=A$&lt;/li&gt;
      &lt;li&gt;$A\cup B=B$&lt;/li&gt;
      &lt;li&gt;$A-B=\emptyset$&lt;/li&gt;
      &lt;li&gt;$A\cup B^c=\emptyset$&lt;/li&gt;
      &lt;li&gt;$A^c\supset B^c$&lt;/li&gt;
      &lt;li&gt;$A^c\cup B=U$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;모든-또는-어떤을-포함한-명제&quot;&gt;‘모든’ 또는 ‘어떤’을 포함한 명제&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;모든&lt;/strong&gt; $x$에 대하여 $p$이다.
    &lt;ul&gt;
      &lt;li&gt;$p$의 진리집합과 모든 $x$, 즉 $U$가 같으면 참이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;어떤&lt;/strong&gt; $x$에 대하여 $p$이다.
    &lt;ul&gt;
      &lt;li&gt;$p$의 진리집합이 하나라도 있으면 참이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;‘모든’과 ‘어떤’은 서로 부정이다.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;역과-대우&quot;&gt;역과 대우&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;역 : 가정과 결론을 바꾸는 것
    &lt;ul&gt;
      &lt;li&gt;$p\rightarrow q$의 역은 $q\rightarrow p$이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;대우 : 가정과 결론을 바꾸고 부정함
    &lt;ul&gt;
      &lt;li&gt;$p\rightarrow q$의 대우은 $\sim q\rightarrow \sim p$이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://w.namu.la/s/eaf398891d96c37b17685e9112c1cc79ce6e7362e1286f4b585c8be7c3cab2980ba072f3968cae89113d109dcafd059ea3946187571381e96a1c6874bf260d755029afd0013862b07731df955615d102a53b61daadbad33086db22fdd3f8780b&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;빨간 선은 역, 파란 선은 대우&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실제로 중요한 것은 &lt;strong&gt;대우&lt;/strong&gt; 관계이다.
    &lt;ul&gt;
      &lt;li&gt;$p\rightarrow q$가 참이면 $P\subset Q$가 성립한다. 드모르간의 법칙에 의해 $Q^c\subset P^c$도 성립하며, 이는 $p\rightarrow q$의 대우 명제인 $\sim q\rightarrow \sim p$도 성립함을 나타낸다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;명제와 대우의 참, 거짓은 일치한다.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;삼단논법&quot;&gt;삼단논법&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$p\Rightarrow q, q \Rightarrow r$이면 $p\Rightarrow r$이다.
    &lt;ul&gt;
      &lt;li&gt;$P\subset Q, Q\subset R$이면 $P\subset R$이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">명제와 조건 명제 : 참, 거짓을 판별할 수 있는 문장 또는 식 2는 홀수이다. (거짓인 명제) 3은 홀수이다. (참인 명제) 나는 잘생겼다. (명제 아님)</summary></entry><entry><title type="html">집합(Set)</title><link href="http://localhost:4000/posts/2020/07/29/set" rel="alternate" type="text/html" title="집합(Set)" /><published>2020-07-29T00:00:00-07:00</published><updated>2020-07-29T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/29/set</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/29/set">&lt;h2 id=&quot;1-정의&quot;&gt;1. 정의&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;기준에 대해서 명화가게 구분할 수 있는 대상들의 모임
    &lt;ul&gt;
      &lt;li&gt;ex. 짝수의 집합, 홀수의 집합&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;대문자로 나타냄
    &lt;ul&gt;
      &lt;li&gt;ex. $A$, $B$, …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;대상 하나하나를 원소라고 함
    &lt;ul&gt;
      &lt;li&gt;소문자로 나타냄&lt;/li&gt;
      &lt;li&gt;ex. $a\in A$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-부분집합&quot;&gt;2. 부분집합&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$A\subset B$ : $A$의 모든 원소가 $B$에 포함되어 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$A=B$인 경우도 $A\subset B$이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\emptyset$은 모든 집합의 부분집합이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$A\subset B, B\subset C \Rightarrow A\subset C$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;“$A\subset B$”를 다음과 같이 표현할 수도 있다.
    &lt;ol&gt;
      &lt;li&gt;$A\cap B=A$&lt;/li&gt;
      &lt;li&gt;$A\cup B=B$&lt;/li&gt;
      &lt;li&gt;$A-B=\emptyset$&lt;/li&gt;
      &lt;li&gt;$A\cup B^c=\emptyset\;\;$(3번과 같은 표현)&lt;/li&gt;
      &lt;li&gt;$A^c\supset B^c$&lt;/li&gt;
      &lt;li&gt;$A^c\cup B=U$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$A=B\Leftrightarrow A\subset B, B\subset A$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$B$의 진부분집합 $A$ : $A\subset B \And A\neq B$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-부분집합의-갯수nan&quot;&gt;3. 부분집합의 갯수($n(A)=n$)&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;부분집합 : $2^n$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;진부분집합 : $2^n-1$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$k$개의 특정 원소를 포함 / k개의 특정 원소를 포함하지 않음 : $2^{n-k}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$k$개의 특정 원소 중 적어도 한 개를 포함 / $k$개의 특정 원소 중 아무것도 포함하지 않음 : $2^n-2^{n-k}$&lt;br /&gt;
 ex. $A=\{1,2,3\}$&lt;br /&gt;
 1, 2 중 적어도 한 개를 포함해야 한다고 하자. 가능한 경우는 다음과 같다.&lt;br /&gt;
 $\{1\},\{2\},\{1,2\}$&lt;br /&gt;
 $\{1,3\},\{2,3\},\{1,2,3\}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4-집합의-연산&quot;&gt;4. 집합의 연산&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$A\cup B=\{x|x\in A\;\;or\;\;x\in B \}$
    &lt;ul&gt;
      &lt;li&gt;$A\cup B=A$&lt;/li&gt;
      &lt;li&gt;$A\cup\emptyset=A$&lt;/li&gt;
      &lt;li&gt;$A\cup U=U$&lt;/li&gt;
      &lt;li&gt;$A\cup A^c=U$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$A\cap B=\{x|x\in A\;\;and\;\;x\in B \}$
    &lt;ul&gt;
      &lt;li&gt;$A\cap A=A$&lt;/li&gt;
      &lt;li&gt;$A\cap\emptyset=\emptyset$&lt;/li&gt;
      &lt;li&gt;$A\cap U=A$&lt;/li&gt;
      &lt;li&gt;$A\cap A^c=\emptyset$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$A-B=\{x|x\in A\;\;and\;\;x\notin B \}$
    &lt;ul&gt;
      &lt;li&gt;$A-B$의 다른 표현
        &lt;ul&gt;
          &lt;li&gt;$A\cap B^c$&lt;/li&gt;
          &lt;li&gt;$A-(A\cap B)$&lt;/li&gt;
          &lt;li&gt;$(A\cup B)-B$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$A^c=\{x|x\in U\;\;and\;\;x\notin A \}$
    &lt;ul&gt;
      &lt;li&gt;$A^c=U-A$&lt;/li&gt;
      &lt;li&gt;$U^c=\emptyset$&lt;/li&gt;
      &lt;li&gt;$\emptyset^c=U$&lt;/li&gt;
      &lt;li&gt;$(A^c)^c=A$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;서로소 : $A\cap B=\emptyset$
    &lt;ul&gt;
      &lt;li&gt;서로소의 다른 표현
        &lt;ul&gt;
          &lt;li&gt;$A-B=A$&lt;/li&gt;
          &lt;li&gt;$B-A=B$&lt;/li&gt;
          &lt;li&gt;$A\subset B^c$&lt;/li&gt;
          &lt;li&gt;$B\subset A^c$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-교환-결합-분배-법칙&quot;&gt;5. 교환, 결합, 분배 법칙&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;교환 법칙
    &lt;ul&gt;
      &lt;li&gt;$A\cap B=B\cap A$&lt;/li&gt;
      &lt;li&gt;$A\cup B=B\cup A$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결합 법칙 : 같은 연산에 대해 괄호의 위치가 바뀌어도 성립
    &lt;ul&gt;
      &lt;li&gt;$A\cap(B\cap C)=(A\cap B)\cap C$&lt;/li&gt;
      &lt;li&gt;$A\cup(B\cup C)=(A\cup B)\cup C$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;분배 법칙 : 서로 다른 연산에 적용. 역연산도 할줄 알아야 한다.
    &lt;ul&gt;
      &lt;li&gt;$A\cap(B\cup C)=(A\cap B)\cup (A\cap C)$&lt;/li&gt;
      &lt;li&gt;$A\cup(B\cap C)=(A\cup B)\cap (A\cup C)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;드모르간 법칙
    &lt;ul&gt;
      &lt;li&gt;$(A\cap B)^c=A^c\cup B^c$&lt;/li&gt;
      &lt;li&gt;$(A\cup B)^c=A^c\cap B^c$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-집합의-원소의-갯수&quot;&gt;6. 집합의 원소의 갯수&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A\cup B)=n(A)+n(B)-n(A\cap B)$
  $n(A\cap B)=\emptyset \Rightarrow n(A\cup B)=n(A)+n(B)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A^c)=n(U)-n(A)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A-B)=n(A)-n(A\cap B)$&lt;br /&gt;
  $\qquad\qquad=n(A\cup B)-n(B)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A^c\cap B^c)=n(U)-n(A\cup B)\;\;$(드모르간 법칙 활용)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A^c\cup B^c)=n(U)-n(A\cap B)\;\;$(드모르간 법칙 활용)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A\cup B)\le n(U)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A\cap B)\le n(A) \le n(A\cup B)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n(A\cap B)\le n(B) \le n(A\cup B)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;** 마지막 세 가지는 부등호가 들어있어 최대 / 최소 문제에 많이 나옴&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">1. 정의 기준에 대해서 명화가게 구분할 수 있는 대상들의 모임 ex. 짝수의 집합, 홀수의 집합</summary></entry><entry><title type="html">03 Naive Bayes Classifier</title><link href="http://localhost:4000/posts/2020/07/28/03_naive_bayes_classifier" rel="alternate" type="text/html" title="03 Naive Bayes Classifier" /><published>2020-07-28T00:00:00-07:00</published><updated>2020-07-28T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/28/03_naive_bayes_classifier</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/28/03_naive_bayes_classifier">&lt;h1 id=&quot;optimal-classification&quot;&gt;Optimal Classification&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88505879-a4704a80-d013-11ea-99f7-ccfb65aec413.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$X=x_1, x_2, …$이 주어지고, $x_i$이 $y_1$으로 분류될 확률이 $P(Y=y_1|X=x_i)$(초록색 선)이고, $y_2$으로 분류될 확률이 $P(Y=y_2|X=x_i)$(빨간색 선)이다.&lt;/p&gt;

&lt;p&gt;$x_i$가 그림의 왼쪽에 위치해 있다고 가정하자. 그렇다면 $P(Y=y_1|X=x_i)&amp;gt;P(Y=y_2|X=x_i)$이므로 $x_i$는 $y_1$으로 분류된다.&lt;/p&gt;

&lt;p&gt;마찬가지로 $x_i$가 그림의 오른쪽에 위치해 있다고 가정하자. 그렇다면 $P(Y=y_1|X=x_i)&amp;lt;P(Y=y_2|X=x_i)$이므로 $x_i$는 $y_2$으로 분류된다.&lt;/p&gt;

&lt;p&gt;이처럼 분류를 하기 위해서는 데이터($X$)가 주어졌을 때 target class에 대한 확률을 계산해야 한다.&lt;/p&gt;

&lt;h1 id=&quot;bayes-classifier&quot;&gt;Bayes Classifier&lt;/h1&gt;
&lt;p&gt;bayes classifier는 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^*=argmin_fP(f(X)\neq Y)&lt;/script&gt;

&lt;p&gt;즉, 예측치와 관측치가 같지 않을 확률을 최소화하는 $f$를 $f^*$라고 한다.&lt;/p&gt;

&lt;p&gt;두 개의 클래스를 분류하는 문제에서는 다음과 같이 표현할 수도 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^*(x)=argmax_{Y=y}P(Y=y|X=x)&lt;/script&gt;

&lt;p&gt;$x_1$의 클래스가 $y_1$이라고 한다면 위 수식을 통해 $y_1$으로 분류될 확률을 최대화할 수 있다. 즉, $y_2$로 오분류될 확률을 최소화할 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;optimal-classification-and-bayes-risk&quot;&gt;Optimal Classification and Bayes Risk&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88509759-b99da700-d01c-11ea-8249-0a5d434b0f8b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에는 점선 classifier, 실선 classifier가 있다.&lt;/p&gt;

&lt;p&gt;$x_i$에서 점선 classifier의 두 확률값을 더하면 1이 된다. $x_1$이라는 입력값이 들어왔을 때, 이것을 빨간색 혹은 초록색으로 만들어줘야 하기 때문이다. 같은 논리도 $x_i$에서 실선 classifier의 두 확률값을 더해도 1이 된다. classifier가 교차하는 부분(decision boundary)에서의 확률은 0.5이다.&lt;/p&gt;

&lt;p&gt;그렇다면 점선 classifier와 실선 classifier 중 어떤 것이 더 나은 classifier일까?&lt;/p&gt;

&lt;p&gt;실선이 더 나은 classifier다.&lt;/p&gt;

&lt;p&gt;Decision boundary보다 살짝 왼쪽에 있는 $x_j$를 생각해보자. 점선 classifier는 선형이기 때문에 $x_j$가 빨간색인지 초록색인지 명확하게 구분하지 못할 가능성이 있다. 반면 실선 classifier는 점선 classifier보다는 명확하게 둘을 구분한다.&lt;/p&gt;

&lt;p&gt;실선이 더 나은 이유를 좀 더 formal하게 설명해 보겠다.&lt;/p&gt;

&lt;p&gt;위 그림에서 Decision Boundary의 왼쪽 부분을 보자. 이 부분에서도 분명히 $X$가 빨간 색으로 분류될 확률이 존재한다. 그런데 초록색으로 분류될 확률이 더 높기 때문에 이를 무시하는 것이다. 우리는 이 부분을 &lt;strong&gt;Bayes Risk&lt;/strong&gt;라고 한다.&lt;/p&gt;

&lt;p&gt;Bayes Risk의 측면에서 봤을 때, 점선 classifier의 risk가 실선 classifier의 risk보다 반달 모양만큼 더 크다. 그러므로 실선 classifier가 더 좋은 classifier다.&lt;/p&gt;

&lt;p&gt;우리의 목표는 작은 Bayes Risk를 가지는 classifier를 만드는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;learning-the-optimal-classifier&quot;&gt;Learning the Optimal Classifier&lt;/h1&gt;
&lt;p&gt;앞서 배웠던 Optimal Classifier는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^*(x)=argmax_{Y=y}P(Y=y|X=x)&lt;/script&gt;

&lt;p&gt;이를 사전확률을 사용할 수 있는 형태로 바꾸면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^*(x)=argmax_{Y=y}P(X=x|Y=y)P(Y=y)&lt;/script&gt;

&lt;p&gt;이 경우 $P(Y=y)$는 경험적으로 알아내거나, 데이터셋에서도 알아낼 수도 있다. 데이터셋 전체에서 $y_1$는 a%, $y_2$는 (1-a)%와 같이 알아낸다.&lt;/p&gt;

&lt;p&gt;$P(X=x|Y=y)$도 conditional density를 만들 수 있다. 그런데 이 경우 문제가 되는 것은 X에 여러 개의 Random Variable이 있는 경우이다. 이 때 여러 Random Variable의 상호작용을 고려해 prediction을 해야 하는데, Random Variable의 수가 많아질수록 고려해야 할 상호작용의 수도 많아진다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결해주는 것이 Naive Bayes classifier다. Naive Bayes는 Random Variable 간의 상호작용을 무시한다.&lt;/p&gt;

&lt;h1 id=&quot;dataset-for-optimal-classifier-learning&quot;&gt;Dataset for Optimal Classifier Learning&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88513569-7c88e300-d023-11ea-9567-516b56a9dcf4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 $f^*(x)=argmax_{Y=y}P(X=x|Y=y)P(Y=y)$를 구할 때, 고려해야 할 X가 여러개 있을 때의 상황에 대한 문제를 제기를 했었다. 여기서는 그에 부합하는 데이터셋을 소개한다.&lt;/p&gt;

&lt;p&gt;$X=x_1, x_2, x_3, x_4, x_5, x_6$이며, 각각 “Sky”, “Temp”, “Humid”, “Wind”, “Water”, “Forecst”를 의미한다.&lt;/p&gt;

&lt;p&gt;$P(Y=y)$를 구하는 것은 쉽다. 데이터셋을 보면 $P(y=yes)=3/4$, $P(y=yes)=1/4$이다.&lt;/p&gt;

&lt;p&gt;반면, P(X=x|Y=y)를 구하는 것은 까다로운데, 1번 row를 대입해보면 다음과 같아진다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1=sunny, x_2=warm, x_3=normal, x_4=strong, x_5=warm, x_6=same|y=yes)&lt;/script&gt;

&lt;p&gt;$x_1, x_2, …x_d$은 두 개의 값만 가지고, $Y$는 $k$개의 값을 가진다고 가정하면, $(2^d-1)k$개 만큼의 확률값을 알아야 이 문제를 해결할 수 있다.&lt;/p&gt;

&lt;p&gt;문제는 고려해야할 확률값의 수가 $X$의 수에 따라 기하급수적으로 증가한다는 점이다.&lt;/p&gt;

&lt;h1 id=&quot;conditional-independence&quot;&gt;Conditional Independence&lt;/h1&gt;
&lt;p&gt;이 문제를 해결하기 위해 도입한 것이 Conditional Independence다. 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1,x_2|y)=P(x_1|y)P(x_2|y)&lt;/script&gt;

&lt;p&gt;이것을 $P(X=&amp;lt; x_1, x_2, …, x_d &amp;gt;|Y=y)$에 적용시키면 $\prod_{i}^d P(X=x_i|Y=y)$으로 바뀐다.&lt;/p&gt;

&lt;p&gt;이 때 필요한 확률값의 수는 $(2-1)dk$개이다.&lt;/p&gt;

&lt;p&gt;사실 모든 입력 변수들이 서로 독립이라는 조건은 현실적이지 않다. 하지만 모델을 단순화시키고, 연산량을 줄이기 위해 마지못해 하는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;naive-bayes-classifier-function&quot;&gt;Naive Bayes Classifier Function&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f_{NB}(x)=argmax_{Y=y}P(Y=y)\prod_{1\le i \le d}P(X=x_i|Y=y)&lt;/script&gt;&lt;/p&gt;

&lt;h1 id=&quot;conditional-independence-vs-marginal-independence&quot;&gt;Conditional Independence vs Marginal Independence&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88601092-cf59ad80-d0aa-11ea-908e-b4a37ea62058.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Commander가 지시를 내렸는데 Officer A는 그 지시를 잘 듣지 못했다. 그 상황에서 Officer B가 앞으로 전진한다면 Officer A는 지시가 무엇이었는지 잘 알지도 못한 채로 Officer B를 따라 전진할 가능성이 크다. Officer B가 가지고 있는 정보가 Officer A에게 영향을 주는 것이다. 즉, 이 둘은 독립적이지 않다.&lt;/p&gt;

&lt;p&gt;반면, Commander의 지시를 명확하게 들었다면 Officer B가 앞으로 가든, 뒤로 가든 상관 없이 들은대로 수행한다. Officer B가 가지고 있는 정보가 Officer A에게 영향을 주지 못하는 것이다. 즉, 이 둘은 독립적이다.&lt;/p&gt;

&lt;p&gt;$Y$라는 변수가 Commander이고, $X_1$, $X_2$가 각각 Officer A, B라고 해보자.&lt;/p&gt;

&lt;p&gt;$Y$에 대한 관측이 없다면, $X_2$의 정보가 $X_1$에 영향을 미친다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_1=Go|X_2=Go, Commander=Go)\neq P(X_1=Go, Commander=Go)&lt;/script&gt;

&lt;p&gt;반면에 $Y$에 대한 관측이 있다면, $X_2$의 정보가 $X_1$에 영향을 미치지 않는다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_1=Go|X_2=Go, Commander=Go)= P(X_1=Go, Commander=Go)&lt;/script&gt;

&lt;p&gt;즉, $Y$가 관측되었기 때문에 우리는 Conditional Independence를 정의할 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;problems-of-naive-bayes-classifier&quot;&gt;Problems of Naive Bayes Classifier&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Naive Assumption
변수들이 conditionally independence하다는 가정 자체가 현실적이지 않음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incorrect Probability Estimation
압정을 세 번 던져서 모두 앞면이 나왔다면, 뒤면이 나올 확률은 분명히 존재함에도 $P(뒷면)=0$이 된다(MLE).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">Optimal Classification</summary></entry><entry><title type="html">Bayes Theroem</title><link href="http://localhost:4000/posts/2020/07/27/bayes_theorem" rel="alternate" type="text/html" title="Bayes Theroem" /><published>2020-07-27T00:00:00-07:00</published><updated>2020-07-27T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/27/bayes_theorem</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/27/bayes_theorem">&lt;h1 id=&quot;베이즈-정리bayes-theorem&quot;&gt;베이즈 정리(Bayes Theorem)&lt;/h1&gt;
&lt;p&gt;머신러닝을 공부하면서 베이즈 정리에 대해 각잡고 정리한 적이 없었다. 여러 리소스에서 나오는 내용들을 보며 파편적으로 베이즈 정리를 이해하고 있었다. 그런데 베이즈 정리라는게 참 특이하게 자료마다 다르게 설명한다. 크게 두 가지 정도 설명이 기억이 난다.&lt;/p&gt;

&lt;p&gt;1&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$P(B|A)$를 구하는 것이 불가능할 때, $P(A|B)$와 $P(B)$를 활용하여 $P(B|A)$를 구하기 위한 것&lt;/p&gt;

  &lt;p&gt;한양대학교 이상화 교수님 확률 및 통계 수업 1강&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;우리가 가지는 견해(예를 들어, 북극의 얼음은 a속도로 녹는다)에 관측 데이터를 추가하여 우리의 견해를 수정해나갈 수 있도록 해주는 것&lt;/p&gt;

  &lt;p&gt;PRML&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 두 내용이 같은 개념을 설명하기 위한 것임을 이해하기는 쉽지 않다. 그래서 베이즈 정리는 볼 때 마다 새롭고, 처음 보는 기분이 들었다.&lt;/p&gt;

&lt;p&gt;이런 애매모호함을 타파하기 위해 베이즈 정리를 정리해봤다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;베이즈 정리는 &lt;strong&gt;사후확률(posterior probability)을 사전확률(prior probability)과 likelihood를 이용해서 계산할 수 있도록 해주는 확률 변환식&lt;/strong&gt;이다.&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y|X)={P(X|Y)P(Y)\over P(X)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$P(Y|X)$ : 사후확률. 사건이 발생한 후 그 사건이 특정 모델에서 발생했을 확률&lt;/li&gt;
  &lt;li&gt;$P(X|Y)$ : likelihood. 어떤 모델에서 해당 데이터(관측치)가 나올 확률&lt;/li&gt;
  &lt;li&gt;$P(Y)$ : 사전확률. 관측자가 관측을 하기 전에 시스템 또는 모델에 대해 가지고 있는 선험적 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;베이즈 정리를 이해했고, 나를 헷갈리게 했던 두 가지 설명도 명확히 이해했는지 확인하기 위해 예를 들어 보자.&lt;/p&gt;

&lt;p&gt;당신은 압정을 던져 어떤 면이 나오는지 맞추는 도박에 참가하였다. 이 도박에서 이기기 위해서는 앞면이 나올 확률을 정확히 파악해야 한다. 당신은 믿을만한 도박 전문가로부터 앞, 뒤가 나올 확률은 동등하게 0.5라는 조언을 얻었으며, 5회의 실험을 통해 앞면이 3번 나왔고, 뒷면이 2번 나왔다. 이 때, 앞면이 나올 확률은 얼마인가?&lt;/p&gt;

&lt;p&gt;우리가 구하고자 하는 것은 앞면이 나올 확률인 $\theta$다. 도박 전문가에게 얻은 조언은 사전확률이라고 할 수 있으며, 실험 $D$의 결과로 likelihood를 계산해낼 수 있다. 이 두 가지 정보를 바탕으로 우리는 사후확률 $P(\theta|D)$를 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;1번 설명은 앞면이 나올 확률인 $\theta$를 알 수 없으니 관측을 통한 결과로부터 likelihood를 구하고, 도박 전문가로부터 조언을 얻어(사전확률) $\theta$를 계산해낼 수 있다는 내용이다.&lt;/p&gt;

&lt;p&gt;2번 설명은 사전확률에 조금 더 초점을 맞춰 설명한다. 도박 전문가에게 앞면이 나올 확률(사전확률)이 0.5라는 조언을 얻었지만 이 내용을 완전히 믿을 수는 없으므로, 관측치를 기반으로 해당 확률을 수정해나간다는 내용이다.&lt;/p&gt;

&lt;p&gt;결국 1번, 2번 설명이 사실은 같은 개념에 대한 것이지만 다르게 느껴졌던 이유는 서로 다른 부분에 집중해 설명했기 때문이다. 사후확률, 사전확률, likelihood의 관계를 명확히 파악하면 앞으로 이 개념 때문에 헷갈릴 일은 없을 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://darkpgmr.tistory.com/119&quot;&gt;다크프로그래머 베이지언 확률(Bayesian Probability)&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">베이즈 정리(Bayes Theorem) 머신러닝을 공부하면서 베이즈 정리에 대해 각잡고 정리한 적이 없었다. 여러 리소스에서 나오는 내용들을 보며 파편적으로 베이즈 정리를 이해하고 있었다. 그런데 베이즈 정리라는게 참 특이하게 자료마다 다르게 설명한다. 크게 두 가지 정도 설명이 기억이 난다.</summary></entry><entry><title type="html">02 Fundamentals of Machine Learning</title><link href="http://localhost:4000/posts/2020/07/26/02_Fundamentals_of_Machine_Learning" rel="alternate" type="text/html" title="02 Fundamentals of Machine Learning" /><published>2020-07-26T00:00:00-07:00</published><updated>2020-07-26T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/26/02_Fundamentals_of_Machine_Learning</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/26/02_Fundamentals_of_Machine_Learning">&lt;h1 id=&quot;rule-based-learning&quot;&gt;Rule-based Learning&lt;/h1&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/oNTXMgqCv6E&quot;&gt; &lt;/iframe&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/gmKvXBBawmk&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;완벽한 세상&lt;/strong&gt;에서만 잘 작동된다.
    &lt;ol&gt;
      &lt;li&gt;관측 에러가 없으며, 일관적이지 않은 관측 또한 없다.&lt;/li&gt;
      &lt;li&gt;랜덤(stochastic) 이벤트가 발생하지 않는다.&lt;/li&gt;
      &lt;li&gt;모든 경우의 수를 다 설명할 수 있는 수준의 많은 데이터를 확보했다.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;decision-tree&quot;&gt;Decision Tree&lt;/h1&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/dPP60RSEBek&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;error가 있는 데이터에 통계적 기법을 가미해 학습을 할 수 있는 가장 간단한 방법이 Decision Tree&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기준이 되는 피처를 정해 데이터셋을 나누고, 나눠진 각각에 대해 다시 피처를 정해 데이터셋을 나누는 작업을 반복해 분류 작업을 수행&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;어떤 피처를 선택할지 결정하기 위해 &lt;strong&gt;Entropy, Information Gain&lt;/strong&gt; 개념 도입&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/kG-IVxyUAUM&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;확률 변수의 불확실성을 측정하기 위한 지표
    &lt;ul&gt;
      &lt;li&gt;ex 1) 압정 A를 반복해서 던졌는데, 계속 Head만 나왔다 -&amp;gt; 불확실성이 낮다.&lt;/li&gt;
      &lt;li&gt;ex 2) 압정 B를 반복해서 던졌는데, Head와 Tail이 반씩 나왔다. -&amp;gt; 불확실성이 높다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;높은 엔트로피는 그 확률 변수가 높은 불확실성을 지니고 있음을 뜻한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X)=-\sum_xP(X=x)log_2P(X=x)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;압정 던지기 게임의 경우 $x$는 Head, Tail이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conditional-entropy&quot;&gt;Conditional Entropy&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{matrix}
H(Y|X) &amp;=&amp; -\sum_xP(X=x)log_2H(Y|X=x)\\
	        &amp;=&amp; -\sum_xP(X=x)\{-\sum_yP(Y=y|X=x)log_2P(Y=y|X=x)\}
\end{matrix} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;information-gain&quot;&gt;Information Gain&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475631-b9de6980-cf6c-11ea-828f-b28eb23260f5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;먼저 전체 엔트로피를 측정해보자.
    &lt;ul&gt;
      &lt;li&gt;위에서 정의한 엔트로피 공식을 그대로 사용하면 된다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
          H(Y) &amp;=&amp; -\sum_{y\in\{+, -\}}P(Y=y)log_2P(Y=y)\\
          &amp;=&amp; -{307\over307+383}log_2{307\over307+383}-{383\over307+383}log_2{383\over307+383}
\end{matrix} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;이번에는 A1, A9이라는 조건을 주고 엔트로피를 측정해보자.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Conditional Entropy 공식을 사용하면 된다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$H(Y|A_1)=-\sum_{x\in{a, b, ?}}\sum_{y\in{+, -}}P(A_1=x|Y=y)log_2{P(A_1=x)\over P(A_1=x|Y=y)}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$H(Y|A_9)=-\sum_{x\in{t,f}}\sum_{y\in{+, -}}P(A_9=x|Y=y)log_2{P(A_9=x)\over P(A_9=x|Y=y)}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;우리는 각 노드들의 엔트로피를 구할 수 있으며, 이를 통해 불확실성이 얼마나 개선되었는지를 측정할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$IG(Y, A_i)=H(Y)-H(Y|A_i)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$IG(Y, A_9)&amp;gt;IG(Y, A_1)$ 임을 알 수 있다. 즉, $A_9$를 기준으로 나눌 때 불확실성이 더 많이 개선된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;size-of-tree&quot;&gt;Size of Tree&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;분기 작업을 반복하면 거대한 트리를 만들 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;하지만 트리가 너무 거대해지면 학습 데이터셋에는 잘 작동하지만 테스트 데이터셋에서는 잘 작동하지 않는 오버피팅 문제가 발생한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475630-b8ad3c80-cf6c-11ea-8e6b-778e3f5054d3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강의에서는 Decision Tree를 Rule-based 기법의 하나로 소개하며, 오버피팅 문제 때문에 실제 상황에서는 많이 사용하지 않는다고 소개함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h1&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/70tDiv30WrM&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이제부터 본격적으로 통계를 활용한 머신러닝 기법들이 나옴&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression은 많은 머신러닝 기법들에 영향을 미친 중요한 기법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;강의에서는 주택 가격 예측 데이터셋을 활용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가정 : 주택 가격은 feature values의 linearly weighted sum으로 표현할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;h:\hat{f}(x;\theta)=\sum_{i=0}^{n}\theta_ix_i&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n$ : feature의 수&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;결국 Linear Regression은 $\theta$를 잘 조정해서 좋은 결과를 내도록 하는 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;수식을 행렬로 표현할 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=\begin{bmatrix}
          1 &amp; \cdots &amp; x_n^1 \\
          \vdots &amp; \ddots &amp; \vdots \\
          1 &amp; \cdots &amp; x_n^D
          \end{bmatrix}, 
      \theta=\begin{bmatrix}
                  \theta_0 \\
                  \vdots \\
                  \theta_n 
                  \end{bmatrix} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;h:\hat{f}=X\theta&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\hat{f}$는 예측값이고, 실제값은 다음과 같이 노이즈를 포함시켜 정의한다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$f=X\theta+e=Y$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Linear Regression의 목표는 실제값과 예측값의 차이를 최소화하는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
\hat{\theta} &amp;=&amp; argmin_\theta(f-\hat{f})^2 \\
	        &amp;=&amp; argmin_\theta(Y-X\theta)^2 \\
	        &amp;=&amp; argmin_\theta(Y-X\theta)^T(Y-X\theta) \\
	        &amp;=&amp; argmin_\theta (\theta^TX^TX\theta-2\theta^TX^TY+Y^TY) \\
	        &amp;=&amp; argmin_\theta (\theta^TX^TX\theta-2\theta^TX^TY)
\end{matrix} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;미분을 통해 극점을 찾는 방식으로 최적화시킬 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta(\theta^TX^TX\theta-2\theta^TX^TY)=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2X^TX\theta-2X^TY=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta=(X^TX)^{-1}X^TY&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$X, Y$ 모두 아는 값이므로 $\theta$를 구할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;아래 그림은 feature를 하나만 사용하여 예측을 해본 것이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475629-b814a600-cf6c-11ea-8365-2b819a6c3bfb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;non-linearity&quot;&gt;Non-Linearity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression은 선형이라는 한계 때문에 표현할 수 없는 부분이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$x$값에 non linearity를 부여해 조금 더 표현력이 높은 모델을 만들 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;h:\hat{f}(x;\theta)=\sum_{i=0}^n\sum_{j=1}^m\theta_{i,j}\Phi_j(x_i)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;위 식에서는 $\Phi$라는 non-linear function을 사용해 $x_i$를 변형시킨다. $\Phi$로 $x^2, x^3, x^4, …$ 등을 사용할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475627-b4811f00-cf6c-11ea-80b4-4fbfacf068c6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;위 그림은 $x^2, x^3, …, x^9$까지 포함하여 나타낸 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;주어진 데이터를 더 잘 표현한 것으로 볼 수도 있으나, Decision Tree에서 주어진 데이터를 잘 표현하기 위해 노드를 늘리는 것이 오버피팅 문제를 일으키듯이, 무작정 non-linearity를 부여하는 것도 오버피팅 문제를 일으킬 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">Rule-based Learning</summary></entry><entry><title type="html">01 Motivations and Basics</title><link href="http://localhost:4000/posts/2020/07/25/01_Motivations_and_Basics" rel="alternate" type="text/html" title="01 Motivations and Basics" /><published>2020-07-25T00:00:00-07:00</published><updated>2020-07-25T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/25/01_Motivations_and_Basics</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/25/01_Motivations_and_Basics">&lt;iframe src=&quot;https://www.youtube.com/embed/3AwO0O6hWBI&quot;&gt; &lt;/iframe&gt;

&lt;h1 id=&quot;thumbtack-question&quot;&gt;Thumbtack Question&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;당신은 백만장자에게 고용되었다. 백만장자는 압정을 던져 앞면이 나오는가 뒷면이 나오는가를 맞추는 도박을 하려고 하는데, 이것을 해도 손해는 보지 않을지 고민이 되어 당신을 고용하였다. 백만장자를 위해 &lt;strong&gt;압정 던지기 게임&lt;/strong&gt;과 그와 관련된 &lt;strong&gt;확률 이론&lt;/strong&gt;을 공부해보자.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;압정 던지기 게임에서 이기기 위해서는 앞면, 뒷면이 나올 확률을 정확하게 알아야 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;압정은 앞, 뒤가 다르게 생겨 각각이 나올 확률이 p, 1-p이다.  p=0.5가 아니라는 점에서 동전 던지기 게임과는 다르다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-tossing&quot;&gt;1. Tossing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 문제를 해결하기 위해 가장 쉽게 떠올릴 수 있는 방법은 압정을 던져보는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;당신은 백만장자 앞에서 압정을 다섯 번 던졌다. 그 결과 앞면이 세 번 나왔고, 뒷면이 두 번 나왔다.(HHTHT)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;실험의 결과에 따르면, 앞면이 나올 확률은 0.6이고, 뒷면이 나올 확률은 0.4이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;결과는 다음 두 이론을 바탕으로 하고 있다.&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;a href=&quot;https://blanik00.github.io/posts/2020/05/21/bern_bin&quot;&gt;Binomial Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://blanik00.github.io/posts/2020/05/03/maximum_likelihood_estimation&quot;&gt;MLE(Maximum Likelihood Estimation)&lt;/a&gt;&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;우리는 압정 던지기 게임의 결과가 Binomial Distribution을 따른다는 것을 가정한다. 이 때, 압정의 특이한 모양에 의해 형성된 앞면이 나올 확률을 $\theta$라고 하자.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(HHTHT)=\theta\theta(1-\theta)\theta(1-\theta)=\theta^3(1-\theta)^2&lt;/script&gt;

    &lt;p&gt;이를 일반화 시켜 확률 $\theta$가 주어졌을 때 데이터 $D$가 나올 확률은 다음과 같다.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$a_H$ : $D$에서 앞면이 나온 횟수&lt;/li&gt;
      &lt;li&gt;$a_T$ : $D$에서 뒷면이 나온 횟수&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;우리는 앞서 압정 던지기 게임의 결과가 Binomial Distribution을 따른다는 것을 가정했다. 그렇다면 어떻게 하면 이 가정을 더 강화할 수 있을까?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-mle&quot;&gt;2. MLE&lt;/h2&gt;

&lt;p&gt;우리는 가정을 강화하기 위해  데이터의 분포를 가장 잘 설명하는 $\theta$를 찾는 것에 집중할 것이다.&lt;/p&gt;

&lt;p&gt;이 목적을 위해 고려할 수 있는 방법이 MLE이다. 이 방법은 관측된 데이터가 등장할 확률을 최대화하는 $\hat{\theta}$를 찾는다. 수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
\hat{\theta} &amp;=&amp; argmax_{\theta}P(D|\theta) \\
       &amp;=&amp; argmax_{\theta}\theta^{a_H}(1-\theta)^{a_T} \\
       &amp;=&amp; argmax_{\theta}ln\{\theta^{a_H}(1-\theta)^{a_T}\} \\
       &amp;=&amp; argmax_{\theta}\{a_Hln\theta+a_Tln(1-\theta)\} \\
\end{matrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{d\over d\theta}\{a_Hln\theta+a_Tln(1-\theta)\}=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{a_H\over\theta}-{a_T\over 1-\theta}=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta={a_H\over a_T+a_H}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}={a_H\over a_T+a_H}&lt;/script&gt;

&lt;h2 id=&quot;3-more-tossing&quot;&gt;3. More Tossing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;당신이 이 내용을 백만장자에게 설명하는 동안 백만장자는 압정을 50번 더 던졌다. 그 결과 앞면이 30번, 뒷면이 20번 나왔다. 호기심이 많은 백만장자는 5번을 던져서 3번 앞면이 나온 것과 50번을 던져서 30번 앞면이 나온 것이 동일한 것인지 물어봤다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;우리가 MLE를 통해서 한 것은 결국 모수인 $\theta$를 추정하는 $\hat{\theta}$을 구한 것이다. 추정값은 어쩔 수 없이 오차를 가지고 있는데, 여러 번 실험을 해보는 백만장자의 시도로 추정의 오차를 줄일 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;오차를 측정하는 방법으로 Hoeffding’s Inequaility를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Hoeffding’s Inequaility
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$P(|\hat{\theta}-\theta^* |\ge \epsilon)\le2e^{-2N\epsilon^2}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;즉, 추정값과 실제값의 차이가 $\epsilon$보다 작을 확률은 $2e^{-2N\epsilon^2}$보다 작거나 같다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;수식에 영향을 미치는 변수는 두 가지가 있다.&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;$\epsilon$ : error bound인 $\epsilon$이 커지면 우변의 값이 작아져 추정값과 실제값의 차이가 error bound보다 커질 확률은 적어진다. 당연한 얘기다.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$N$ : 마찬가지로 시행 횟수인 $N$이 커지면 우변의 값이 작아져 추정값과 실제값의 차이가 error bound보다 커질 확률은 적어진다.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이 원리를 이용하면 error bound를 0.1로 잡았을 때, 몇 번의 시행을 해야 에러가 발생할 확률이 0.01% 이하로 떨어지는가? 등과 같은 계산을 할 수 있다. (error bound를 0.1로 고정시키고 우변이 0.01 이하로 떨어질 때 까지 $N$을 늘리면 된다)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이러한 것을 PAC(Probably Approximate Correct) learning이라고 한다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;그렇기 때문에 시행 횟수인 $N$을 늘린 백만장자의 시도는 에러가 발생할 확률을 줄여준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/LbYCQxKAv2E&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;4-bayes&quot;&gt;4. Bayes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bayes라는 사람이 찾아왔다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그는 지금까지의 실험을 보며 앞면이 나올 확률이 정말로 60%일 것인지 의심해봐야 한다고 주장했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그러면서 앞면과 뒷면이 나올 확률이 동등하게 0.5가 아닐까하며 백만장자를 설득했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;백만장자는 처음엔 0.5일 것이라고 생각은 했었는데, 실험을 해보니 그게 아니었다고 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;베이즈는 때를 놓치지 않고 백만장자의 사전 정보(앞뒤 확률이 같을 것이라는 것)를 파라미터를 추정하는 과정에 반영시킬 수 있다고 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;베이즈는 다음과 같은 공식을 제안했다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|D)={P(D|\theta)P(\theta) \over P(D)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;다음과 같이  표현할 수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Posterior={Likelihood \times Prior\;Knowledge \over Normalizing\;Constant}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;앞과 뒤가 나올 확률이 나올 확률이 0.5로 동일할 것이라는 백만장자의 생각은 Prior Knowledge에 들어간다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}$로 이미 구해두었으며, Prior Knowledge는 사전 지식이므로 $P(\theta|D)$를 바로 계산할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사실 $P(D)$는 이미 주어진 사실이므로, 이것이 발생할 확률은 정해져 있다. 즉, $\theta$에 영향을 받지 않는다. 그래서 보통 $P(D)$를 빼고 다음과 같은 식을 계산한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D)\propto P(D|\theta)P(\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta)=???&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;그렇다면 $P(\theta)$는 어떻게 표현할 수 있을까?
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$P(D|\theta)$는 Binomial Distribution을 따른다고 가정하고 계산했다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이처럼 $P(\theta)$도 특정 분포를 따른다고 가정하고 계산해야 한다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;여러가지 방법이 있는데 베이즈는 베타 분포를 제안했다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;베타 분포는 특정 범위에 있는 값을 0과 1 사이의 실수로 만들어 주기 때문에 확률로 사용하기 좋다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;베타 분포에 따르면 $P(\theta)$는 다음과 같이 표현할 수 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta)={\theta^{\alpha-1}(1-\theta)^{\beta-1} \over B(\alpha, \beta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(\alpha, \beta)={\Gamma(\alpha)\Gamma(\beta)\over \Gamma(\alpha+\beta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Gamma(\alpha)=(\alpha-1)!&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D)\propto P(D|\theta)P(\theta)\propto \theta^{a_H}(1-\theta)^{a_T}\theta^{\alpha-1}(1-\theta)^{\beta-1}&lt;/script&gt;

&lt;p&gt;($P(\theta|D)={P(D|\theta)P(\theta) \over P(D)}$의 $P(D)$, $P(\theta)={\theta^{\alpha-1}(1-\theta)^{\beta-1} \over B(\alpha, \beta)}$의 $B(\alpha, \beta)$는 $\theta$에 영향을 받는 요소가 아니라서 제거하였다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\theta^{a_T\alpha-1}(1-\theta)^{a_T+\beta-1}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MLE에서는 $\hat{\theta}=argmax_{\theta}P(D|\theta)$를 찾는 것이 문제였다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이번에 할 것은 MAP이며, $\hat{\theta}=argmax_{\theta}P(\theta|D)$를 구하는 문제이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;앞에서 $P(D)\propto \theta^{a_H}(1-\theta)^{a_T}\theta^{\alpha-1}(1-\theta)^{\beta-1}$인 것을 배웠다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MLE에서 최적값을 구했던 것과 같은 방법으로 구해보면 $\hat{\theta}$은 다음과 같다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}={a_H+\alpha-1\over a_H+\alpha+a_T+\beta-2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;문제를 바라보는 관점에서 차이가 있는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mle-vs-map&quot;&gt;MLE vs MAP&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;그렇다면 어떤 방법이 더 좋은 것일까?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사실 두 방법은 시행 횟수가 아주 많을 때 같아진다. MAP에 있는 $\alpha$, $\beta$는 $a_T$, $a_H$가 커짐에 따라 그 영향력이 작아지기 때문이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반대로 시행 횟수가 적은 상황에서는 사전 정보가 중요한 영향을 미치게 된다. 사전 정보인 $\alpha$, $\beta$을 어떻게 설정하느냐에 따라 MAP의 성능이 MLE보다 좋을 수도 있고, 나쁠 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Chi Square, T, F Distribution</title><link href="http://localhost:4000/posts/2020/07/05/chisq_t_f" rel="alternate" type="text/html" title="Chi Square, T, F Distribution" /><published>2020-07-05T00:00:00-07:00</published><updated>2020-07-05T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/05/chisq_t_f</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/05/chisq_t_f">&lt;iframe src=&quot;https://www.youtube.com/embed/DyBxYsCow9k&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;chi-square-distribution&quot;&gt;Chi-Square Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;iid인 표준 정규 분포가 n개 있다.($Z_1, Z_2, …, Z_n$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Z=Z_1^2+ Z_2^2+ …+ Z_n^2$는 카이제곱 분포를 따른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Z\sim \chi^2(v)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;파라미터인 $v$는 자유도를 뜻하며, 카이제곱 분포를 형성할 때 더한 표준 정규 분포의 갯수를 뜻한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;카이제곱 분포의 PDF&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/86525674-3e395180-bec5-11ea-9a1c-78a13c8d4c3a.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$E[X]=v,\;V[X]=2v$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\alpha=v/2, \lambda=2$인 특별한 형태의 감마분포이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;표준 정규 분포가 표준 정규 분포표를 가지고 있듯이, 카이제곱 분포도 카이제곱 분포표를 가지고 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-variance&quot;&gt;Sampling Distribution of Sample Variance&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$\mu$는 모평균이다.
 $\chi^2(n)=\sum_{i=1}^{n}(X_i-\mu)^2/\sigma^2$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\bar{X}$는 표본 평균이다. 표본 평균으로 추정을 했으므로 알고 있는 것이다. 그래서 $n$개 중에서 하나(표본 평균)는 이미 정해진 값이고, 나머지 $n-1$개는 자유롭게 정해질 수 있는 값이다. 그래서 자유도가 $n-1$이다.
 $\chi^2(n-1)=\sum_{i=1}^{n}(X_i-\bar{X})^2/\sigma^2$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;자유도보충설명&quot;&gt;자유도(보충설명)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;자유도를 명확하게 배우기는 쉽지 않다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;많이 접하면서 감을 잡을 수 밖에 없다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예시&quot;&gt;예시&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$X_1=2, X_2=3, X_3=4, X_4=5, X_5=?$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\sum_{i=1}^{5}X_i=20$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 두 조건을 만족할 때, $X_5$는 얼마인가?&lt;/p&gt;

&lt;p&gt;$\sum_{i=1}^{4}X_i=14$이므로 $X_5$는 6이 될 수 밖에 없다.&lt;/p&gt;

&lt;p&gt;이 문제에서 자유도는 얼마일까?&lt;/p&gt;

&lt;p&gt;문제에서 자유롭게 정해질 수 있는 값은 $X_1, X_2, X_3, X_4$이며, $X_5$는 나머지 값들에게 종속된다. 그러므로 자유도는 4이다.&lt;/p&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-variance---example&quot;&gt;Sampling Distribution of Sample Variance - Example&lt;/h2&gt;
&lt;p&gt;$N(\mu=15,\mu^2=100)$인 모집단으로부터 독립적이고 동일하게(iid) 25개의 샘플을 뽑았다($X_1, X_2, …, X_{25}$). 이 때 $Y=(n-1)S^2/\sigma^2$는 $\chi^2(24)$를 따른다(자유도는 24). 즉, $P[Y&amp;gt;\chi^2_{\alpha,24}]=\alpha$라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;$P[S^2&amp;gt;c]=0.95$일 때 c의 값을 구하라.&lt;/p&gt;

&lt;p&gt;$S^2$의 분포를 바로 파악하기 힘드므로 양 변에 $(n-1)$을 곱하고, $\sigma^2$으로 나눈다. 즉, $P[(n-1)S^2/\sigma^2&amp;gt;(n-1)c/\sigma^2]=0.95$을 만족하는 c를 찾으면 된다.&lt;/p&gt;

&lt;p&gt;$P[Y&amp;gt;24c/100]=0.95$&lt;/p&gt;

&lt;p&gt;카이제곱 분포표에서 $n$ 24이고, $\alpha$가 0.95인 것을 찾으면 $24c/100=13.848$이 된다. $c=57.7$&lt;/p&gt;

&lt;h2 id=&quot;t-distribution&quot;&gt;t Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$Z\sim N(0,1)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y\sim \chi^2(v)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;T={Z\over\sqrt{Y/v}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$T$ 통계량은 자유도가 $v$인 t-분포(t-distribution)를 따른다.($t(v)$로 표기)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;카이제곱 분포의 자유도가 그대로 t-분포의 자유도가 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;t-분포의 기댓값은 0이고, 이를 기준으로 좌우 대칭이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;표준정규분포보다 긴 꼬리를 가지고 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;기댓값을 기준으로 좌우 대칭이라는 특성 때문에 아래 수식을 만족한다.
$P[T&amp;lt;-t_{\alpha, v}]=P[T&amp;gt;t_{\alpha, v}]=\alpha$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/86525673-3d082480-bec5-11ea-8154-a9d041a0a4df.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽 그림에서 알 수 있듯이 표준 정규 분포보다 긴 꼬리를 가지고 있다.&lt;/p&gt;

&lt;p&gt;왼쪽 그림에서 알 수 있듯이 자유도가 작을수록 긴 꼬리를 가진다.&lt;/p&gt;

&lt;h3 id=&quot;t-분포-조금-더-자세히&quot;&gt;t-분포 조금 더 자세히&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$N(\mu,\sigma^2)$에서 표본 $n$개를 추출한다($X_1, X_2, …, X_n$).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Z=\bar{X}-\mu/(\sigma/\sqrt{n})$이므로 $N(0,1)$을 따른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y=(n-1)S^2/\sigma^2$이므로 $\chi^2(n-1)$을 따른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;T={Z\over\sqrt{Y/v}}={\bar{X}-\mu/(\sigma/\sqrt{n})\over\sqrt{((n-1)S^2/\sigma^2)/(n-1)}}={\bar{X}-\mu\over S/\sqrt{n}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 때, $T$는 자유도가 $n-1$인 t-분포를 따른다.($T\sim t(n-1)$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;신기하게도 $T$ 통계량과 표준 정규 분포는 매우 닮았다. 다른 점이 있다면 표준 정규 분포에서는 $\sigma$(모표준편차)가 사용되고, $T$ 통계량에서는 $S$(표본표준편차)가 사용된다는 점이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;그래서 모집단의 표준편차를 알고 있을 때는 표준 정규 분포로 표현하면 되고, 모집단의 표준 편차를 모른다면 t-분포로 표현하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;t-distribution---example&quot;&gt;t Distribution - Example&lt;/h2&gt;
&lt;p&gt;$N(\mu=15,\mu^2=100)$인 모집단으로부터 독립적이고 동일하게(iid) 25개의 샘플을 뽑았다($X_1, X_2, …, X_{25}$).&lt;/p&gt;

&lt;p&gt;$T={\bar{X}-\mu\over S/\sqrt{n}}$이고, 이는 $t(v=24)$를 따른다.&lt;/p&gt;

&lt;p&gt;$P[T&amp;lt;-t_{\alpha, 24}]=P[T&amp;gt;t_{\alpha, 24}]=\alpha$&lt;/p&gt;

&lt;p&gt;$P[T &amp;lt; c]=0.05$인 $c$를 구하라.&lt;/p&gt;

&lt;p&gt;$t$분포 테이블에서 자유도가 24이고, $\alpha$가 0.05인 것을 찾으면 1.711이다. $c$보다 작을 확률이므로 $c=-1.711$이 된다.&lt;/p&gt;

&lt;h2 id=&quot;f-distribution&quot;&gt;F Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$Y_1=\chi^2(v_1)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y_2=\chi^2(v_2)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;F={Y_1/v_1\over Y_2/v_2}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이렇게 정의되는 것을 F통계량이라고 하고, 이는 F 분포를 따른다. F분포는 $v_1, v_2$ 두 개의 자유도를 가지며, $F(v_1, v_2)$로 표기한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;F 테이블이 있어 확률값을 쉽게 구할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$P[F&amp;gt;f_a(v_1,v_2)]=f_{1-\alpha}(v_1, v_2)={1\over f_{\alpha}(v_2,v_1)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/86525671-39749d80-bec5-11ea-9663-a8bc3cacb227.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;f-distribution---example&quot;&gt;F Distribution - Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모집단 1 : $N(\mu_1=4,\sigma_1^2=16)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모집단 2 : $N(\mu_2=12,\sigma_2^2=48)$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;모집단 1로부터 동일하고 독립적인 $n_1=15$개의 샘플을 추출하고, 모집단 2로부터 동일하고 독립적인 $n_2=10$개의 샘플을 추출한다.&lt;/p&gt;

&lt;p&gt;$F={S_1^2/\sigma_1^2\over S_2^2/\sigma_2^2}$는 $F(14,9)$를 따른다.&lt;/p&gt;

&lt;p&gt;이 때 $P[S_1^2/S_2^2\le c]=0.05$일 때, c를 구하라.&lt;/p&gt;

&lt;p&gt;$P[S_1^2/S_2^2\le c]=P[S_1^2\sigma_2^2/S_2^2\sigma_1^2\le (\sigma_2^2/\sigma_1^2)c]=P(F&amp;lt;3c)=0.05$&lt;/p&gt;

&lt;p&gt;$P[F&amp;gt;f_{0.95}(14,9)]=0.95$&lt;/p&gt;

&lt;p&gt;$P[F\le f_{0.95}(14,9)]=0.05$&lt;/p&gt;

&lt;p&gt;$3c=f_{0.95}(14,9)=1/f_{0.05}(9,14)=1/2.65$&lt;/p&gt;

&lt;p&gt;$c=0.1258$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Sample Distribution</title><link href="http://localhost:4000/posts/2020/06/27/sample_distribution" rel="alternate" type="text/html" title="Sample Distribution" /><published>2020-06-28T00:00:00-07:00</published><updated>2020-06-28T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/27/Sample%20Distribution</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/27/sample_distribution">&lt;iframe src=&quot;https://www.youtube.com/embed/EjQq2s9C9Sc&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;population-sample&quot;&gt;Population, Sample&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Population : 전체(모집단)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sample : 부분(모집단의 부분집합, 표본)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling&quot;&gt;Sampling&lt;/h2&gt;
&lt;p&gt;모집단에서 표본을 추출하는 작업을 의미&lt;/p&gt;

&lt;h3 id=&quot;표기&quot;&gt;표기&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Mean&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Variance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Population&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\mu$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\sigma^2$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sample&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\bar{X}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$S^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;statistic통계량&quot;&gt;Statistic(통계량)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Statistic : 샘플들의 함수&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;통계량의 복수형인 Statistics를 통계학을 의미하는 Statistics와 혼동하기 쉽다. 문맥을 보고 파악하면 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;통계학에서 배우는 가장 중요한 함수가 세 가지 있다.
    &lt;ul&gt;
      &lt;li&gt;확률 변수(Random Variable)&lt;/li&gt;
      &lt;li&gt;확률 함수(Probability Function)&lt;/li&gt;
      &lt;li&gt;통계량(Statistic)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;대표적인 통계량은 다음과 같다.
    &lt;ul&gt;
      &lt;li&gt;$\bar{X}=(x_1+x_2+…+x_n)/n$&lt;/li&gt;
      &lt;li&gt;$S^2={1\over n-1}\sum_{i=1}^{n}(x_1-\bar{X})^2$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution&quot;&gt;Sampling Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sampling Distribution : 통계량의 분포&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그렇다면 Sampling Distribution은 $\bar{X}$, $S^2$의 분포를 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;여기서 Sampling된 것들은 서로 독립이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-mean&quot;&gt;Sampling Distribution of Sample Mean&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\bar{X}=(x_1+x_2+…+x_n)/n$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그렇다면 $\bar{X}$의 기댓값은 얼마일까&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$E[\bar{X}]=E[(x_1+x_2+…+x_n)/n]$&lt;/p&gt;

&lt;p&gt;$=E[x_1]/n+E[x_2]/n+…+E[x_n]/n$&lt;/p&gt;

&lt;p&gt;$=\mu/n+\mu/n+…+\mu/n$&lt;/p&gt;

&lt;p&gt;$=n(\mu/n)=\mu$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;또한 $\bar{X}$의 분산은 얼마일까&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$V[\bar{X}]=V[(x_1+x_2+…+x_n)/n]$&lt;/p&gt;

&lt;p&gt;$=V[x_1]/n^2+V[x_2]/n^2+…+V[x_n]/n^2$&lt;/p&gt;

&lt;p&gt;$=\sigma^2/n^2+\sigma^2/n^2+…+\sigma^2/n^2$&lt;/p&gt;

&lt;p&gt;$=n(\sigma^2/n^2)=\sigma^2/n$&lt;/p&gt;

&lt;h3 id=&quot;정규-분포를-예로-들어보자&quot;&gt;정규 분포를 예로 들어보자.&lt;/h3&gt;
&lt;p&gt;모집단이 $N(\mu, \sigma^2)$를 따른다고 하자. 여기서 iid인 샘플 n개를 추출했다.($X_1$, $X_2$, …, $X_n$) 그렇다면 $\bar{X}$는 $N(\mu, \sigma^2/n)$를 따른다. 표준정규분포로 바꾸려면 $Z=(\bar{X}-\mu)/(\sigma/\sqrt{n})$로 두면 된다.&lt;/p&gt;

&lt;h2 id=&quot;central-limit-theorem중심극한정리&quot;&gt;Central Limit Theorem(중심극한정리)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;평균이 $\mu$이고 분산이 $\sigma^2$인 임의의 모집단에서 표본의 크기인 $n$이 충분히 크면($n\ge 30$), 표본 평균 $\bar{X}$는 근사적으로 정규분포 $N(\mu, \sigma^2/n)$를 따른다. (여기서 n은 한 표본 안에 들어있는 원소의 수를 뜻한다.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-mean---two-population&quot;&gt;Sampling Distribution of Sample Mean - Two Population&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;두 개의 독립적인 모집단의 평균의 차이를 알고 싶다. 하지만 모집단을 모두 파악하는 것은 불가능하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;두 개의 모집단(어떤 분포를 따르든 상관 없다.)으로부터 샘플을 추출해 비교하는 방법을 취한다.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Pop1. ($\mu_1$, $\sigma_1^2$) $\rightarrow$ $n_1$개 샘플링 $\rightarrow$ $\bar{X_1}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pop2. ($\mu_2$, $\sigma_2^2$) $\rightarrow$ $n_2$개 샘플링 $\rightarrow$ $\bar{X_2}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\bar{X_1}-\bar{X_2}$의 기댓값과 분산을 보고 두 모집단의 차이를 확인할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;$E[\bar{X_1}-\bar{X_2}]=E[\bar{X_1}]-E[\bar{X_2}]=\mu_1-\mu_2$&lt;/li&gt;
      &lt;li&gt;$V[\bar{X_1}-\bar{X_2}]=V[\bar{X_1}]+V[\bar{X_2}]=\sigma_1^2/n_1+\sigma_2^2/n_2$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, $\bar{X_1}-\bar{X_2}$는 $N(\mu_1-\mu_2, \sigma_1^2/n_1+\sigma_2^2/n_2)$를 따른다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry></feed>