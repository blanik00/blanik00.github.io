<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-11T05:08:44-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JeongUk Jang</title><subtitle>Github Pages template for academic personal websites, forked from mmistakes/minimal-mistakes</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><entry><title type="html">CNN</title><link href="http://localhost:4000/posts/2020/05/09/cnn" rel="alternate" type="text/html" title="CNN" /><published>2020-05-09T00:00:00-07:00</published><updated>2020-05-09T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/09/CNN</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/09/cnn">&lt;h2 id=&quot;cnn이란&quot;&gt;CNN이란?&lt;/h2&gt;
&lt;p&gt;CNN은 아래 세 가지 요소로 이루어져 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Convolution : Input Image에 Convolution 연산을 하면 “Convolutional Feature Map”이 나온다.&lt;/li&gt;
  &lt;li&gt;Subsampling : sampling을 통해 image 혹은 feature map의 크기(spatial한 정보)가 줄어듦.&lt;/li&gt;
  &lt;li&gt;Full Connection(Dense)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Convolution과 Subsampling은 &lt;strong&gt;특성 추출(feature extraction)&lt;/strong&gt;을 수행한다. 즉, CNN은 feature들의 조합으로 물체를 구분한다.&lt;/p&gt;

&lt;p&gt;또한 Full Connection은 feature들을 기반으로 분류를 수행한다.&lt;/p&gt;

&lt;p&gt;즉, CNN은 이미지로부터 특징을 추출해내고, 이를 기반으로 분류를 하는 모델이다.&lt;/p&gt;

&lt;h2 id=&quot;왜-cnn이-잘-될까&quot;&gt;왜 CNN이 잘 될까?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Local Invariance
동일한 사이즈의 filter가 이미지의 모든 부분을 돌아다니기 때문에 찾고자 하는 이미지가 어디에 있는지는 중요하지 않다.&lt;/li&gt;
  &lt;li&gt;Compositionality
CNN을 여러 층 쌓으면 계층 구조가 생겨 성능이 좋다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convolution이란&quot;&gt;Convolution이란&lt;/h2&gt;
&lt;p&gt;이미지의 특정 부분과 filter가 얼마나 유사한지 연산하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81528300-572af800-9397-11ea-9833-d7abea9d8664.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 Image의 진한 파란 부분과 filter의 convolution 연산을 수행한다면 두 행렬을 element-wise product하고 sum하면 된다. (=51)&lt;/p&gt;

&lt;p&gt;해당 부분에 대한 convolution을 완료했다면, 다음 칸으로 옮겨 동일한 연산을 수행한다.&lt;/p&gt;

&lt;h2 id=&quot;zero-padding&quot;&gt;Zero-Padding&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81528296-54c89e00-9397-11ea-9864-7c430aa6c784.gif&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convolution을 해야 하는데, 이를 위해서는 원본 이미지의 사이즈를 좀 더 크게 만들어야 할 때 zero-padding을 한다. 위 그림에서 실선 부분이 zero-padding한 곳이다.&lt;/p&gt;

&lt;h2 id=&quot;stride&quot;&gt;Stride&lt;/h2&gt;
&lt;p&gt;Convolution 연산을 마치고 몇 칸을 뛰는지를 결정하는 것이 Stride이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81528298-56926180-9397-11ea-9723-a2f49c6c2d26.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 왼쪽은 stride=1이고, 오른쪽은 stride=2이다.&lt;/p&gt;

&lt;p&gt;stride의 수와 filter의 Width, Height가 같다면 Overlapping없이 feature map을 생성할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;input-image&quot;&gt;Input Image&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;[batch, in_height, in_width, in_channels]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;batch : batch size
in_channels : RGB인 경우 3, Grey인 경우 1&lt;/p&gt;

&lt;h2 id=&quot;filter&quot;&gt;Filter&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;[filter_height, filter_width, in_channels, out_channels]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;input_ image의 in_channels와 filter의 in_channels는 항상 같아야 한다.
out_channels는 필터의 개수&lt;/p&gt;

&lt;h2 id=&quot;파라미터의-수&quot;&gt;파라미터의 수&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;모델 보면 파리미터의 수를 셀 줄 알아야 한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$4\times 4\times 3$ 이미지를 $3\times 3\times 3$ filter로 convolution해서 $4\times 4\times 7$인 feature map이 나왔다고 하자. 이 경우 filter_height=3, filter_width=3, in_channels=3, out_channels=7이므로, 파라미터의 수는 $3\times 3\times 3 \times 7=189$이다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">CNN이란? CNN은 아래 세 가지 요소로 이루어져 있다.</summary></entry><entry><title type="html">최대우도추정(MLE, Maximum Likelihood Estimation)</title><link href="http://localhost:4000/posts/2020/05/03/maximum_likelihood_estimation" rel="alternate" type="text/html" title="최대우도추정(MLE, Maximum Likelihood Estimation)" /><published>2020-05-03T00:00:00-07:00</published><updated>2020-05-03T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/03/maximum_likelihood_estimation</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/03/maximum_likelihood_estimation">&lt;iframe src=&quot;https://www.youtube.com/embed/XepXtl9YKwc&quot;&gt; &lt;/iframe&gt;

&lt;p&gt;최대우도추정 : 데이터를 가장 잘 설명하는 모델의 모수($\theta$)를 찾는 것&lt;/p&gt;

&lt;p&gt;관측된 데이터 $X_1=x_1$, $X_2=x_2$, …, $X_n=x_n$이 있다고 할 때 $\theta$의 가능도는 다음과 같다.&lt;/p&gt;

&lt;p&gt;$L(\theta)=P(x_1, x_2, …, x_n|\theta)$&lt;/p&gt;

&lt;p&gt;(참고로 여기서 나오는 $P(x_1, x_2, …, x_n|\theta)$에서 $|$는 조건부 확률을 나타내는 기호가 아니다. $|$ 뒤에 있는 기호들이 모델의 모수라는 것을 강조하기 위해 사용되는 기호에 불과하다. $P(x_1, x_2, …, x_n;\theta)$으로 표현하기도 한다.)&lt;/p&gt;

&lt;p&gt;이 때 확률변수가 iid(independent and identically distributed)라면 가능도는 다음과 같이 표현할 수도 있다.&lt;/p&gt;

&lt;p&gt;$L(\theta)=\prod_{i=1}^{n}P(x_i|\theta)$&lt;/p&gt;

&lt;p&gt;가능도를 최대로 만드는 $\theta_{MLE}$는 다음과 같다.&lt;/p&gt;

&lt;p&gt;$\theta_{MLE}=argmax_{\theta}\prod_{i=1}^nP(x_i|\theta)$&lt;/p&gt;

&lt;p&gt;이 때 확률값은 1보다 작기 때문에 계속 곱하면 그 값이 지나치게 작아져 언더플로우(underflow) 문제가 발생하므로 로그를 취한다. 이 때 로그를 취하더라도 가능도를 최대로 만드는 $\theta$가 전과 동일하다는 보장이 있어야 하는데, 로그는 단조증가(monotonically increasing)한다는 성질이 있기 때문에 로그를 취하더라도 가능도를 최대로 만드는 $\theta$는 변하지 않는다.&lt;/p&gt;

&lt;p&gt;$\theta_{MLE}=argmax_{\theta}\sum_{i=1}^nlogP(x_i|\theta)$&lt;/p&gt;

&lt;h2 id=&quot;예시&quot;&gt;예시&lt;/h2&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/Dn6b9fCIUpM&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XepXtl9YKwc&quot;&gt;StatQuest: Maximum Likelihood, clearly explained!!!&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=Dn6b9fCIUpM&quot;&gt;Maximum Likelihood For the Normal Distribution, step-by-step!&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://ratsgo.github.io/statistics/2017/09/23/MLE/&quot;&gt;최대우도추정(Maximum Likelihood Estimation)&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">확률(Probability)과 가능도(Likelihood)의 차이</title><link href="http://localhost:4000/posts/2020/05/02/probability_likelihood" rel="alternate" type="text/html" title="확률(Probability)과 가능도(Likelihood)의 차이" /><published>2020-05-02T00:00:00-07:00</published><updated>2020-05-02T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/02/probability_likelihood</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/02/probability_likelihood">&lt;iframe src=&quot;https://www.youtube.com/embed/pYxNSUDSFH4&quot;&gt; &lt;/iframe&gt;

&lt;p&gt;확률과 가능도의 차이를 가장 직관적으로 설명한 영상이라고 생각한다. 한번 보면 감이 올 것이다.&lt;/p&gt;

&lt;p&gt;영상에서는 확률과 가능도를 다음과 같이 요약한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80852722-b4230180-8c65-11ea-9d9f-922949fad27c.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;내-나름의-정리&quot;&gt;내 나름의 정리&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;확률 : 분포를 알고 있을 때, 전체 관측값에서 우리가 관심있는 관측값이 차지하는 비중&lt;/li&gt;
  &lt;li&gt;가능도 : 분포를 모를 때, 분포 $\theta$가 관측값을 얼마나 잘 설명하는지 수치화한 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw&quot;&gt;StatQuest with Josh Starmer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability&quot;&gt;What is the difference between “likelihood” and “probability”?
&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Neural Collaborative Filtering</title><link href="http://localhost:4000/paper_review/2020/04/24/Neural_Collaborative_Filtering" rel="alternate" type="text/html" title="Neural Collaborative Filtering" /><published>2020-04-24T00:00:00-07:00</published><updated>2020-04-24T00:00:00-07:00</updated><id>http://localhost:4000/paper_review/2020/04/24/Neural_Collaborative_Filtering</id><content type="html" xml:base="http://localhost:4000/paper_review/2020/04/24/Neural_Collaborative_Filtering">&lt;h1 id=&quot;요약&quot;&gt;요약&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Implicit Feedback을 활용한 Collaborative Filtering을 다룬다.&lt;/li&gt;
  &lt;li&gt;output이 1이면 User와 Item은 연관성이 있는 것이며, 0이면 User와 Item은 연관성이 없는 것이다. 즉, binary classification 문제이다.&lt;/li&gt;
  &lt;li&gt;Collaborative Filtering에서 Matrix Factorization은 많이 쓰이는 기법이다.&lt;/li&gt;
  &lt;li&gt;기존의 Matrix Factorization은 User Latent Factor와 Item Latent Factor를 구하고, 두 Latent Factor를 내적하는 방법을 통해 Rating Matrix를 복원한다.&lt;/li&gt;
  &lt;li&gt;하지만 이 방법은 User-Item Interaction을 충분히 표현하지 못한다.&lt;/li&gt;
  &lt;li&gt;저자는 &lt;strong&gt;내적&lt;/strong&gt;이라는 지나치게 단순한 방법으로 Rating Matrix를 복원했기 때문이라고 본다.&lt;/li&gt;
  &lt;li&gt;대안으로 &lt;strong&gt;뉴럴 네트워크&lt;/strong&gt;의 사용을 권한다.&lt;/li&gt;
  &lt;li&gt;뉴럴 네트워크를 도입함으로써 Matrix Factorization이 가지는 Linearity와 뉴럴 네트워크가 가지는 Non-Linearity를 모두 가질 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;문제-제시&quot;&gt;문제 제시&lt;/h1&gt;
&lt;p&gt;기존의 Matrix Factorization은 SGD나 ALS 등의 방법을 사용해서 User-Item Interaction($M\times N$)을 User Latent Factor($M\times K$)와 Item Latent Factor($K\times N$)로 분해한다(K « M, N).&lt;/p&gt;

&lt;p&gt;그 후 두 Latent Factor를 내적해 Rating Matrix를 복원하는 방법을 통해 User가 Item을 얼마나 선호하는지 등을 예측한다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499831-6a59c300-89a8-11ea-9e96-de5ff1654354.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;내적의 의미를 생각해보면 내적을 한다는 것은 User Latent Factor와 Item Latent Factor를 선형 결합해서 Rating Matrix를 복원한다는 것이다(Linear). 하지만 Linear한 방법은 단순한 예제에서는 잘 작동하지만, User-Item Interaction과 같이 복잡한 관계를 가지는 것을 잘 표현하지 못한다는 단점이 있다.&lt;/p&gt;

&lt;h1 id=&quot;대안-제시&quot;&gt;대안 제시&lt;/h1&gt;
&lt;p&gt;저자는 이 문제를 해결하기 위해 두 가지 방법이 있다고 말한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Latent Factor의 Dimension(K)을 늘리는 것&lt;/li&gt;
  &lt;li&gt;Non-Linear한 방법을 통해 모델 구축&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;둘 다 좋은 방법이지만 1번 방법의 경우 오버피팅이 발생한다는 문제가 있어 2번 방법을 채택했으며, Non-Linearity를 추가하기 위한 방법으로 Neural Network를 도입한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;뉴럴-네트워크-도입으로-인해-달라지는-것들&quot;&gt;뉴럴 네트워크 도입으로 인해 달라지는 것들&lt;/h1&gt;

&lt;h2 id=&quot;1-embedding-layer&quot;&gt;1. Embedding Layer&lt;/h2&gt;
&lt;p&gt;Embedding Layer를 한 문장으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Turns positive integers (indexes) into dense vectors of fixed size.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, categorical한 input을 고정된 사이즈의 dense한 벡터로 바꿔주는 Layer이다. 이 논문에서는 User Latent Factor와 Item Latent Factor를 표현하는 데 사용한다.&lt;/p&gt;
&lt;h2 id=&quot;2-필요한-모델&quot;&gt;2. 필요한 모델&lt;/h2&gt;
&lt;h3 id=&quot;0-neural-collaborative-filtering-framework&quot;&gt;(0) Neural Collaborative Filtering Framework&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499827-69c12c80-89a8-11ea-920a-969720de8bec.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒤에 나오는 모델들의 뼈대가 되는 것이다. 다음과 같은 과정을 거친다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Input Layer&lt;/li&gt;
  &lt;li&gt;Embedding Layer&lt;/li&gt;
  &lt;li&gt;Neural CF Layers&lt;/li&gt;
  &lt;li&gt;Output Layer&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1-mlpmulti-layer-perceptron&quot;&gt;(1) MLP(Multi-Layer Perceptron)&lt;/h3&gt;

&lt;p&gt;앞의 Neural Collaborative Filtering Framework을 충실히 재현한 모델이다. User Latent Factor와 Item Latent Factor를 여러 Layer를 거치게 해 결과를 낸다.(Layer의 activation function을 relu로 해 Non-Linearity를 얻는다.)&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499829-6a59c300-89a8-11ea-97fb-11ba105acc76.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$W_x$ : weight matrix for x-th layer&lt;br /&gt;
$b_x$ : bias vector for x-th layer&lt;br /&gt;
$a_x$ : activation function for x-th layer&lt;br /&gt;
$h$ : edge weights of the output layer&lt;/p&gt;

&lt;p&gt;수식에서 볼 수 있듯이 첫 번째 layer는 User Latent Factor와 Item Latent Factor를 Concatenate한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;2-gmfgeneralized-matrix-factorization&quot;&gt;(2) GMF(Generalized Matrix Factorization)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499823-69289600-89a8-11ea-959a-3d6377197ead.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 앞에서 본 Neural Collaborative Filtering Framework의 특별한 케이스 중 하나를 다룰 것이다.&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499822-688fff80-89a8-11ea-8ca8-fa287685f3d2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$a_{out}$ : activation function&lt;br /&gt;
$h$ : edge weights of the output layer&lt;br /&gt;
$\odot$ : element-wise product&lt;/p&gt;

&lt;p&gt;만약 activation function으로 non-linear한 함수를 사용하면 non-linear한 	모델로도 사용할 수 있다. 하지만 논문에서 저자는 GMF 모델이 Linearity를 가지게 한다(저자가 작성한 &lt;a href=&quot;https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/GMF.py&quot;&gt;코드&lt;/a&gt;를 보면 확인할 수 있다).&lt;/p&gt;

&lt;p&gt;$a_{out}$이 identity function($y=x$)이며, $h$가 1로 이루어진 벡터라면, Matrix Factorization과 똑같아진다.&lt;/p&gt;

&lt;p&gt;실제로는 $a_{out}$과 $h$를 없애고, $p_u^G\odot p_u^M$만을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;3-neumfneural-matrix-factorization&quot;&gt;(3) NeuMF(Neural Matrix Factorization)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499819-67f76900-89a8-11ea-9f46-525a6837a031.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NeuMF는 GMF와 MLP에서 얻은 최종 Layer를 Concatenate하여 결과를 낸다. GMF와 MLP를 결합해 Linearity와 Non-Linearity 모두를 얻고자 하는 모델이다.&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499815-67f76900-89a8-11ea-984f-43480da9eb07.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$p_u^G$ : User Embedding for GMF&lt;br /&gt;
$p_u^M$ : User Embedding for MLP&lt;br /&gt;
$q_i^G$ : Item Embedding for GMF&lt;br /&gt;
$q_i^M$ : Item Embedding for MLP&lt;/p&gt;

&lt;h1 id=&quot;결과&quot;&gt;결과&lt;/h1&gt;
&lt;h2 id=&quot;1-다른-모델과의-비교&quot;&gt;1. 다른 모델과의 비교&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499809-675ed280-89a8-11ea-8e8d-49c6c72d7ea3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-pre-training의-성과&quot;&gt;2. Pre-training의 성과&lt;/h2&gt;
&lt;p&gt;Pre-training에 대한 내용은 뒤에 나온다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499805-662da580-89a8-11ea-97a6-29afd932fd3d.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-3가지-모델의-성능-비교&quot;&gt;3. 3가지 모델의 성능 비교&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499803-65950f00-89a8-11ea-925d-f29d1bca75af.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-negative-sample의-수에-따른-성능-비교&quot;&gt;4. Negative Sample의 수에 따른 성능 비교&lt;/h2&gt;
&lt;p&gt;Negative Sampling에 대한 내용은 뒤에 나온다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499801-6463e200-89a8-11ea-9ad3-1ff6c31a7290.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-mlp-layer의-수에-따른-성능-비교&quot;&gt;5. MLP Layer의 수에 따른 성능 비교&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499792-62018800-89a8-11ea-92d8-3ed1cd21bbf9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;기타&quot;&gt;기타&lt;/h1&gt;
&lt;h2 id=&quot;1-pre-training&quot;&gt;1. Pre-training&lt;/h2&gt;
&lt;p&gt;NCF의 Loss Function은 non-convex하기 때문에 GD를 사용하면 Local Optimum에 수렴할 가능성이 있다. 이러한 경우 초기값을 어떻게 주는지가 딥러닝 모델의 성능에 큰 영향을 미친다. 저자는 미리 훈련된(pretrained) GMF, MLP 모델을 사용하여 NeuMF의 weights를 초기화하는 방식을 제안한다.&lt;/p&gt;

&lt;p&gt;방법은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;GMF와 MLP 모델을 수렴할 때 까지 훈련시킨다.&lt;/li&gt;
  &lt;li&gt;GMF와 MLP의 파라미터를 NeuMF의 해당하는 부분에 초기값으로 대입한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2-how-to-deal-with-absence-of-negative-feedback&quot;&gt;2. How to deal with Absence of Negative Feedback&lt;/h2&gt;
&lt;p&gt;Implicit Feedback을 활용한 추천 시스템은 아마 unobserved data를 어떻게든 해결해야 할 것이다. 이 문제는 Explicit Feedback과 비교하면 명백하게 알 수 있다.&lt;/p&gt;

&lt;p&gt;점수가 1~5점 사이에 분포해 있는 Explicit Feedback의 경우 1점은 “유저가 아이템을 좋아하지 않는다”는 것을 나타내며, 5점은 “유저가 아이템을 좋아한다”는 것을 나타낸다.&lt;/p&gt;

&lt;p&gt;하지만 Implicit Feedback의 경우 0점을 기록(ex. 유저가 해당 아이템을 구매한 적이 없음)하더라도 “유저가 해당 아이템을 좋아하지 않는다”고 말할 수 없다. 유저가 실제로 아이템을 좋아하지 않는 경우 이외에도 유저가 아이템을 인지하지 못했던 경우가 있기 때문이다. 이러한 문제를 부정적인 피드백의 부재(absence of negative feedback)라고 한다.&lt;/p&gt;

&lt;p&gt;만약 부정적인 피드백를 모두 포함시켜 학습을 시킨다면 부정적인 피드백의 수가 긍정적인 피드백보다 훨씬 많아 긍정적인 피드백 무시하는 방향으로 학습이 진행될 것이다. 반대로 부정적인 피드백를 모두 제외시킨다면 긍정적인 피드백만 학습이 진행되어 우리의 최종 목표인 binary classification을 수행하지 못할 것이다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위한 두 가지 방법을 제안한다. 하나는 이번 논문인 Neural Collaborative Filtering에서 사용한 방법이고, 다른 하나는 Collaborative Filtering for Implicit Feedback Datasets이라는 논문에서 사용한 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;1-negative-sampling&quot;&gt;1. Negative Sampling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Neural Collaborative Filtering&lt;/li&gt;
  &lt;li&gt;유저가 긍정적인 피드백을 준 것 이외에 유저가 아직 피드백을 주지 않은 k개의 아이템을 포함시켜 Training Set을 구성한다.(k는 하이퍼 파라미터이며, 이 논문에서는 $k=4$)&lt;/li&gt;
  &lt;li&gt;긍정적인 피드백이 $n$개라면 Training Set은 $n(k+1)$개가 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback의 일부만 샘플링해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-confidence-level&quot;&gt;2. Confidence Level&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/li&gt;
  &lt;li&gt;Confidence Level이라는 개념을 도입한다. 유저가 아이템과 연관성이 있는지 정도를 수치적으로 표현하기 위한 것이다.&lt;/li&gt;
  &lt;li&gt;$c_{ui}=1+\alpha r_{ui}$로 계산할 수 있다. $c_ui$가 클수록 유저와 아이템은 연관성이 큰 것이다.&lt;/li&gt;
  &lt;li&gt;$r_{ui}$는 긍정적인 피드백의 수이며, $r_{ui}$가 증가할수록 $c_ui$도 커진다. (증가하는 정도를 나타낸 것이 $\alpha$다. $\alpha$는 하이퍼 파라미터이며, 이 논문에서는 $\alpha=40$)&lt;/li&gt;
  &lt;li&gt;$c_{ui}$는 모두 1 이상의 값을 갖게 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback도 모두 사용해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-loss-function&quot;&gt;3. Loss Function&lt;/h2&gt;
&lt;p&gt;이 모델에서 output은 0과 1이므로 Bernoulli Distribution을 따른다.&lt;/p&gt;

&lt;p&gt;베르누이 분포에서 가능도는 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80507963-659a0c80-89b2-11ea-9c1a-a85714fa00d1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가능도의 negative logarithm은 다음과 같다. 이 식을 활용해서 loss를 최소화하는 방향으로 학습을 진행시킬 수 있다(식이 binary cross-entropy와 같다).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80507967-6763d000-89b2-11ea-8d63-b4fc7d23c660.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05031.pdf&quot;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://yifanhu.net/PUB/cf.pdf&quot;&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;review&quot;]" /><summary type="html">요약 Implicit Feedback을 활용한 Collaborative Filtering을 다룬다. output이 1이면 User와 Item은 연관성이 있는 것이며, 0이면 User와 Item은 연관성이 없는 것이다. 즉, binary classification 문제이다. Collaborative Filtering에서 Matrix Factorization은 많이 쓰이는 기법이다. 기존의 Matrix Factorization은 User Latent Factor와 Item Latent Factor를 구하고, 두 Latent Factor를 내적하는 방법을 통해 Rating Matrix를 복원한다. 하지만 이 방법은 User-Item Interaction을 충분히 표현하지 못한다. 저자는 내적이라는 지나치게 단순한 방법으로 Rating Matrix를 복원했기 때문이라고 본다. 대안으로 뉴럴 네트워크의 사용을 권한다. 뉴럴 네트워크를 도입함으로써 Matrix Factorization이 가지는 Linearity와 뉴럴 네트워크가 가지는 Non-Linearity를 모두 가질 수 있다.</summary></entry><entry><title type="html">Recommender System for Ecommerce</title><link href="http://localhost:4000/project/2020/04/23/Recommender_System_for_Ecommerce" rel="alternate" type="text/html" title="Recommender System for Ecommerce" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>http://localhost:4000/project/2020/04/23/Recommender_System_for_Ecommerce</id><content type="html" xml:base="http://localhost:4000/project/2020/04/23/Recommender_System_for_Ecommerce"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><summary type="html"></summary></entry><entry><title type="html">확률 및 통계 7강</title><link href="http://localhost:4000/lecture/2020/04/16/prob_stat_07" rel="alternate" type="text/html" title="확률 및 통계 7강" /><published>2020-04-16T00:00:00-07:00</published><updated>2020-04-16T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/16/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%207%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/16/prob_stat_07">&lt;h2 id=&quot;chebyshev-inequality&quot;&gt;Chebyshev Inequality&lt;/h2&gt;

&lt;p&gt;$P(|X-E[X]|\ge a)\le {\sigma_X^2 \over a^2}$&lt;/p&gt;

&lt;p&gt;어떤 랜덤한 $X$와 $X$의 평균의 차이의 정도를 $a$라 보고, $a$보다 차이가 더 많이 날 확률은 ${\sigma_X^2 \over a^2}$보다 작다.($\sigma_X^2$는 X의 분산)&lt;/p&gt;

&lt;p&gt;즉, 임의로 선택한 $X$는 $X$의 평균과 차이를 보이기 마련인데, 우리가 예측한 것(a)보다 더 크게 차이를 보일 확률은 얼마나 되는지 수치적으로 표현한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;chapter-4-special-distribution&quot;&gt;Chapter 4. Special Distribution&lt;/h1&gt;
&lt;h2 id=&quot;42-bernoulli-distribution베르누이-분포&quot;&gt;4.2 Bernoulli Distribution(베르누이 분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RV X : binary&lt;/li&gt;
  &lt;li&gt;보통 하나를 success, 다른 하나를 failure라고 한다.&lt;/li&gt;
  &lt;li&gt;$P(success) = p, P(failure) = 1-p$&lt;/li&gt;
  &lt;li&gt;보통 success한 경우를 1로 두고, failure한 경우를 0으로 둔다.&lt;/li&gt;
  &lt;li&gt;$E[X]=p$&lt;/li&gt;
  &lt;li&gt;$\sigma_X^2=p(1-p)$&lt;/li&gt;
  &lt;li&gt;Bernoulli Distribution를 바탕으로 많은 Distribution이 만들어진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;43-binomial-distribution이항분포&quot;&gt;4.3 Binomial Distribution(이항분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RV X : number of successes out of n Bernoulli trails(베르누이 시행을 n 번 반복해서 성공한 횟수)&lt;/li&gt;
  &lt;li&gt;$x=0,1,2,…,n$. 즉, discrete하다.&lt;/li&gt;
  &lt;li&gt;$P_X(x)={n \choose x}p^x(1-p)^{n-x}$&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$\sum_{x=0}^nP_X(x)=\sum_{x=0}^n{n \choose x}p^x(1-p)^{n-x}=(p+(1-p))^n=1(\because (a+b)^n=\sum_{x=0}^n{n \choose x}a^xb^{n-x})$&lt;/li&gt;
  &lt;li&gt;$E[X]=\sum_{x=0}^nx{n \choose x}p^x(1-p)^{n-x}=np$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{xn! \over (n-x)!x!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n! \over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n(n-1)! \over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n(n-1)!p \over (n-1-(x-1))!(x-1)!}p^{x-1}(1-p)^{(n-1-(x-1))}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n(n-1)!p \over (n-1-(x-1))!(x-1)!}p^{x-1}(1-p)^{(n-1-(x-1))}$&lt;br /&gt;
$\qquad x’=x-1$로 치환&lt;br /&gt;
$\qquad =\sum_{x=0}^{n-1}np{(n-1)!px’(1-p)^{n-1-x’} \over (n-1-x’)!(x’)!}$&lt;br /&gt;
$\qquad =np(p+(1-p))^{n-1}$&lt;br /&gt;
$\qquad =np$&lt;/li&gt;
  &lt;li&gt;$\sigma_X^2[X]=E[X^2]-(E[X])^2$&lt;br /&gt;
$E[X^2]=\sum_{x=0}^n{x^2n!\over (n-x)!x!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=1}^n{xn!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=1}^n{(x-1)n!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}+\sum_{x=1}^n{n!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=1}^n{(x-1)n!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}+np$&lt;br /&gt;
$\qquad =\sum_{x=2}^n{n!\over (n-x)!(x-2)!}p^x(1-p)^{n-x}+np$&lt;br /&gt;
$\qquad =\sum_{x=2}^n{n(n-1)(n-2)!\over ((n-2)-(x-2))!(x-2)!}p^2p^{x-2}(1-p)^{(n-x)-(x-2)}+np$&lt;br /&gt;
$\qquad x’=x-2$로 치환&lt;br /&gt;
$\qquad =\sum_{x=2}^n{n-2 \choose x’}p^{x’}(1-p)^{n-2-x’}n(n-1)p^2+np$&lt;br /&gt;
$\qquad =(p+(1-p))^{n-2}n(n-1)p^2+np$&lt;br /&gt;
$\qquad \therefore \sigma_X^2=E[X^2]-n^2p^2$&lt;br /&gt;
$\qquad =np(1-p)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;44-geometrid-distribution기하분포&quot;&gt;4.4 Geometrid Distribution(기하분포)&lt;/h2&gt;
&lt;p&gt;RV X : Number of Bernoulli trials until first success&lt;br /&gt;
$P_X(x)=(1-p)^{x-1}p$, $x=1,2,3,…$&lt;br /&gt;
$E[X]={1\over p}$&lt;br /&gt;
$\sigma_X^2={1-p\over p^2}$&lt;/p&gt;

&lt;h3 id=&quot;forgetfulnessmemoryless&quot;&gt;Forgetfulness(Memoryless)&lt;/h3&gt;
&lt;p&gt;6이 나올 때 까지 주사위를 던지는 확률 변수라고 하자.&lt;br /&gt;
10번을 던졌는데 6이 안나왔다. 이 경우에 다섯 번을 더 던져 6이 나올 확률은 얼마일까? 이것은 기하분포 공식을 통해 계산해낼 수 있다. $(1-p)^4p$이다.&lt;br /&gt;
이번에는 15번을 던졌는데 6이 안나왔다. 이 경우에도 다섯 번을 더 던져 6이 나올 확률은 얼마인지 계산해보자. $(1-p)^4p$이다.&lt;br /&gt;
이 예시는 기하 분포가 과거 기록에 영향을 받지 않는다는 것을 보여준다. 이런 성질을 Forgetfulness라고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Consider K additional trials until the first success, given n trials fail.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$P(X=n+k|X&amp;gt;n)$&lt;br /&gt;
$={P(X=n+k\cap X&amp;gt;n)\over P(X&amp;gt;n)}$&lt;br /&gt;
$={P(X=n+k)\over P(X&amp;gt;n)}$&lt;br /&gt;
$={(1-p)^{n+k-1}p\over \sum_{x=n+1}^{\infty}(1-p)^{x-1}p}$&lt;br /&gt;
$={(1-p)^{n+k-1}p\over {p(1-p)^n\over 1-(1-p)}}$&lt;br /&gt;
$=p(1-p)^{k-1}$&lt;br /&gt;
$=P(X=k)$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">Chebyshev Inequality</summary></entry><entry><title type="html">Collaborative Filtering for Implicit Feedback Datasets</title><link href="http://localhost:4000/paper_review/2020/04/16/Collaborative_Filtering_for_Implicit_Feedback_Datasets" rel="alternate" type="text/html" title="Collaborative Filtering for Implicit Feedback Datasets" /><published>2020-04-16T00:00:00-07:00</published><updated>2020-04-16T00:00:00-07:00</updated><id>http://localhost:4000/paper_review/2020/04/16/Collaborative_Filtering_for_Implicit_Feedback_Datasets</id><content type="html" xml:base="http://localhost:4000/paper_review/2020/04/16/Collaborative_Filtering_for_Implicit_Feedback_Datasets"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;review&quot;]" /><summary type="html"></summary></entry><entry><title type="html">미분적분학 1강</title><link href="http://localhost:4000/lecture/2020/04/14/calculus_01" rel="alternate" type="text/html" title="미분적분학 1강" /><published>2020-04-14T00:00:00-07:00</published><updated>2020-04-14T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/14/%EB%AF%B8%EB%B6%84%EC%A0%81%EB%B6%84%ED%95%99%201%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/14/calculus_01">&lt;h2 id=&quot;1-함수의-정의&quot;&gt;1. 함수의 정의&lt;/h2&gt;
&lt;h3 id=&quot;함수의-정의&quot;&gt;함수의 정의&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$f:X\to Y$&lt;/li&gt;
  &lt;li&gt;X라는 집합에 있는 원소를 어떻게 Y로 보내느냐, 그 규칙을 결정하는 것이 함수&lt;/li&gt;
  &lt;li&gt;X를 정의역, Y를 공역이라 한다. 즉, $x \to f(x)=y$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;함수의-조건&quot;&gt;함수의 조건&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;모든 x가 y로 가야 한다.&lt;/li&gt;
  &lt;li&gt;각 x는 하나의 y로만 가야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;예시&quot;&gt;예시&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;아래 함수들은 앞으로의 예시에서 계속 등장하니 자주 참고해야 한다.&lt;/li&gt;
  &lt;li&gt;정의역 X는 실수 전체&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$y=e^x$&lt;/li&gt;
  &lt;li&gt;$y=lnx$&lt;/li&gt;
  &lt;li&gt;$y=sinx$&lt;/li&gt;
  &lt;li&gt;$y=x^3$&lt;/li&gt;
  &lt;li&gt;$x^2+y^2=1$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371682-3e1d4400-88cd-11ea-821f-c97ee6594d23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2번 : 함수가 아니다. $x&amp;lt;0$에서 y값을 대응시킬 수 없다. 즉, 1번 조건을 만족시키지 못한다. 단, 정의역을 $x&amp;gt;0$이라고 제한한다면 함수라고 할 수 있다.&lt;/li&gt;
  &lt;li&gt;5번 : 하나의 x가 두 개의 y를 가질 수 있다. 즉, 2번 조건을 만족시키지 못한다. 단, $y&amp;gt;=0$이라고 제한한다면 함수라고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-일대일-대응함수&quot;&gt;2. 일대일 대응함수&lt;/h2&gt;
&lt;h3 id=&quot;조건&quot;&gt;조건&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;one-to-one : y축과 평행한 선을 그어 한 점과만 만나야 한다.&lt;/li&gt;
  &lt;li&gt;onto : 치역과 공역이 같아야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;3번 : 일대일 대응이 아니다. y축과 평행한 선을 그어보면 여러 점에서 만난다. 즉, 1번 조건을 만족시키지 못한다.&lt;/li&gt;
  &lt;li&gt;1번 : $y&amp;gt;0$에서만 정의된다. 즉, onto 조건을 만족시키지 못한다. 단, $y&amp;gt;0$이라는 조건이 있다면 onto 조건을 만족시킨다.&lt;/li&gt;
  &lt;li&gt;4번 : 일대일 대응 조건을 만족&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;참고&quot;&gt;참고&lt;/h3&gt;
&lt;p&gt;x값을 제한하면 one-to-one 조건을 만족할 수 있도록 만들 수 있으며, y값을 제한하면 onto 조건을 만족할 수 있도록 만들 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;일대일-대응의-의미&quot;&gt;일대일 대응의 의미&lt;/h3&gt;
&lt;p&gt;$y=f(x)$가 일대일 대응이다 $\Leftrightarrow$ 역함수 존재&lt;/p&gt;

&lt;h2 id=&quot;3-역함수와-도함수&quot;&gt;3. 역함수와 도함수&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$y=f(x)$의 역함수 : $x=f(y) \Leftrightarrow y=f^{-1}(x)$&lt;/li&gt;
  &lt;li&gt;X가 치역, Y가 정의역이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---yex&quot;&gt;예제 - $y=e^x$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $-\infty &amp;lt; x &amp;lt; \infty, y&amp;gt;0$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=e^y\Leftrightarrow y=lnx$  ($-\infty &amp;lt; y &amp;lt; \infty, x&amp;gt;0$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=e^x$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=lnx \Rightarrow y={1 \over x}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---ysinx&quot;&gt;예제 - $y=sinx$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $-1 \le y \le 1, -{\pi \over 2} \le x \le {\pi \over 2}$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=siny \Leftrightarrow y=arcsinx$  ($-1 \le x \le 1, -{\pi \over 2} \le y \le {\pi \over 2}$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=cosx$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=arcsinx \Rightarrow y’={1 \over \sqrt{1-x^2}}$&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371685-3f4e7100-88cd-11ea-9f23-a9d189454215.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$y=arcsinx \Rightarrow y’={1 \over \sqrt{1-x^2}}$&lt;br /&gt;
이 식은 어떻게 도출된 것일까?&lt;br /&gt;
$y=arcsinx\Leftrightarrow x=siny$
$\Rightarrow 1=cosy \cdot y’$(양 변을 $x$로 미분)&lt;br /&gt;
$\Rightarrow y’={1\over cosy}$&lt;br /&gt;
$={1\over \sqrt{1-sin^2y}}$($\because sin^2x+cos^2x=1$)&lt;br /&gt;
$={1\over \sqrt{1-x^2}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---ycosx&quot;&gt;예제 - $y=cosx$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $0\le x\le \pi, -1 \le y \le 1$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=cosy \Leftrightarrow y=arccosx$  ($0\le y\le \pi, -1 \le x \le 1$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=-sinx$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=arccosx \Rightarrow y’=-{1 \over \sqrt{1-x^2}}$&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371674-3c538080-88cd-11ea-9101-e466cda44a4c.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$y=arccosx \Rightarrow y’=-{1 \over \sqrt{1-x^2}}$&lt;br /&gt;
이 식은 어떻게 도출된 것일까?&lt;br /&gt;
$y=arccosx\Leftrightarrow x=cosy$
$\Rightarrow 1=-siny \cdot y’$(양 변을 $x$로 미분)&lt;br /&gt;
$\Rightarrow y’=-{1\over siny}$&lt;br /&gt;
$=-{1\over \sqrt{1-cos^2y}}$($\because sin^2x+cos^2x=1$)&lt;br /&gt;
$=-{1\over \sqrt{1-x^2}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---ytanx&quot;&gt;예제 - $y=tanx$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $-{1\over 2}\pi \le x\le {1\over 2}\pi$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=tany \Leftrightarrow y=arctanx$  ($-{1\over 2}\pi \le y\le {1\over 2}\pi$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=sec^2x$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=arctanx \Rightarrow y’={1 \over 1+x^2}$&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371671-3b225380-88cd-11ea-87c3-72991eb83b3f.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$y=arctanx \Rightarrow y’={1 \over 1+x^2}$&lt;br /&gt;
이 식은 어떻게 도출된 것일까?&lt;br /&gt;
$y=arctanx \Leftrightarrow x=tany$
$\Rightarrow 1=sec^2y \cdot y’$(양 변을 $x$로 미분)&lt;br /&gt;
$\Rightarrow y’={1\over sec^2y}$&lt;br /&gt;
$={1\over \sqrt{1+tan^2y}}$($\because sec^2x-tan^2x=1$)&lt;br /&gt;
$={1\over \sqrt{1+x^2}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;예제---역함수와-도함수&quot;&gt;예제 - 역함수와 도함수&lt;/h2&gt;

&lt;h2 id=&quot;4-쌍곡선함수와-그-도함수&quot;&gt;4. 쌍곡선함수와 그 도함수&lt;/h2&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="미분적분학" /><summary type="html">1. 함수의 정의 함수의 정의 $f:X\to Y$ X라는 집합에 있는 원소를 어떻게 Y로 보내느냐, 그 규칙을 결정하는 것이 함수 X를 정의역, Y를 공역이라 한다. 즉, $x \to f(x)=y$</summary></entry><entry><title type="html">확률 및 통계 6강</title><link href="http://localhost:4000/lecture/2020/04/14/prob_stat_06" rel="alternate" type="text/html" title="확률 및 통계 6강" /><published>2020-04-14T00:00:00-07:00</published><updated>2020-04-14T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/14/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%206%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/14/prob_stat_06">&lt;h2 id=&quot;예시---geometric-distribution기하분포&quot;&gt;예시 - Geometric Distribution(기하분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RV K: Number of trials until first success&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예시---tossing-a-dice-until-first-6&quot;&gt;예시 - Tossing a dice until first 6&lt;/h3&gt;
&lt;p&gt;p: 6이 나올 확률&lt;br /&gt;
(1-p): 6 이외의 수가 나올 확률&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;첫 번째 시도에서 성공&lt;br /&gt;
$k=1, p$&lt;/li&gt;
  &lt;li&gt;두 번째 시도에서 성공&lt;br /&gt;
$k=2, (1-p)p$&lt;/li&gt;
  &lt;li&gt;세 번째 시도에서 성공&lt;br /&gt;
$k=3, (1-p)(1-p)p$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;즉, $P_K(k)=(1-p)^{k-1}p\quad(k=1,2,3,4,…)$&lt;/p&gt;

&lt;h3 id=&quot;mean&quot;&gt;Mean&lt;/h3&gt;
&lt;p&gt;$E[K]=\sum_{k=1}^{\infty}kp(1-p)^{k-1}$&lt;br /&gt;
$\sum_{k=1}^{\infty}(1-p)^{k}={1-p \over p}$&lt;br /&gt;
${d \over dp}\sum_{k=1}^{\infty}(1-p)^{k}={d \over dp}({1-p \over p})$&lt;br /&gt;
$=-\sum_{k=1}^{\infty}k(1-p)^{k-1}=-{1 \over p^2}$&lt;br /&gt;
$=\sum_{k=1}^{\infty}k(1-p)^{k-1}={1 \over p^2}$&lt;br /&gt;
$\therefore E[K]=p{1 \over p^2}={1 \over p}$&lt;/p&gt;

&lt;h3 id=&quot;variance&quot;&gt;Variance&lt;/h3&gt;
&lt;p&gt;$\sigma_k^2=E[K^2]-\mu^2$&lt;br /&gt;
$E[K^2]=\sum_{k=1}^{\infty}k^2p(1-p)^{k-1}$&lt;br /&gt;
$\Rightarrow -{d \over dp}\sum_{k=1}^{\infty}k(1-p)^{k-1}=-{d \over dp}{1 \over p^2}$(Mean 구하는 공식에서 가져옴)&lt;br /&gt;
$\qquad=-\sum_{k=1}^{\infty}k(k-1)(1-p)^{k-2}=-{2\over p^3}$&lt;br /&gt;
$\qquad=\sum_{k=1}^{\infty}k(k-1)(1-p)^{k-2}={2\over p^3}$&lt;br /&gt;
$\Rightarrow \sum_{k=1}^{\infty}k^2(1-p)^{k-2}=\sum_{k=1}^{\infty}k(1-p)^{k-2}+{2\over p^3}$&lt;br /&gt;
$\qquad =\sum_{k=1}^{\infty}k^2(1-p)^{k-2}p(1-p)=(\sum_{k=1}^{\infty}k(1-p)^{k-2}+{2\over p^3})p(1-p)$&lt;br /&gt;
$\qquad =\sum_{k=1}^{\infty}k(1-p)^{k-1}p+{2(1-p)\over p^2}$&lt;br /&gt;
$\qquad ={1\over p}+{2(1-p)\over p^2}$&lt;br /&gt;
$\therefore \sigma_k^2=E[K^2]-\mu^2={1\over p}+{2(1-p)\over p^2}-{1\over p^2}={1-p \over p^2}$&lt;/p&gt;

&lt;h2 id=&quot;conditional-mean&quot;&gt;Conditional Mean&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$E[X|A]=\sum_{x_i\in A}x_ip(x_i|A)$&lt;/li&gt;
  &lt;li&gt;A의 조건을 만족하는 변수값들에 대해서만 평균값을 구한 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;이산확률변수의 경우&lt;/strong&gt;&lt;br /&gt;
$\sum_{x_i\in A}x_i{p(x_i \cap A) \over p(A)}$&lt;br /&gt;
$\qquad =\sum_{x_i\in A}x_i{p(x_i) \over p(A)}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;연속확률변수의 경우&lt;/strong&gt;&lt;br /&gt;
$E[X|A]=\int_{x\in A}xf_X(x|A)$
연속형이기 때문에 density를 구해줘야 하는데, 이 경우에는 conditional density($f_X(x|A)$)를 구해야 한다.&lt;br /&gt;
이것을 바로 구할 수는 없고, conditional CDF를 활용해 구한다.&lt;br /&gt;
$f_X(x|A)={d \over dx}F_X(x|A)$&lt;br /&gt;
$\qquad\qquad ={d \over dx}P(X\le x|A)$&lt;br /&gt;
$\qquad\qquad ={d \over dx}{P(X\le x\cap A)\over P(A)}$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">예시 - Geometric Distribution(기하분포) RV K: Number of trials until first success</summary></entry><entry><title type="html">Taylor Series</title><link href="http://localhost:4000/posts/2020/04/13/taylor_series" rel="alternate" type="text/html" title="Taylor Series" /><published>2020-04-13T00:00:00-07:00</published><updated>2020-04-13T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/04/13/taylor_series</id><content type="html" xml:base="http://localhost:4000/posts/2020/04/13/taylor_series">&lt;h2 id=&quot;테일러-급수&quot;&gt;테일러 급수&lt;/h2&gt;
&lt;p&gt;테일러 급수(Taylor Series) 또는 테일러 전개(Taylor Expansion)은 어떤 함수 $f(x)$를 우리가 다루기 쉬운 다항함수 형태로 바꾸어 준다.&lt;br /&gt;
  &lt;br /&gt;
$f(x)=p_{\infty}(x)$  &lt;br /&gt;
$p_n(x)=f(a)+f’(a)(x-a)+{f''(a)\over 2!}(x-a)^2+…+{f^{(n)}(a)\over n!}(x-a)^n$  &lt;br /&gt;
$\qquad\quad=\Sigma_{k=0}^{n}{f^{(k)}(a)\over k!}(x-a)^k$  &lt;br /&gt;
  &lt;br /&gt;
근사다항식의 차수가 높으면 높을수록 $p_n(x)$는 $f(x)$를 잘 근사하게 된다.  &lt;br /&gt;
  &lt;br /&gt;
주의해야 할 사항은 모든 $x$에 대해서 잘 근사하는 것이 아니라 $x=a$ 근처에서만 잘 근사한다는 것이다. 즉, $x$가 $a$로부터 멀어지면 멀어질수록 $f(x)=p(x)$는 큰 오차를 갖게 된다.  &lt;br /&gt;
  &lt;br /&gt;
그렇다면 위에서 봤던 $p_n(x)$는 어떻게 구한 것일까  &lt;br /&gt;
  &lt;br /&gt;
테일러 급수는 미적분의 기본 정리로부터 출발한다.  &lt;br /&gt;
  &lt;br /&gt;
$\int_a^xf’(t)dt=f(x)-f(a)$  &lt;br /&gt;
$f(x)=f(a)+\int_a^xf’(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
이제 우변의 마지막 항인 $\int_a^xf’(t)dt$를 부분적분을 활용해 바꿔줄 것이다.  &lt;br /&gt;
  &lt;br /&gt;
$\int_a^xf’(t)dt=\int_a^x1\cdot f’(t)dt$  &lt;br /&gt;
$\qquad\qquad =[(t+c)f’(t)dt]_a^x-\int_a^x(t+c)f''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
여기서 $c$는 적분상수이기 때문에 우리가 원하는 값을 넣을 수 있다. $c$에 $-x$를 대입한다.  &lt;/p&gt;

&lt;p&gt;$\qquad\qquad =[(t-x)f’(t)dt]_a^x-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
$\qquad\qquad =(x-x)f’(x)-(a-x)f’(a)-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
$\qquad\qquad =(x-a)f’(a)-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
정리하면 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_1(x)=f(a)+(x-a)f’(a)-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
마찬가지로 마지막 항인 $-\int_a^x(t-x)f''(t)dt$를 부분적분을 활용해 바꿔줄 것이다.  &lt;br /&gt;
$-\int_a^x(t-x)f''(t)dt=-([{(t-x)^2\over 2}f''(t)]_a^x-\int_a^x{(t-x)^2\over 2}f'''(t)dt)$  &lt;br /&gt;
$\qquad\qquad\qquad\qquad ={(x-a)^2 \over 2}f''(a)+\int_a^x{(t-x)^2\over 2}f'''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
정리하면 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_2(x)=f(a)+(x-a)f’(a)+{(x-a)^2 \over 2}f''(a)+\int_a^x{(t-x)^2\over 2}f'''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
마찬가지로 마지막 항인 $\int_a^x{(t-x)^2\over 2}f'''(t)dt$를 부분적분을 활용해 바꿔줄 것이다.  &lt;br /&gt;
  &lt;br /&gt;
$\int_a^x{(t-x)^2\over 2}f'''(t)dt=[{(t-x)^3\over 2\cdot 3}f'''(t)]_a^x-\int_a^x{(t-x)^3\over 2\cdot 3}f''''(t)dt$  &lt;br /&gt;
$\qquad\qquad\qquad\quad ={(x-a)^3 \over 2\cdot 3}f'''(a)-\int_a^x{(t-x)^2\over 2\cdot 3}f''''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
정리하면 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_3(x)=f(a)+(x-a)f’(a)+{(x-a)^2 \over 2}f''(a)+{(x-a)^3 \over 2\cdot 3}f'''(a)-\int_a^x{(t-x)^2\over 2\cdot 3}f''''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
이 정도 해보면 패턴이 눈에 보일 것이다. $p_n(x)$는 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_n(x)=f(a)+f’(a)(x-a)+{f''(a)\over 2!}(x-a)^2+…+{f^{(n)}(a)\over n!}(x-a)^n$  &lt;br /&gt;
$\qquad\quad=\Sigma_{k=0}^{n}{f^{(k)}(a)\over k!}(x-a)^k$  &lt;/p&gt;

&lt;p&gt;참고로 테일러 급수는 보통 $a$에 0을 넣어서 사용하는데, 이를 매클로린 급수(Maclaurin Series)라고 한다.  &lt;/p&gt;

&lt;p&gt;또한, 일반적으로 테일러 급수는 1차 또는 2차까지만 하는 경우가 많다. 1차, 2차 다항함수로 근사할 경우에는  &lt;br /&gt;
  &lt;br /&gt;
$f(x)\approx p_1=f(a)+(x-a)f’(a)+Q_2(x)$  &lt;br /&gt;
$f(x)\approx p_2=f(a)+(x-a)f’(a)+{(x-a)^2 \over 2}f''(a)+Q_3(x)$  &lt;br /&gt;
  &lt;br /&gt;
와 같이 놓고, $Q(x)$를 0으로 간주해 무시한다. 이 경우, $f(x)$를 무한차수 다항함수로 근사하는 것 보다는 근사오차가 크지만, $x$가 충분히 $a$에 가까운 경우에는 근사오차가 거의 없다.&lt;/p&gt;

&lt;h2 id=&quot;테일러급수가필요한이유&quot;&gt;테일러 급수가 필요한 이유&lt;/h2&gt;

&lt;p&gt;1. 잘 모르거나 복잡한 함수를 다루기 쉽고 이해하기 쉬운 다항함수로 대체&lt;br /&gt;
2. 함수의 특성을 분석하기 용이&lt;/p&gt;

&lt;h2 id=&quot;예시---e&quot;&gt;예시 - $e$&lt;/h2&gt;
&lt;p&gt;$e^x = 1 + {x \over 1!} + {x^2 \over 2!} + {x^3 \over 3!} + …$&lt;br /&gt;
$\quad =\sum_{k=0}^{\infty}{x^k \over k!}$&lt;/p&gt;

&lt;h2 id=&quot;정리&quot;&gt;정리&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;매클로린 급수&lt;br /&gt;
$f(x)=\sum_{k=0}^{\infty}{f^{(k)}(0) \over k!}x^k$&lt;/li&gt;
  &lt;li&gt;테일러 급수&lt;br /&gt;
$f(x-a)=\sum_{k=0}^{\infty}{f^{(k)}(a) \over k!}(x-a)^k$&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">테일러 급수 테일러 급수(Taylor Series) 또는 테일러 전개(Taylor Expansion)은 어떤 함수 $f(x)$를 우리가 다루기 쉬운 다항함수 형태로 바꾸어 준다.    $f(x)=p_{\infty}(x)$   $p_n(x)=f(a)+f’(a)(x-a)+{f''(a)\over 2!}(x-a)^2+…+{f^{(n)}(a)\over n!}(x-a)^n$   $\qquad\quad=\Sigma_{k=0}^{n}{f^{(k)}(a)\over k!}(x-a)^k$      근사다항식의 차수가 높으면 높을수록 $p_n(x)$는 $f(x)$를 잘 근사하게 된다.      주의해야 할 사항은 모든 $x$에 대해서 잘 근사하는 것이 아니라 $x=a$ 근처에서만 잘 근사한다는 것이다. 즉, $x$가 $a$로부터 멀어지면 멀어질수록 $f(x)=p(x)$는 큰 오차를 갖게 된다.      그렇다면 위에서 봤던 $p_n(x)$는 어떻게 구한 것일까      테일러 급수는 미적분의 기본 정리로부터 출발한다.      $\int_a^xf’(t)dt=f(x)-f(a)$   $f(x)=f(a)+\int_a^xf’(t)dt$      이제 우변의 마지막 항인 $\int_a^xf’(t)dt$를 부분적분을 활용해 바꿔줄 것이다.      $\int_a^xf’(t)dt=\int_a^x1\cdot f’(t)dt$   $\qquad\qquad =[(t+c)f’(t)dt]_a^x-\int_a^x(t+c)f''(t)dt$      여기서 $c$는 적분상수이기 때문에 우리가 원하는 값을 넣을 수 있다. $c$에 $-x$를 대입한다.  </summary></entry></feed>