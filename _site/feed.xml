<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-27T06:34:08-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JeongUk Jang</title><subtitle>Github Pages template for academic personal websites, forked from mmistakes/minimal-mistakes</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><entry><title type="html">Bayes Theroem</title><link href="http://localhost:4000/posts/2020/07/27/bayes_theorem" rel="alternate" type="text/html" title="Bayes Theroem" /><published>2020-07-27T00:00:00-07:00</published><updated>2020-07-27T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/27/bayes_theorem</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/27/bayes_theorem">&lt;h1 id=&quot;베이즈-정리bayes-theorem&quot;&gt;베이즈 정리(Bayes Theorem)&lt;/h1&gt;
&lt;p&gt;머신러닝을 공부하면서 베이즈 정리에 대해 각잡고 정리한 적이 없었다. 여러 리소스에서 나오는 내용들을 보며 파편적으로 베이즈 정리를 이해하고 있었다. 그런데 베이즈 정리라는게 참 특이하게 자료마다 다르게 설명한다. 크게 두 가지 정도 설명이 기억이 난다.&lt;/p&gt;

&lt;p&gt;1&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$P(B|A)$를 구하는 것이 불가능할 때, $P(A|B)$와 $P(B)$를 활용하여 $P(B|A)$를 구하기 위한 것&lt;/p&gt;

  &lt;p&gt;한양대학교 이상화 교수님 확률 및 통계 수업 1강&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;우리가 가지는 견해(예를 들어, 북극의 얼음은 a속도로 녹는다)에 관측 데이터를 추가하여 우리의 견해를 수정해나갈 수 있도록 해주는 것&lt;/p&gt;

  &lt;p&gt;PRML&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 두 내용이 같은 개념을 설명하기 위한 것임을 이해하기는 쉽지 않다. 그래서 베이즈 정리는 볼 때 마다 새롭고, 처음 보는 기분이 들었다.&lt;/p&gt;

&lt;p&gt;이런 애매모호함을 타파하기 위해 베이즈 정리를 정리해봤다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;베이즈 정리는 &lt;strong&gt;사후확률(posterior probability)을 사전확률(prior probability)과 likelihood를 이용해서 계산할 수 있도록 해주는 확률 변환식&lt;/strong&gt;이다.&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y|X)={P(X|Y)P(Y)\over P(X)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$P(Y|X)$ : 사후확률. 사건이 발생한 후 그 사건이 특정 모델에서 발생했을 확률&lt;/li&gt;
  &lt;li&gt;$P(X|Y)$ : likelihood. 어떤 모델에서 해당 데이터(관측치)가 나올 확률&lt;/li&gt;
  &lt;li&gt;$P(Y)$ : 사전확률. 관측자가 관측을 하기 전에 시스템 또는 모델에 대해 가지고 있는 선험적 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;베이즈 정리를 이해했고, 나를 헷갈리게 했던 두 가지 설명도 명확히 이해했는지 확인하기 위해 예를 들어 보자.&lt;/p&gt;

&lt;p&gt;당신은 압정을 던져 어떤 면이 나오는지 맞추는 도박에 참가하였다. 이 도박에서 이기기 위해서는 앞면이 나올 확률을 정확히 파악해야 한다. 당신은 믿을만한 도박 전문가로부터 앞, 뒤가 나올 확률은 동등하게 0.5라는 조언을 얻었으며, 5회의 실험을 통해 앞면이 3번 나왔고, 뒷면이 2번 나왔다. 이 때, 앞면이 나올 확률은 얼마인가?&lt;/p&gt;

&lt;p&gt;우리가 구하고자 하는 것은 앞면이 나올 확률인 $\theta$다. 도박 전문가에게 얻은 조언은 사전확률이라고 할 수 있으며, 실험 $D$의 결과로 likelihood를 계산해낼 수 있다. 이 두 가지 정보를 바탕으로 우리는 사후확률 $P(\theta|D)$를 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;1번 설명은 앞면이 나올 확률인 $\theta$를 알 수 없으니 관측을 통한 결과로부터 likelihood를 구하고, 도박 전문가로부터 조언을 얻어(사전확률) $\theta$를 계산해낼 수 있다는 내용이다.&lt;/p&gt;

&lt;p&gt;2번 설명은 사전확률에 조금 더 초점을 맞춰 설명한다. 도박 전문가에게 앞면이 나올 확률(사전확률)이 0.5라는 조언을 얻었지만 이 내용을 완전히 믿을 수는 없으므로, 관측치를 기반으로 해당 확률을 수정해나간다는 내용이다.&lt;/p&gt;

&lt;p&gt;결국 1번, 2번 설명이 사실은 같은 개념에 대한 것이지만 다르게 느껴졌던 이유는 서로 다른 부분에 집중해 설명했기 때문이다. 사후확률, 사전확률, likelihood의 관계를 명확히 파악하면 앞으로 이 개념 때문에 헷갈릴 일은 없을 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://darkpgmr.tistory.com/119&quot;&gt;다크프로그래머 베이지언 확률(Bayesian Probability)&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">베이즈 정리(Bayes Theorem) 머신러닝을 공부하면서 베이즈 정리에 대해 각잡고 정리한 적이 없었다. 여러 리소스에서 나오는 내용들을 보며 파편적으로 베이즈 정리를 이해하고 있었다. 그런데 베이즈 정리라는게 참 특이하게 자료마다 다르게 설명한다. 크게 두 가지 정도 설명이 기억이 난다.</summary></entry><entry><title type="html">02 Fundamentals of Machine Learning</title><link href="http://localhost:4000/posts/2020/07/26/02_Fundamentals_of_Machine_Learning" rel="alternate" type="text/html" title="02 Fundamentals of Machine Learning" /><published>2020-07-26T00:00:00-07:00</published><updated>2020-07-26T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/26/02_Fundamentals_of_Machine_Learning</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/26/02_Fundamentals_of_Machine_Learning">&lt;h1 id=&quot;rule-based-learning&quot;&gt;Rule-based Learning&lt;/h1&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/oNTXMgqCv6E&quot;&gt; &lt;/iframe&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/gmKvXBBawmk&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;완벽한 세상&lt;/strong&gt;에서만 잘 작동된다.
    &lt;ol&gt;
      &lt;li&gt;관측 에러가 없으며, 일관적이지 않은 관측 또한 없다.&lt;/li&gt;
      &lt;li&gt;랜덤(stochastic) 이벤트가 발생하지 않는다.&lt;/li&gt;
      &lt;li&gt;모든 경우의 수를 다 설명할 수 있는 수준의 많은 데이터를 확보했다.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;decision-tree&quot;&gt;Decision Tree&lt;/h1&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/dPP60RSEBek&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;error가 있는 데이터에 통계적 기법을 가미해 학습을 할 수 있는 가장 간단한 방법이 Decision Tree&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기준이 되는 피처를 정해 데이터셋을 나누고, 나눠진 각각에 대해 다시 피처를 정해 데이터셋을 나누는 작업을 반복해 분류 작업을 수행&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;어떤 피처를 선택할지 결정하기 위해 &lt;strong&gt;Entropy, Information Gain&lt;/strong&gt; 개념 도입&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/kG-IVxyUAUM&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;확률 변수의 불확실성을 측정하기 위한 지표
    &lt;ul&gt;
      &lt;li&gt;ex 1) 압정 A를 반복해서 던졌는데, 계속 Head만 나왔다 -&amp;gt; 불확실성이 낮다.&lt;/li&gt;
      &lt;li&gt;ex 2) 압정 B를 반복해서 던졌는데, Head와 Tail이 반씩 나왔다. -&amp;gt; 불확실성이 높다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;높은 엔트로피는 그 확률 변수가 높은 불확실성을 지니고 있음을 뜻한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X)=-\sum_xP(X=x)log_2P(X=x)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;압정 던지기 게임의 경우 $x$는 Head, Tail이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conditional-entropy&quot;&gt;Conditional Entropy&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{matrix}
H(Y|X) &amp;=&amp; -\sum_xP(X=x)log_2H(Y|X=x)\\
	        &amp;=&amp; -\sum_xP(X=x)\{-\sum_yP(Y=y|X=x)log_2P(Y=y|X=x)\}
\end{matrix} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;information-gain&quot;&gt;Information Gain&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475631-b9de6980-cf6c-11ea-828f-b28eb23260f5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;먼저 전체 엔트로피를 측정해보자.
    &lt;ul&gt;
      &lt;li&gt;위에서 정의한 엔트로피 공식을 그대로 사용하면 된다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
          H(Y) &amp;=&amp; -\sum_{y\in\{+, -\}}P(Y=y)log_2P(Y=y)\\
          &amp;=&amp; -{307\over307+383}log_2{307\over307+383}-{383\over307+383}log_2{383\over307+383}
\end{matrix} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;이번에는 A1, A9이라는 조건을 주고 엔트로피를 측정해보자.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Conditional Entropy 공식을 사용하면 된다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$H(Y|A_1)=-\sum_{x\in{a, b, ?}}\sum_{y\in{+, -}}P(A_1=x|Y=y)log_2{P(A_1=x)\over P(A_1=x|Y=y)}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$H(Y|A_9)=-\sum_{x\in{t,f}}\sum_{y\in{+, -}}P(A_9=x|Y=y)log_2{P(A_9=x)\over P(A_9=x|Y=y)}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;우리는 각 노드들의 엔트로피를 구할 수 있으며, 이를 통해 불확실성이 얼마나 개선되었는지를 측정할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$IG(Y, A_i)=H(Y)-H(Y|A_i)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$IG(Y, A_9)&amp;gt;IG(Y, A_1)$ 임을 알 수 있다. 즉, $A_9$를 기준으로 나눌 때 불확실성이 더 많이 개선된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;size-of-tree&quot;&gt;Size of Tree&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;분기 작업을 반복하면 거대한 트리를 만들 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;하지만 트리가 너무 거대해지면 학습 데이터셋에는 잘 작동하지만 테스트 데이터셋에서는 잘 작동하지 않는 오버피팅 문제가 발생한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475630-b8ad3c80-cf6c-11ea-8e6b-778e3f5054d3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;강의에서는 Decision Tree를 Rule-based 기법의 하나로 소개하며, 오버피팅 문제 때문에 실제 상황에서는 많이 사용하지 않는다고 소개함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h1&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/70tDiv30WrM&quot;&gt; &lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이제부터 본격적으로 통계를 활용한 머신러닝 기법들이 나옴&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression은 많은 머신러닝 기법들에 영향을 미친 중요한 기법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;강의에서는 주택 가격 예측 데이터셋을 활용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가정 : 주택 가격은 feature values의 linearly weighted sum으로 표현할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;h:\hat{f}(x;\theta)=\sum_{i=0}^{n}\theta_ix_i&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$n$ : feature의 수&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;결국 Linear Regression은 $\theta$를 잘 조정해서 좋은 결과를 내도록 하는 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;수식을 행렬로 표현할 수 있다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=\begin{bmatrix}
          1 &amp; \cdots &amp; x_n^1 \\
          \vdots &amp; \ddots &amp; \vdots \\
          1 &amp; \cdots &amp; x_n^D
          \end{bmatrix}, 
      \theta=\begin{bmatrix}
                  \theta_0 \\
                  \vdots \\
                  \theta_n 
                  \end{bmatrix} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;h:\hat{f}=X\theta&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\hat{f}$는 예측값이고, 실제값은 다음과 같이 노이즈를 포함시켜 정의한다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$f=X\theta+e=Y$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Linear Regression의 목표는 실제값과 예측값의 차이를 최소화하는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
\hat{\theta} &amp;=&amp; argmin_\theta(f-\hat{f})^2 \\
	        &amp;=&amp; argmin_\theta(Y-X\theta)^2 \\
	        &amp;=&amp; argmin_\theta(Y-X\theta)^T(Y-X\theta) \\
	        &amp;=&amp; argmin_\theta (\theta^TX^TX\theta-2\theta^TX^TY+Y^TY) \\
	        &amp;=&amp; argmin_\theta (\theta^TX^TX\theta-2\theta^TX^TY)
\end{matrix} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;미분을 통해 극점을 찾는 방식으로 최적화시킬 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta(\theta^TX^TX\theta-2\theta^TX^TY)=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2X^TX\theta-2X^TY=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta=(X^TX)^{-1}X^TY&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$X, Y$ 모두 아는 값이므로 $\theta$를 구할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;아래 그림은 feature를 하나만 사용하여 예측을 해본 것이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475629-b814a600-cf6c-11ea-8365-2b819a6c3bfb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;non-linearity&quot;&gt;Non-Linearity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression은 선형이라는 한계 때문에 표현할 수 없는 부분이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$x$값에 non linearity를 부여해 조금 더 표현력이 높은 모델을 만들 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;h:\hat{f}(x;\theta)=\sum_{i=0}^n\sum_{j=1}^m\theta_{i,j}\Phi_j(x_i)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;위 식에서는 $\Phi$라는 non-linear function을 사용해 $x_i$를 변형시킨다. $\Phi$로 $x^2, x^3, x^4, …$ 등을 사용할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/88475627-b4811f00-cf6c-11ea-80b4-4fbfacf068c6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;위 그림은 $x^2, x^3, …, x^9$까지 포함하여 나타낸 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;주어진 데이터를 더 잘 표현한 것으로 볼 수도 있으나, Decision Tree에서 주어진 데이터를 잘 표현하기 위해 노드를 늘리는 것이 오버피팅 문제를 일으키듯이, 무작정 non-linearity를 부여하는 것도 오버피팅 문제를 일으킬 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">Rule-based Learning</summary></entry><entry><title type="html">01 Motivations and Basics</title><link href="http://localhost:4000/posts/2020/07/25/01_Motivations_and_Basics" rel="alternate" type="text/html" title="01 Motivations and Basics" /><published>2020-07-25T00:00:00-07:00</published><updated>2020-07-25T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/25/01_Motivations_and_Basics</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/25/01_Motivations_and_Basics">&lt;iframe src=&quot;https://www.youtube.com/embed/3AwO0O6hWBI&quot;&gt; &lt;/iframe&gt;

&lt;h1 id=&quot;thumbtack-question&quot;&gt;Thumbtack Question&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;당신은 백만장자에게 고용되었다. 백만장자는 압정을 던져 앞면이 나오는가 뒷면이 나오는가를 맞추는 도박을 하려고 하는데, 이것을 해도 손해는 보지 않을지 고민이 되어 당신을 고용하였다. 백만장자를 위해 &lt;strong&gt;압정 던지기 게임&lt;/strong&gt;과 그와 관련된 &lt;strong&gt;확률 이론&lt;/strong&gt;을 공부해보자.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;압정 던지기 게임에서 이기기 위해서는 앞면, 뒷면이 나올 확률을 정확하게 알아야 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;압정은 앞, 뒤가 다르게 생겨 각각이 나올 확률이 p, 1-p이다.  p=0.5가 아니라는 점에서 동전 던지기 게임과는 다르다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-tossing&quot;&gt;1. Tossing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이러한 문제를 해결하기 위해 가장 쉽게 떠올릴 수 있는 방법은 압정을 던져보는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;당신은 백만장자 앞에서 압정을 다섯 번 던졌다. 그 결과 앞면이 세 번 나왔고, 뒷면이 두 번 나왔다.(HHTHT)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;실험의 결과에 따르면, 앞면이 나올 확률은 0.6이고, 뒷면이 나올 확률은 0.4이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;결과는 다음 두 이론을 바탕으로 하고 있다.&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;a href=&quot;https://blanik00.github.io/posts/2020/05/21/bern_bin&quot;&gt;Binomial Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://blanik00.github.io/posts/2020/05/03/maximum_likelihood_estimation&quot;&gt;MLE(Maximum Likelihood Estimation)&lt;/a&gt;&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;우리는 압정 던지기 게임의 결과가 Binomial Distribution을 따른다는 것을 가정한다. 이 때, 압정의 특이한 모양에 의해 형성된 앞면이 나올 확률을 $\theta$라고 하자.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(HHTHT)=\theta\theta(1-\theta)\theta(1-\theta)=\theta^3(1-\theta)^2&lt;/script&gt;

    &lt;p&gt;이를 일반화 시켜 확률 $\theta$가 주어졌을 때 데이터 $D$가 나올 확률은 다음과 같다.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$a_H$ : $D$에서 앞면이 나온 횟수&lt;/li&gt;
      &lt;li&gt;$a_T$ : $D$에서 뒷면이 나온 횟수&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;우리는 앞서 압정 던지기 게임의 결과가 Binomial Distribution을 따른다는 것을 가정했다. 그렇다면 어떻게 하면 이 가정을 더 강화할 수 있을까?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-mle&quot;&gt;2. MLE&lt;/h2&gt;

&lt;p&gt;우리는 가정을 강화하기 위해  데이터의 분포를 가장 잘 설명하는 $\theta$를 찾는 것에 집중할 것이다.&lt;/p&gt;

&lt;p&gt;이 목적을 위해 고려할 수 있는 방법이 MLE이다. 이 방법은 관측된 데이터가 등장할 확률을 최대화하는 $\hat{\theta}$를 찾는다. 수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix}
\hat{\theta} &amp;=&amp; argmax_{\theta}P(D|\theta) \\
       &amp;=&amp; argmax_{\theta}\theta^{a_H}(1-\theta)^{a_T} \\
       &amp;=&amp; argmax_{\theta}ln\{\theta^{a_H}(1-\theta)^{a_T}\} \\
       &amp;=&amp; argmax_{\theta}\{a_Hln\theta+a_Tln(1-\theta)\} \\
\end{matrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{d\over d\theta}\{a_Hln\theta+a_Tln(1-\theta)\}=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{a_H\over\theta}-{a_T\over 1-\theta}=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta={a_H\over a_T+a_H}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}={a_H\over a_T+a_H}&lt;/script&gt;

&lt;h2 id=&quot;3-more-tossing&quot;&gt;3. More Tossing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;당신이 이 내용을 백만장자에게 설명하는 동안 백만장자는 압정을 50번 더 던졌다. 그 결과 앞면이 30번, 뒷면이 20번 나왔다. 호기심이 많은 백만장자는 5번을 던져서 3번 앞면이 나온 것과 50번을 던져서 30번 앞면이 나온 것이 동일한 것인지 물어봤다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;우리가 MLE를 통해서 한 것은 결국 모수인 $\theta$를 추정하는 $\hat{\theta}$을 구한 것이다. 추정값은 어쩔 수 없이 오차를 가지고 있는데, 여러 번 실험을 해보는 백만장자의 시도로 추정의 오차를 줄일 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;오차를 측정하는 방법으로 Hoeffding’s Inequaility를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Hoeffding’s Inequaility
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$P(|\hat{\theta}-\theta^* |\ge \epsilon)\le2e^{-2N\epsilon^2}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;즉, 추정값과 실제값의 차이가 $\epsilon$보다 작을 확률은 $2e^{-2N\epsilon^2}$보다 작거나 같다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;수식에 영향을 미치는 변수는 두 가지가 있다.&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;$\epsilon$ : error bound인 $\epsilon$이 커지면 우변의 값이 작아져 추정값과 실제값의 차이가 error bound보다 커질 확률은 적어진다. 당연한 얘기다.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$N$ : 마찬가지로 시행 횟수인 $N$이 커지면 우변의 값이 작아져 추정값과 실제값의 차이가 error bound보다 커질 확률은 적어진다.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이 원리를 이용하면 error bound를 0.1로 잡았을 때, 몇 번의 시행을 해야 에러가 발생할 확률이 0.01% 이하로 떨어지는가? 등과 같은 계산을 할 수 있다. (error bound를 0.1로 고정시키고 우변이 0.01 이하로 떨어질 때 까지 $N$을 늘리면 된다)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이러한 것을 PAC(Probably Approximate Correct) learning이라고 한다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;그렇기 때문에 시행 횟수인 $N$을 늘린 백만장자의 시도는 에러가 발생할 확률을 줄여준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src=&quot;https://www.youtube.com/embed/LbYCQxKAv2E&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;4-bayes&quot;&gt;4. Bayes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bayes라는 사람이 찾아왔다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그는 지금까지의 실험을 보며 앞면이 나올 확률이 정말로 60%일 것인지 의심해봐야 한다고 주장했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그러면서 앞면과 뒷면이 나올 확률이 동등하게 0.5가 아닐까하며 백만장자를 설득했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;백만장자는 처음엔 0.5일 것이라고 생각은 했었는데, 실험을 해보니 그게 아니었다고 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;베이즈는 때를 놓치지 않고 백만장자의 사전 정보(앞뒤 확률이 같을 것이라는 것)를 파라미터를 추정하는 과정에 반영시킬 수 있다고 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;베이즈는 다음과 같은 공식을 제안했다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|D)={P(D|\theta)P(\theta) \over P(D)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;다음과 같이  표현할 수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Posterior={Likelihood \times Prior\;Knowledge \over Normalizing\;Constant}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;앞과 뒤가 나올 확률이 나올 확률이 0.5로 동일할 것이라는 백만장자의 생각은 Prior Knowledge에 들어간다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}$로 이미 구해두었으며, Prior Knowledge는 사전 지식이므로 $P(\theta|D)$를 바로 계산할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사실 $P(D)$는 이미 주어진 사실이므로, 이것이 발생할 확률은 정해져 있다. 즉, $\theta$에 영향을 받지 않는다. 그래서 보통 $P(D)$를 빼고 다음과 같은 식을 계산한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D)\propto P(D|\theta)P(\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta)=???&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;그렇다면 $P(\theta)$는 어떻게 표현할 수 있을까?
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$P(D|\theta)$는 Binomial Distribution을 따른다고 가정하고 계산했다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이처럼 $P(\theta)$도 특정 분포를 따른다고 가정하고 계산해야 한다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;여러가지 방법이 있는데 베이즈는 베타 분포를 제안했다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;베타 분포는 특정 범위에 있는 값을 0과 1 사이의 실수로 만들어 주기 때문에 확률로 사용하기 좋다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;베타 분포에 따르면 $P(\theta)$는 다음과 같이 표현할 수 있다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta)={\theta^{\alpha-1}(1-\theta)^{\beta-1} \over B(\alpha, \beta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B(\alpha, \beta)={\Gamma(\alpha)\Gamma(\beta)\over \Gamma(\alpha+\beta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Gamma(\alpha)=(\alpha-1)!&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D)\propto P(D|\theta)P(\theta)\propto \theta^{a_H}(1-\theta)^{a_T}\theta^{\alpha-1}(1-\theta)^{\beta-1}&lt;/script&gt;

&lt;p&gt;($P(\theta|D)={P(D|\theta)P(\theta) \over P(D)}$의 $P(D)$, $P(\theta)={\theta^{\alpha-1}(1-\theta)^{\beta-1} \over B(\alpha, \beta)}$의 $B(\alpha, \beta)$는 $\theta$에 영향을 받는 요소가 아니라서 제거하였다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\theta^{a_T\alpha-1}(1-\theta)^{a_T+\beta-1}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MLE에서는 $\hat{\theta}=argmax_{\theta}P(D|\theta)$를 찾는 것이 문제였다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이번에 할 것은 MAP이며, $\hat{\theta}=argmax_{\theta}P(\theta|D)$를 구하는 문제이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;앞에서 $P(D)\propto \theta^{a_H}(1-\theta)^{a_T}\theta^{\alpha-1}(1-\theta)^{\beta-1}$인 것을 배웠다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MLE에서 최적값을 구했던 것과 같은 방법으로 구해보면 $\hat{\theta}$은 다음과 같다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}={a_H+\alpha-1\over a_H+\alpha+a_T+\beta-2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;문제를 바라보는 관점에서 차이가 있는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mle-vs-map&quot;&gt;MLE vs MAP&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;그렇다면 어떤 방법이 더 좋은 것일까?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사실 두 방법은 시행 횟수가 아주 많을 때 같아진다. MAP에 있는 $\alpha$, $\beta$는 $a_T$, $a_H$가 커짐에 따라 그 영향력이 작아지기 때문이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반대로 시행 횟수가 적은 상황에서는 사전 정보가 중요한 영향을 미치게 된다. 사전 정보인 $\alpha$, $\beta$을 어떻게 설정하느냐에 따라 MAP의 성능이 MLE보다 좋을 수도 있고, 나쁠 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Chi Square, T, F Distribution</title><link href="http://localhost:4000/posts/2020/07/05/chisq_t_f" rel="alternate" type="text/html" title="Chi Square, T, F Distribution" /><published>2020-07-05T00:00:00-07:00</published><updated>2020-07-05T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/05/chisq_t_f</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/05/chisq_t_f">&lt;iframe src=&quot;https://www.youtube.com/embed/DyBxYsCow9k&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;chi-square-distribution&quot;&gt;Chi-Square Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;iid인 표준 정규 분포가 n개 있다.($Z_1, Z_2, …, Z_n$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Z=Z_1^2+ Z_2^2+ …+ Z_n^2$는 카이제곱 분포를 따른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Z\sim \chi^2(v)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;파라미터인 $v$는 자유도를 뜻하며, 카이제곱 분포를 형성할 때 더한 표준 정규 분포의 갯수를 뜻한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;카이제곱 분포의 PDF&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/86525674-3e395180-bec5-11ea-9a1c-78a13c8d4c3a.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$E[X]=v,\;V[X]=2v$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\alpha=v/2, \lambda=2$인 특별한 형태의 감마분포이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;표준 정규 분포가 표준 정규 분포표를 가지고 있듯이, 카이제곱 분포도 카이제곱 분포표를 가지고 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-variance&quot;&gt;Sampling Distribution of Sample Variance&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$\mu$는 모평균이다. 
$\chi^2(n)=\sum_{i=1}^{n}(X_i-\mu)^2/\sigma^2$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\bar{X}$는 표본 평균이다. 표본 평균으로 추정을 했으므로 알고 있는 것이다. 그래서 $n$개 중에서 하나(표본 평균)는 이미 정해진 값이고, 나머지 $n-1$개는 자유롭게 정해질 수 있는 값이다. 그래서 자유도가 $n-1$이다.
$\chi^2(n-1)=\sum_{i=1}^{n}(X_i-\bar{X})^2/\sigma^2$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;자유도보충설명&quot;&gt;자유도(보충설명)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;자유도를 명확하게 배우기는 쉽지 않다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;많이 접하면서 감을 잡을 수 밖에 없다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예시&quot;&gt;예시&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$X_1=2, X_2=3, X_3=4, X_4=5, X_5=?$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\sum_{i=1}^{5}X_i=20$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 두 조건을 만족할 때, $X_5$는 얼마인가?&lt;/p&gt;

&lt;p&gt;$\sum_{i=1}^{4}X_i=14$이므로 $X_5$는 6이 될 수 밖에 없다.&lt;/p&gt;

&lt;p&gt;이 문제에서 자유도는 얼마일까?&lt;/p&gt;

&lt;p&gt;문제에서 자유롭게 정해질 수 있는 값은 $X_1, X_2, X_3, X_4$이며, $X_5$는 나머지 값들에게 종속된다. 그러므로 자유도는 4이다.&lt;/p&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-variance---example&quot;&gt;Sampling Distribution of Sample Variance - Example&lt;/h2&gt;
&lt;p&gt;$N(\mu=15,\mu^2=100)$인 모집단으로부터 독립적이고 동일하게(iid) 25개의 샘플을 뽑았다($X_1, X_2, …, X_{25}$). 이 때 $Y=(n-1)S^2/\sigma^2$는 $\chi^2(24)$를 따른다(자유도는 24). 즉, $P[Y&amp;gt;\chi^2_{\alpha,24}]=\alpha$라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;$P[S^2&amp;gt;c]=0.95$일 때 c의 값을 구하라.&lt;/p&gt;

&lt;p&gt;$S^2$의 분포를 바로 파악하기 힘드므로 양 변에 $(n-1)$을 곱하고, $\sigma^2$으로 나눈다. 즉, $P[(n-1)S^2/\sigma^2&amp;gt;(n-1)c/\sigma^2]=0.95$을 만족하는 c를 찾으면 된다.&lt;/p&gt;

&lt;p&gt;$P[Y&amp;gt;24c/100]=0.95$&lt;/p&gt;

&lt;p&gt;카이제곱 분포표에서 $n$ 24이고, $\alpha$가 0.95인 것을 찾으면 $24c/100=13.848$이 된다. $c=57.7$&lt;/p&gt;

&lt;h2 id=&quot;t-distribution&quot;&gt;t Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$Z\sim N(0,1)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y\sim \chi^2(v)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;T={Z\over\sqrt{Y/v}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$T$ 통계량은 자유도가 $v$인 t-분포(t-distribution)를 따른다.($t(v)$로 표기)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;카이제곱 분포의 자유도가 그대로 t-분포의 자유도가 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;t-분포의 기댓값은 0이고, 이를 기준으로 좌우 대칭이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;표준정규분포보다 긴 꼬리를 가지고 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;기댓값을 기준으로 좌우 대칭이라는 특성 때문에 아래 수식을 만족한다.
$P[T&amp;lt;-t_{\alpha, v}]=P[T&amp;gt;t_{\alpha, v}]=\alpha$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/86525673-3d082480-bec5-11ea-8154-a9d041a0a4df.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽 그림에서 알 수 있듯이 표준 정규 분포보다 긴 꼬리를 가지고 있다.&lt;/p&gt;

&lt;p&gt;왼쪽 그림에서 알 수 있듯이 자유도가 작을수록 긴 꼬리를 가진다.&lt;/p&gt;

&lt;h3 id=&quot;t-분포-조금-더-자세히&quot;&gt;t-분포 조금 더 자세히&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$N(\mu,\sigma^2)$에서 표본 $n$개를 추출한다($X_1, X_2, …, X_n$).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Z=\bar{X}-\mu/(\sigma/\sqrt{n})$이므로 $N(0,1)$을 따른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y=(n-1)S^2/\sigma^2$이므로 $\chi^2(n-1)$을 따른다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;T={Z\over\sqrt{Y/v}}={\bar{X}-\mu/(\sigma/\sqrt{n})\over\sqrt{((n-1)S^2/\sigma^2)/(n-1)}}={\bar{X}-\mu\over S/\sqrt{n}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 때, $T$는 자유도가 $n-1$인 t-분포를 따른다.($T\sim t(n-1)$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;신기하게도 $T$ 통계량과 표준 정규 분포는 매우 닮았다. 다른 점이 있다면 표준 정규 분포에서는 $\sigma$(모표준편차)가 사용되고, $T$ 통계량에서는 $S$(표본표준편차)가 사용된다는 점이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;그래서 모집단의 표준편차를 알고 있을 때는 표준 정규 분포로 표현하면 되고, 모집단의 표준 편차를 모른다면 t-분포로 표현하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;t-distribution---example&quot;&gt;t Distribution - Example&lt;/h2&gt;
&lt;p&gt;$N(\mu=15,\mu^2=100)$인 모집단으로부터 독립적이고 동일하게(iid) 25개의 샘플을 뽑았다($X_1, X_2, …, X_{25}$).&lt;/p&gt;

&lt;p&gt;$T={\bar{X}-\mu\over S/\sqrt{n}}$이고, 이는 $t(v=24)$를 따른다.&lt;/p&gt;

&lt;p&gt;$P[T&amp;lt;-t_{\alpha, 24}]=P[T&amp;gt;t_{\alpha, 24}]=\alpha$&lt;/p&gt;

&lt;p&gt;$P[T &amp;lt; c]=0.05$인 $c$를 구하라.&lt;/p&gt;

&lt;p&gt;$t$분포 테이블에서 자유도가 24이고, $\alpha$가 0.05인 것을 찾으면 1.711이다. $c$보다 작을 확률이므로 $c=-1.711$이 된다.&lt;/p&gt;

&lt;h2 id=&quot;f-distribution&quot;&gt;F Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$Y_1=\chi^2(v_1)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$Y_2=\chi^2(v_2)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;F={Y_1/v_1\over Y_2/v_2}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이렇게 정의되는 것을 F통계량이라고 하고, 이는 F 분포를 따른다. F분포는 $v_1, v_2$ 두 개의 자유도를 가지며, $F(v_1, v_2)$로 표기한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;F 테이블이 있어 확률값을 쉽게 구할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$P[F&amp;gt;f_a(v_1,v_2)]=f_{1-\alpha}(v_1, v_2)={1\over f_{\alpha}(v_2,v_1)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/86525671-39749d80-bec5-11ea-9663-a8bc3cacb227.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;f-distribution---example&quot;&gt;F Distribution - Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모집단 1 : $N(\mu_1=4,\sigma_1^2=16)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모집단 2 : $N(\mu_2=12,\sigma_2^2=48)$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;모집단 1로부터 동일하고 독립적인 $n_1=15$개의 샘플을 추출하고, 모집단 2로부터 동일하고 독립적인 $n_2=10$개의 샘플을 추출한다.&lt;/p&gt;

&lt;p&gt;$F={S_1^2/\sigma_1^2\over S_2^2/\sigma_2^2}$는 $F(14,9)$를 따른다.&lt;/p&gt;

&lt;p&gt;이 때 $P[S_1^2/S_2^2\le c]=0.05$일 때, c를 구하라.&lt;/p&gt;

&lt;p&gt;$P[S_1^2/S_2^2\le c]=P[S_1^2\sigma_2^2/S_2^2\sigma_1^2\le (\sigma_2^2/\sigma_1^2)c]=P(F&amp;lt;3c)=0.05$&lt;/p&gt;

&lt;p&gt;$P[F&amp;gt;f_{0.95}(14,9)]=0.95$&lt;/p&gt;

&lt;p&gt;$P[F\le f_{0.95}(14,9)]=0.05$&lt;/p&gt;

&lt;p&gt;$3c=f_{0.95}(14,9)=1/f_{0.05}(9,14)=1/2.65$&lt;/p&gt;

&lt;p&gt;$c=0.1258$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Sample Distribution</title><link href="http://localhost:4000/posts/2020/06/27/sample_distribution" rel="alternate" type="text/html" title="Sample Distribution" /><published>2020-06-28T00:00:00-07:00</published><updated>2020-06-28T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/27/Sample%20Distribution</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/27/sample_distribution">&lt;iframe src=&quot;https://www.youtube.com/embed/EjQq2s9C9Sc&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;population-sample&quot;&gt;Population, Sample&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Population : 전체(모집단)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sample : 부분(모집단의 부분집합, 표본)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling&quot;&gt;Sampling&lt;/h2&gt;
&lt;p&gt;모집단에서 표본을 추출하는 작업을 의미&lt;/p&gt;

&lt;h3 id=&quot;표기&quot;&gt;표기&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Mean&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Variance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Population&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\mu$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\sigma^2$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sample&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$\bar{X}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$S^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;statistic통계량&quot;&gt;Statistic(통계량)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Statistic : 샘플들의 함수&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;통계량의 복수형인 Statistics를 통계학을 의미하는 Statistics와 혼동하기 쉽다. 문맥을 보고 파악하면 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;통계학에서 배우는 가장 중요한 함수가 세 가지 있다.
    &lt;ul&gt;
      &lt;li&gt;확률 변수(Random Variable)&lt;/li&gt;
      &lt;li&gt;확률 함수(Probability Function)&lt;/li&gt;
      &lt;li&gt;통계량(Statistic)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;대표적인 통계량은 다음과 같다.
    &lt;ul&gt;
      &lt;li&gt;$\bar{X}=(x_1+x_2+…+x_n)/n$&lt;/li&gt;
      &lt;li&gt;$S^2={1\over n-1}\sum_{i=1}^{n}(x_1-\bar{X})^2$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution&quot;&gt;Sampling Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sampling Distribution : 통계량의 분포&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그렇다면 Sampling Distribution은 $\bar{X}$, $S^2$의 분포를 말한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;여기서 Sampling된 것들은 서로 독립이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-mean&quot;&gt;Sampling Distribution of Sample Mean&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\bar{X}=(x_1+x_2+…+x_n)/n$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그렇다면 $\bar{X}$의 기댓값은 얼마일까&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$E[\bar{X}]=E[(x_1+x_2+…+x_n)/n]$&lt;/p&gt;

&lt;p&gt;$=E[x_1]/n+E[x_2]/n+…+E[x_n]/n$&lt;/p&gt;

&lt;p&gt;$=\mu/n+\mu/n+…+\mu/n$&lt;/p&gt;

&lt;p&gt;$=n(\mu/n)=\mu$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;또한 $\bar{X}$의 분산은 얼마일까&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$V[\bar{X}]=V[(x_1+x_2+…+x_n)/n]$&lt;/p&gt;

&lt;p&gt;$=V[x_1]/n^2+V[x_2]/n^2+…+V[x_n]/n^2$&lt;/p&gt;

&lt;p&gt;$=\sigma^2/n^2+\sigma^2/n^2+…+\sigma^2/n^2$&lt;/p&gt;

&lt;p&gt;$=n(\sigma^2/n^2)=\sigma^2/n$&lt;/p&gt;

&lt;h3 id=&quot;정규-분포를-예로-들어보자&quot;&gt;정규 분포를 예로 들어보자.&lt;/h3&gt;
&lt;p&gt;모집단이 $N(\mu, \sigma^2)$를 따른다고 하자. 여기서 iid인 샘플 n개를 추출했다.($X_1$, $X_2$, …, $X_n$) 그렇다면 $\bar{X}$는 $N(\mu, \sigma^2/n)$를 따른다. 표준정규분포로 바꾸려면 $Z=(\bar{X}-\mu)/(\sigma/\sqrt{n})$로 두면 된다.&lt;/p&gt;

&lt;h2 id=&quot;central-limit-theorem중심극한정리&quot;&gt;Central Limit Theorem(중심극한정리)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;평균이 $\mu$이고 분산이 $\sigma^2$인 임의의 모집단에서 표본의 크기인 $n$이 충분히 크면($n\ge 30$), 표본 평균 $\bar{X}$는 근사적으로 정규분포 $N(\mu, \sigma^2/n)$를 따른다. (여기서 n은 한 표본 안에 들어있는 원소의 수를 뜻한다.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sampling-distribution-of-sample-mean---two-population&quot;&gt;Sampling Distribution of Sample Mean - Two Population&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;두 개의 독립적인 모집단의 평균의 차이를 알고 싶다. 하지만 모집단을 모두 파악하는 것은 불가능하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;두 개의 모집단(어떤 분포를 따르든 상관 없다.)으로부터 샘플을 추출해 비교하는 방법을 취한다.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Pop1. ($\mu_1$, $\sigma_1^2$) $\rightarrow$ $n_1$개 샘플링 $\rightarrow$ $\bar{X_1}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pop2. ($\mu_2$, $\sigma_2^2$) $\rightarrow$ $n_2$개 샘플링 $\rightarrow$ $\bar{X_2}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\bar{X_1}-\bar{X_2}$의 기댓값과 분산을 보고 두 모집단의 차이를 확인할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;$E[\bar{X_1}-\bar{X_2}]=E[\bar{X_1}]-E[\bar{X_2}]=\mu_1-\mu_2$&lt;/li&gt;
      &lt;li&gt;$V[\bar{X_1}-\bar{X_2}]=V[\bar{X_1}]+V[\bar{X_2}]=\sigma_1^2/n_1+\sigma_2^2/n_2$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, $\bar{X_1}-\bar{X_2}$는 $N(\mu_1-\mu_2, \sigma_1^2/n_1+\sigma_2^2/n_2)$를 따른다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Moment Functions</title><link href="http://localhost:4000/posts/2020/06/27/moment_functions" rel="alternate" type="text/html" title="Moment Functions" /><published>2020-06-27T00:00:00-07:00</published><updated>2020-06-27T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/27/moment_functions</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/27/moment_functions">&lt;iframe src=&quot;https://www.youtube.com/embed/V6a3MPrv0dM&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;moment-functions&quot;&gt;Moment Functions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;확률 변수 X의 Moment Function(적률 함수)은 다음과 같이 정의된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E[X^r]=
\begin{cases}
\sum_{x}X^rp(x) &amp; if\;X\;is\;discrete\\
\int_{-\infty}^{\infty}X^rf(x)dx  &amp; if\;X\;is\;continuous
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;$r$의 값에 따라 적률 함수의 성격이 달라진다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;moment(r)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Moment Function&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;전처리한 결과&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;전처리한 결과&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[X]$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[X]$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;기댓값(평균)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[X^2]$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[(x-\mu)^2]$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;분산&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[X^3]$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[(x-\mu)^3]/\sigma^3$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;왜도(Skewness)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[X^4]$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$E[(x-\mu)^3]/\sigma^4$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;첨도(Kurtosis)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$r=2$부터는 mean centering 전처리를 한 결과이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;왜도 - 얼마나 기울여져 있는가&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;첨도 - 분포가 얼마나 뾰족한가&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;moment-generating-functionmgf&quot;&gt;Moment Generating Function(MGF)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;바로 앞에서 배운 Moment Function을 생성하는 함수&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
M_X(t)=E[e^{tX}]=
\begin{cases}
\sum_{x}e^{tX}p(x) &amp; if\;X\;is\;discrete\\
\int_{-\infty}^{\infty}e^{tX}f(x)dx  &amp; if\;X\;is\;continuous
\end{cases} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;MGF가 생성하는 분포는 unique하다. 즉, 두 개의 확률 변수가 같은 확률 변수를 가지고 있다면, 이 둘은 같은 확률 분포를 가진다. 즉, MGF와 확률 분포는 일대일 대응. 중요&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MGF를 t에 대해 한 번 미분해서 $t=0$을 대입하면 첫 번째 모멘트이고, t에 대해 두 번 미분해서 $t=0$을 대입하면 두 번째 모멘트이다. 일반화 시켜보면 t에 대해 n번 미분해서 $t=0$을 대입하면 n 번째 모멘트이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;첫-번째-모멘트를-구해보자&quot;&gt;첫 번째 모멘트를 구해보자.&lt;/h3&gt;

&lt;p&gt;$E[e^{tx}]=M_X(t)$&lt;/p&gt;

&lt;p&gt;$M_X’(t)={d\over dt}E[e^{tx}]=E[{d\over dt}e^{tx}]=E[xe^{tx}]$&lt;/p&gt;

&lt;p&gt;$M_X’(t=0)=E[x]$&lt;/p&gt;

&lt;h3 id=&quot;두-번째-모멘트를-구해보자&quot;&gt;두 번째 모멘트를 구해보자.&lt;/h3&gt;
&lt;p&gt;$M_X’‘(t)={d\over dt}M_X’(t)={d\over dt}E[xe^{tx}]=E[{d\over dt}xe^{tx}]=E[x^2e^{tx}]$&lt;/p&gt;

&lt;p&gt;$M_X’‘(t=0)=E[x^2]$&lt;/p&gt;

&lt;h3 id=&quot;n-번째-모멘트를-구해보자&quot;&gt;n 번째 모멘트를 구해보자.&lt;/h3&gt;
&lt;p&gt;$M_X^{(n)}(t)={d^n\over dt^n}M_X(t)=E[x^ne^{tx}]$&lt;/p&gt;

&lt;p&gt;$M_X^{(0)}(t=0)=E[x^n]$&lt;/p&gt;

&lt;h3 id=&quot;그렇다면-왜-mgf를-n번-미분하면-n번째-모멘트가-나올까&quot;&gt;그렇다면 왜 MGF를 n번 미분하면 n번째 모멘트가 나올까?&lt;/h3&gt;
&lt;p&gt;$e^x=1+x+{x^2\over2!}+{x^3\over3!}+…+{x^n\over n!}$&lt;/p&gt;

&lt;p&gt;$e^{tx}=1+tx+{(tx)^2\over2!}+{(tx)^3\over3!}+…+{(tx)^n\over n!}$&lt;/p&gt;

&lt;p&gt;$E[e^{tx}]=E(1+tx+{(tx)^2\over2!}+{(tx)^3\over3!}+…+{(tx)^n\over n!})$&lt;/p&gt;

&lt;p&gt;$=E[1]+tE[x]+{t^2\over2!}E[x^2]+{t^3\over3!}E[x^3]+…+{t^n\over n!}E[x^n]$&lt;/p&gt;

&lt;p&gt;${d\over dt}E[e^{tx}]={d\over dt}(E[1]+tE[x]+{t^2\over2!}E[x^2]+{t^3\over3!}E[x^3]+…+{t^n\over n!}E[x^n])$&lt;/p&gt;

&lt;p&gt;$t=0$&lt;/p&gt;

&lt;p&gt;${d\over dt}E[e^{tx}]=E[x]$&lt;/p&gt;

&lt;h2 id=&quot;moment-generating-functions---binomial&quot;&gt;Moment Generating Functions - Binomial&lt;/h2&gt;
&lt;p&gt;$M_X(t)=E[e^{tx}]$&lt;/p&gt;

&lt;p&gt;$=\sum_{x=0}^{n}e^{tx}{n\choose x}p^x(1-p)^{n-x}$&lt;/p&gt;

&lt;p&gt;$=\sum_{x=0}^{n}{n\choose x}(pe^t)^x(1-p)^{n-x}$&lt;/p&gt;

&lt;p&gt;$=(pe^t+1-p)^n(\because 이항정리)$&lt;/p&gt;

&lt;h3 id=&quot;첫-번째-모멘트&quot;&gt;첫 번째 모멘트&lt;/h3&gt;
&lt;p&gt;$M’_X(t)=n(pe^t+1-p)pe^t$&lt;/p&gt;

&lt;p&gt;$M’_X(t=0)=n(p+1-p)p=np$&lt;/p&gt;

&lt;p&gt;Binomial Distribution에서 배웠던 기댓값과 같다는 것을 확인해볼 수 있다.&lt;/p&gt;

&lt;p&gt;두 번째 모멘트 또한 같은 방법으로 구할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;moment-generating-functions---poisson&quot;&gt;Moment Generating Functions - Poisson&lt;/h2&gt;
&lt;p&gt;$M_X(t)=E[e^{tx}]$&lt;/p&gt;

&lt;p&gt;$=\sum_{x=0}^{\infty}e^{tx}(e^{-\lambda}\lambda^x/x!)$&lt;/p&gt;

&lt;p&gt;$=e^{-\lambda}\sum_{x=0}^{\infty}((\lambda e^t)^x/x!)$&lt;/p&gt;

&lt;p&gt;$=e^{-\lambda}(e^{\lambda})^{e^t}(\because Taylor\;Series)$&lt;/p&gt;

&lt;h3 id=&quot;첫-번째-모멘트-1&quot;&gt;첫 번째 모멘트&lt;/h3&gt;
&lt;p&gt;$M’_X(t)=e^{-\lambda}\lambda e^t(e^{\lambda})^{e^t}$&lt;/p&gt;

&lt;p&gt;$M’_X(t=0)=e^{-\lambda}\lambda e^{\lambda}=\lambda$&lt;/p&gt;

&lt;h2 id=&quot;moment-generating-functions---geometric&quot;&gt;Moment Generating Functions - Geometric&lt;/h2&gt;
&lt;h2 id=&quot;moment-generating-functions---exponential&quot;&gt;Moment Generating Functions - Exponential&lt;/h2&gt;
&lt;p&gt;$M_X(t)=E[e^{tx}]$&lt;/p&gt;

&lt;p&gt;$=\int_{x=0}^{\infty}e^{tx}\lambda e^{-\lambda x}dx$&lt;/p&gt;

&lt;p&gt;$=\lambda\int_{x=0}^{\infty}e^{-(\lambda-t)}dx$&lt;/p&gt;

&lt;p&gt;$=\lambda/\lambda-t\quad(\lambda&amp;gt;t)$&lt;/p&gt;

&lt;h3 id=&quot;첫-번째-모멘트-2&quot;&gt;첫 번째 모멘트&lt;/h3&gt;
&lt;p&gt;$M’_X(t)=-\lambda (-1)/(\lambda-t)^2=\lambda/(\lambda-t)^2$&lt;/p&gt;

&lt;p&gt;$M’_X(t=0)=1/\lambda$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Variance, Covariance, Correlation</title><link href="http://localhost:4000/posts/2020/06/21/var_cov_cor" rel="alternate" type="text/html" title="Variance, Covariance, Correlation" /><published>2020-06-21T00:00:00-07:00</published><updated>2020-06-21T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/21/var_cov_cor</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/21/var_cov_cor">&lt;iframe src=&quot;https://www.youtube.com/embed/dpnDuSPM1XU&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;기댓값-복습&quot;&gt;기댓값 복습&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;X가 이산형
$E[X]=\sum_ix_if_X(x_i)$&lt;br /&gt;
$E[g(x)]=\sum_ig(x_i)p(x_i)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;X가 연속형
$E[X]=\int xf(x)\;dx$&lt;br /&gt;
$E[X]=\int g(x)f(x)\;dx$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;기대값으로부터 유도되는 것들도 있는데, 대표적으로 Variance가 있다.&lt;/p&gt;

&lt;p&gt;$V[X]=E[(X-E[X])^2]$&lt;/p&gt;

&lt;h2 id=&quot;기댓값의-특성---independence&quot;&gt;기댓값의 특성 - Independence&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;독립인 확률 변수 X, Y가 있고, 임의의 함수 h, g가 있을 때, 다음을 만족한다.
&lt;script type=&quot;math/tex&quot;&gt;E[g(X)h(Y)]=E[g(X)]E[h(Y)]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;증명&lt;br /&gt;
$E[g(X)h(Y)]$&lt;br /&gt;
$=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x)h(y)f_{X,Y}(x,y)dxdy$&lt;br /&gt;
$=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x)h(y)f_{X}(x)f_{Y}(y)dxdy$&lt;br /&gt;
$=\int_{-\infty}^{\infty}g(x)f_{X}(x)dx\int_{-\infty}^{\infty}h(y)f_{Y}(y)dy$&lt;br /&gt;
$=E[g(X)]E[h(Y)]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;covariance-variance-of-sums-and-correlation&quot;&gt;Covariance, Variance of Sums, and Correlation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;두 확률 변수의 Covariance(공분산)은 두 확률 변수의 관계를 알려준다.
&lt;script type=&quot;math/tex&quot;&gt;Cov(X,Y)=E[(X-E[X])(Y-E[Y])]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;식에서 볼 수 있듯이 Covariance도 Expectation으로부터 나온 것임&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;식을 한번 풀어보자.&lt;br /&gt;
$Cov(X,Y)$&lt;br /&gt;
$=E[(X-E[X])(Y-E[Y])]$&lt;br /&gt;
$=E[XY-XE[Y]-YE[X]+E[X]E[Y]]$&lt;br /&gt;
$=E[XY]-E[X]E[Y]-E[Y]E[X]+E[X]E[Y]$&lt;br /&gt;
$=E[XY]-E[X]E[Y]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, $Cov(X,Y)=E[XY]-E[X]E[Y]$로 표현할 수 있다. 앞으로는 이 공식을 사용할 것이다.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-covariance&quot;&gt;Properties of Covariance&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;두 확률 변수 X, Y가 독립이라면, $Cov(X, Y)=0$이다.&lt;/li&gt;
  &lt;li&gt;하지만 $Cov(X, Y)=0$이라고 해서 두 확률 변수 X, Y가 독립인 것은 아니다.&lt;/li&gt;
  &lt;li&gt;$Cov(X,Y)=Cov(Y,X)$&lt;/li&gt;
  &lt;li&gt;$Cov(X,X)=Var(X)$&lt;/li&gt;
  &lt;li&gt;$Cov(aX,Y)=aCov(X,Y)$&lt;/li&gt;
  &lt;li&gt;$Cov(\sum_{i=1}^{n}X_i+\sum_{j=1}^{m}Y_i)=\sum_{i=1}^{n}\sum_{j=1}^{m}Cov(X_i, Y_j)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variance-of-a-sum-of-random-variables&quot;&gt;Variance of a Sum of Random Variables&lt;/h2&gt;
&lt;p&gt;확률 변수 $X_1, X_2, …, X_n$의 합의 분산은 다음과 같다.&lt;/p&gt;

&lt;p&gt;$Var(\sum_{i=1}^nX_i)$&lt;br /&gt;
$=Cov(\sum_{i=1}^nX_i, \sum_{j=1}^nX_j)$&lt;br /&gt;
$=\sum_{i=1}^n\sum_{j=1}^nCov(X_i, X_j)$&lt;br /&gt;
$=\sum_{i=1}^nVar(X_i)+\sum_{i\neq j}Cov(X_i, X_j)$&lt;br /&gt;
$=\sum_{i=1}^nVar(X_i)+2\sum_{i&amp;lt; j}Cov(X_i, X_j)$&lt;br /&gt;
$(\because Cov(X_i, X_j)=Cov(X_j, X_i))$&lt;/p&gt;

&lt;p&gt;여기서 본것처럼 논문 등에서도 $\sum_{i\neq j}$를 $2\sum_{i&amp;lt; j}$로 많이 표기한다.&lt;/p&gt;

&lt;h2 id=&quot;variance-of-a-sum-of-random-variables---example&quot;&gt;Variance of a Sum of Random Variables - Example&lt;/h2&gt;
&lt;p&gt;$Var(X_1+X_2)$&lt;br /&gt;
$=Cov(X_1+X_2, X_1+X_2)$&lt;br /&gt;
$=Cov(X_1+X_2, X_1)+Cov(X_1+X_2, X_2)$&lt;br /&gt;
$=Cov(X_1, X_1)+Cov(X_1, X_2)+Cov(X_1, X_2)+Cov(X_2, X_2)$&lt;br /&gt;
$=Var(X_1)+2Cov(X_1,X_2)+Var(X_2)$&lt;/p&gt;

&lt;h2 id=&quot;variance-of-a-sum-of-random-variables---independence&quot;&gt;Variance of a Sum of Random Variables - Independence&lt;/h2&gt;
&lt;p&gt;만약 $X_1, X_2, …, X_n$이 pairwise independent($i\neq j$일 때 $X_i, X_j$는 독립)라면 다음을 만족한다.&lt;/p&gt;

&lt;p&gt;$Var(\sum_{i=1}^nX_i)=Var(X_1)+Var(X_2)+…+Var(X_n)$&lt;/p&gt;

&lt;h2 id=&quot;variance-of-a-sum-of-random-variables---sample-variance&quot;&gt;Variance of a Sum of Random Variables - Sample Variance&lt;/h2&gt;
&lt;p&gt;$X_1, X_2, …, X_n$을 평균이 $\mu$이고, 분산이 $\sigma^2$이며, iid인 확률 변수라고 하자. sample의 평균을 $\bar{X}={1\over n}\sum_{i=1}^nX_i$이라고 할 수 있으며, $X_i-\bar{X}$를 deviation이라고 부른다.&lt;/p&gt;

&lt;p&gt;이 때, 확률 변수 $S^2=\sum_{i=1}^n{(X_i-\bar{X})^2\over n-1}$을 sample variance라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;variance-of-a-sum-of-random-variables---examle-1&quot;&gt;Variance of a Sum of Random Variables - Examle 1&lt;/h2&gt;
&lt;p&gt;파라미터 n과 p를 가지는 이항 분포의 분산을 구하라.&lt;/p&gt;

&lt;p&gt;X는 파라미터 p를 가지는 n개의 독립인 베르누이 확률 변수의 합으로 두자. 즉, $X=X_1+X_2+…+X_n$이다.&lt;/p&gt;

&lt;p&gt;$Var(X)$&lt;br /&gt;
$=Var(X_1)+Var(X_2)+…+Var(X_n)$&lt;br /&gt;
$=p(1-p)+p(1-p)+…+p(1-p)$&lt;br /&gt;
$np(1-p)$&lt;/p&gt;

&lt;h2 id=&quot;variance-of-a-sum-of-random-variables---examle-2&quot;&gt;Variance of a Sum of Random Variables - Examle 2&lt;/h2&gt;
&lt;p&gt;$X_1, X_2, …, X_n$을 iid이며, 분산이 $\sigma^2$이라고 하자. $Cov(X_i-\bar{X}, \bar{X})=0$임을 보여라.&lt;/p&gt;

&lt;p&gt;$Cov(X_i-\bar{X}, \bar{X})$&lt;br /&gt;
$=Cov(X_i, \bar{X})-Cov(\bar{X},\bar{X})$&lt;br /&gt;
$=Cov(X_i, {1\over n}\sum_{j=1}^nX_j)-Var(\bar{X})$&lt;br /&gt;
$={1\over n}\sum_{j=1}^nCov(X_i, X_j)-{\sigma^2\over n}$&lt;br /&gt;
$={1\over n}Cov(X_i, X_i)-{\sigma^2\over n}$&lt;br /&gt;
$={1\over n}\sigma^2-{\sigma^2\over n}$&lt;br /&gt;
$=0$&lt;/p&gt;

&lt;h2 id=&quot;correlation&quot;&gt;Correlation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;상관계수&lt;/li&gt;
  &lt;li&gt;두 확률 변수 X, Y의 상관계수 $\rho(X, Y)$는 다음과 같이 정의된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\rho(X, Y)={Cov(X, Y)\over \sqrt{Var(X)Var(Y)}}, \quad -1\le \rho(X, Y)\le 1&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Covariance와 비슷한 개념으로 생각하면 됨. 다만 Correlation은 -1과 1 사이에 떨어지도록 설계한 값. 즉, Scaled version of Covariance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Covariance가 10이라고 한다면 그것이 큰 것인지 작은 것인지 알 수 없으나, Correlation이 0.9라고 한다면 이것은 명백하게 큰 것이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Conditional Distribution</title><link href="http://localhost:4000/posts/2020/06/14/conditional_distribution" rel="alternate" type="text/html" title="Conditional Distribution" /><published>2020-06-14T00:00:00-07:00</published><updated>2020-06-14T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/14/conditional_distribution</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/14/conditional_distribution">&lt;iframe src=&quot;https://www.youtube.com/embed/6GM_cxHn6FY&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;conditional-distribution&quot;&gt;Conditional Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;사건 $E, F$가 있을 때 조건부 확률 $P(E|F)={P(E\cap F)\over P(F)}$이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$X, Y$가 이산 확률 변수일 때, Y가 주어졌을 때 X의 conditional PMF는 다음과 같다.&lt;br /&gt;
$P_{X|Y}(x|y)$&lt;br /&gt;
$=P(X=x|Y=y)$&lt;br /&gt;
$={P(X=x, Y=y)\over P(Y=y)}$&lt;br /&gt;
$={p_{XY}(x,y)\over p_Y(y)}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$X, Y$가 연속 확률 변수일 때, Y가 주어졌을 때 X의 conditional PDF는 다음과 같다.&lt;br /&gt;
$P(X\in A|Y=y)$&lt;br /&gt;
$=\int_Af_{X|Y}(x|y)\;dx$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conditional-distribution---example-1&quot;&gt;Conditional Distribution - Example 1&lt;/h2&gt;
&lt;p&gt;확률 변수 $X, Y$의 결합 확률 분포 $p_{XY}(x,y)$는 다음과 같다.&lt;/p&gt;

&lt;p&gt;$P(0,0)=0.4$&lt;br /&gt;
$P(0,1)=0.2$&lt;br /&gt;
$P(1,0)=0.1$&lt;br /&gt;
$P(1,1)=0.3$&lt;/p&gt;

&lt;p&gt;이 때, $Y=1$일 때의 X의 conditional PMF를 구하라.&lt;/p&gt;

&lt;p&gt;$P_{X|Y}(X=0|Y=1)={P_{XY}(0,1)\over P_Y(1)}={0.2\over 0.5}=0.4$&lt;br /&gt;
$P_{X|Y}(X=1|Y=1)={P_{XY}(1,1)\over P_Y(1)}={0.3\over 0.5}=0.6$&lt;/p&gt;

&lt;h2 id=&quot;conditional-distribution---example-2&quot;&gt;Conditional Distribution - Example 2&lt;/h2&gt;
&lt;p&gt;확률 변수 $X, Y$의 결합 확률 분포 $f_{XY}(x,y)$는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_{XY}(x,y)=
\begin{cases}
{12\over 5}x(2-x-y) &amp; 0 &lt; x &lt;1, 0 &lt; y &lt;1 \\
0 &amp; else \\
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;$f_{X|Y}(x|y)$&lt;br /&gt;
$={f_{XY}(x,y)\over  f_Y(y)}$&lt;br /&gt;
$={f_{XY}(x,y)\over \int_0^1f(x,y)dx}$&lt;br /&gt;
$={f_{XY}(x,y)\over \int_0^1f(x,y)dx}$&lt;br /&gt;
$={ {12\over 5}x(2-x-y))\over {12\over 5}\int_0^1x(2-x-y)dx}$&lt;br /&gt;
$={x(2-x-y))\over {2\over 3}-{y\over 2}}$&lt;br /&gt;
$={6x(2-x-y)\over 4-3y}$&lt;/p&gt;

&lt;h2 id=&quot;conditional-distribution---independent&quot;&gt;Conditional Distribution - Independent&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이산형 확률 변수
$P_{X|Y}(x|y)=P(X=x)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;연속형 확률 변수
$f_{X|Y}(x|y)=f_X(x)$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conditional-distribution---independent---example&quot;&gt;Conditional Distribution - Independent - Example&lt;/h2&gt;
&lt;p&gt;$X, Y$가 서로 독립이며, 파라미터가 $\lambda_1, \lambda_2$인 포아송 분포를 따를 때, $X+Y=n$이 주어졌을 때 $X$의 conditional PMF를 구하라.&lt;/p&gt;

&lt;p&gt;$P(X=k|X+Y=n)$&lt;br /&gt;
$={P(X=k,X+Y=n)\over P(X+Y=n)}$&lt;br /&gt;
$={P(X=k,Y=n-k)\over P(X+Y=n)}$&lt;br /&gt;
$={P(X=k)P(Y=n-k)\over P(X+Y=n)}$&lt;br /&gt;
$={e^{-\lambda_1}\lambda_1^k\over k!}{e^{-\lambda_2}\lambda_2^{n-k}\over (n-k)!}[{e^{-(\lambda_1+\lambda_2)}(\lambda_1+\lambda_2)^{n}\over n!}]^{-1}\quad (\because X+Y\sim Poisson(\lambda_1, \lambda_2))$&lt;br /&gt;
$\sim Binomial(n,{1\over\lambda_1+\lambda_2})$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Joint Probability Distribution - Convolution</title><link href="http://localhost:4000/posts/2020/06/13/convolution" rel="alternate" type="text/html" title="Joint Probability Distribution - Convolution" /><published>2020-06-13T00:00:00-07:00</published><updated>2020-06-13T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/13/convolution</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/13/convolution">&lt;iframe src=&quot;https://www.youtube.com/embed/Uc5MuEfGZ6Q&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;sums-of-independent-random-variables&quot;&gt;Sums of Independent Random Variables&lt;/h2&gt;
&lt;p&gt;확률 변수 $X, Y$는 독립이다. $X + Y$의 확률 분포는 무엇인가&lt;/p&gt;

&lt;p&gt;우선은 $X+Y$의 누적 확률 함수를 구해보자.&lt;/p&gt;

&lt;p&gt;$F_{XY}(a)=P[X+Y &amp;lt; a]$&lt;/p&gt;

&lt;p&gt;$=\int\int_{X+Y &amp;lt; a}f_{XY}(xy)\;dx\;dy$&lt;/p&gt;

&lt;p&gt;$=\int\int_{X+Y &amp;lt; a}f_{X}(x)f_{Y}(y)\;dx\;dy$&lt;/p&gt;

&lt;p&gt;$=\int_{-\infty}^{\infty}\int_{-\infty}^{a-y} f_{X}(x)f_{Y}(y)\;dx\;dy$&lt;/p&gt;

&lt;p&gt;$=\int_{-\infty}^{\infty}\int_{-\infty}^{a-y} f_{X}(x)\;dxf_{Y}(y)\;dy$&lt;/p&gt;

&lt;p&gt;$=\int_{-\infty}^{\infty}F_X(a-y)f_{Y}(y)\;dy$&lt;/p&gt;

&lt;p&gt;$F_{X+Y}$는 $F_X$, $F_Y$의 convolution이라고 한다.&lt;/p&gt;

&lt;p&gt;convolution($F_{X+Y}$)을 미분해서 PDF인 $f_{XY}$를 구할 수 있다.&lt;/p&gt;

&lt;p&gt;$f_{XY}$&lt;/p&gt;

&lt;p&gt;$={d\over da}\int_{-\infty}^{\infty}F_X(a-y)f_Y(y)\;dy$&lt;/p&gt;

&lt;p&gt;$=\int_{-\infty}^{\infty}{d\over da}F_X(a-y)f_Y(y)\;dy$&lt;/p&gt;

&lt;p&gt;$=\int_{-\infty}^{\infty}f_X(a-y)f_Y(y)\;dy$&lt;/p&gt;

&lt;p&gt;즉, 확률 변수 $X, Y$가 독립이라면 $X+Y$의 PDF는 $\int_{-\infty}^{\infty}f_X(a-y)f_Y(y)\;dy$와 같이 표현한다.&lt;/p&gt;

&lt;h2 id=&quot;identically-distributed-uniform-random-variables&quot;&gt;Identically Distributed Uniform Random Variables&lt;/h2&gt;
&lt;p&gt;X, Y가 독립이며, Uniform Distribution을 따르며, 파라미터는 (0,1)이다. 이 때 $X+Y$의 PDF를 구하라.&lt;/p&gt;

&lt;p&gt;$f_{X+Y}(a)=\int_{0}^{1}f_X(a-y)f_Y(y)\;dy$&lt;/p&gt;

&lt;p&gt;$=\int_{0}^{1}f_X(a-y)\;dy$  ($\because f_Y(y)=1$)&lt;/p&gt;

&lt;p&gt;$Let\;a-y=t,\;dy=-dt$&lt;/p&gt;

&lt;p&gt;$=\int_{a}^{a-1}f_X(t)\;(-dt)$&lt;/p&gt;

&lt;p&gt;$=\int_{a}^{a-1}f_X(t)\;(-dt)$&lt;/p&gt;

&lt;p&gt;$=\int_{a-1}^{a}f_X(t)\;dt$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 1.&lt;/strong&gt; $0\le a \le 1$&lt;/p&gt;

&lt;p&gt;$\int_{0}^{a}f_X(t)\;dt=a$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 2.&lt;/strong&gt; $1 &amp;lt; a &amp;lt; 2$&lt;/p&gt;

&lt;p&gt;$\int_{a-1}^{1}f_X(t)\;dt=2-a$&lt;/p&gt;

&lt;p&gt;&amp;lt;그림 1&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;gamma-random-variables&quot;&gt;Gamma Random Variables&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;서로 독립이며, 파라미터가 $(s,\lambda)$, $(t,\lambda)$인 감마 분포를 따르는 확률 변수 X, Y가 있다. 이 때, X+Y는 파라미터가 $(s+t, \lambda)$인 감마 분포를 따른다.&lt;/li&gt;
  &lt;li&gt;일반화 시켜 보면, 서로 독립이며, 파라미터가 $(t_1,\lambda)$, …, $(t_n,\lambda)$인 감마 분포를 따르는 확률 변수 $X_1$, …, $X_n$이 있다. 이 때, $X_1+…+X_n$는 파라미터가 $(\sum_{i=1}^{n}t_i, \lambda)$인 감마 분포를 따른다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gamma-and-exponential-random-variables&quot;&gt;Gamma and Exponential Random Variables&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;파라미터가 $\lambda$이며, 지수 분포를 따르는 확률 변수 $X_1$, …, $X_n$이 있다. 이 때, $X_1+…+X_n$는 파라미터가 $(n, \lambda)$인 감마 분포를 따른다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;증명 생략&lt;/p&gt;

&lt;h2 id=&quot;normal-random-distribution&quot;&gt;Normal Random Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;파라미터가 $(\mu_i, \sigma_i)$이며, 정규 분포를 따르는 확률 변수 $X_1$, …, $X_n$이 있다. 이 때, $X_1+…+X_n$는 파라미터가 $(\sum_{i=1}^{n}\mu_i, \sum_{i=1}^{n}\sigma_i^2)$인 감마 분포를 따른다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;poisson-random-distribution&quot;&gt;Poisson Random Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;파라미터가 $\lambda_i$이며, 포아송 분포를 따르는 확률 변수 $X_1$, …, $X_n$이 있다. 이 때, $X_1+…+X_n$는 파라미터가 $\sum_{i=1}^{n}\lambda_i$인 포아송 분포 분포를 따른다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;binomial-random-distribution&quot;&gt;Binomial Random Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;파라미터가 $(n_i, p)$이며, 이항 분포를 따르는 확률 변수 $X_1$, …, $X_n$이 있다. 이 때, $X_1+…+X_n$는 파라미터가 $(\sum_{i=1}^{n}n_i, p)$인 이항 분포를 따른다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Joint Probability Distribution(2)</title><link href="http://localhost:4000/posts/2020/06/11/joint_probability_distribution2" rel="alternate" type="text/html" title="Joint Probability Distribution(2)" /><published>2020-06-11T00:00:00-07:00</published><updated>2020-06-11T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/06/11/joint_probability_distribution2</id><content type="html" xml:base="http://localhost:4000/posts/2020/06/11/joint_probability_distribution2">&lt;iframe src=&quot;https://www.youtube.com/embed/XOP_ENqWdJ0&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;marginal-probability-function주변-확률-변수&quot;&gt;Marginal Probability Function(주변 확률 변수)&lt;/h2&gt;
&lt;h3 id=&quot;이산형&quot;&gt;이산형&lt;/h3&gt;
&lt;p&gt;$g(x)=\sum_{y}p_{XY}(x,y)$ - Summation of $p_{XY}(x,y)$ over the values of $Y$&lt;br /&gt;
$h(y)=\sum_{x}p_{XY}(x,y)$ - Summation of $p_{XY}(x,y)$ over the values of $X$&lt;/p&gt;

&lt;h3 id=&quot;연속형&quot;&gt;연속형&lt;/h3&gt;
&lt;p&gt;$g(x)=\int_{-\infty}^{\infty}f_{XY}(x,y)\;dy$ - Integral of $f_{XY}(x,y)$ with respect to $Y$ over the values of $Y$.
$h(y)=\int_{-\infty}^{\infty}f_{XY}(x,y)\;dx$ - Integral of $f_{XY}(x,y)$ with respect to $X$ over the values of $X$.&lt;/p&gt;

&lt;h3 id=&quot;x와-y의-결합-확률-분포로부터-x의-분포를-구할-수-있다&quot;&gt;$X$와 $Y$의 결합 확률 분포로부터 $X$의 분포를 구할 수 있다.&lt;/h3&gt;

&lt;p&gt;$F_{X}(a)$&lt;br /&gt;
$=P[X\le a]$&lt;br /&gt;
$=P[X\le a, Y &amp;lt; \infty]$&lt;br /&gt;
$=\lim_{b\to\infty}P[X\le a, Y &amp;lt; b]$ (결합 누적 확률 분포)&lt;br /&gt;
$=\lim_{b\to\infty}F_{XY}(a,b)$&lt;/p&gt;

&lt;h2 id=&quot;independent-random-variables독립-확률-변수&quot;&gt;Independent Random Variables(독립 확률 변수)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;확률 변수 $X$와 $Y$는 다음 조건을 만족할 때 독립이다.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P[X\in A, Y\in B]=P[X\in A]P[Y\in B]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이를 일반화 시켜보면, 확률 변수 $X_1, X_2, …, X_n$은 다음 조건을 만족할 때 독립이다.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P[X_1\in A_1, ..., X_n\in A_n]=P[X_1\in A_1],..., P[X_n\in A_n]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 결합 확률 함수를 각각의 주변 확률 변수의 곱으로 표현할 수 있다면 독립이다.&lt;/p&gt;

&lt;p&gt;independent $\Leftrightarrow$ dependent&lt;/p&gt;

&lt;p&gt;Loosely speaking, 분포 하나를 알더라도 그것이 다른 분포에 영향을 미치지 않을 때 독립이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;증명&quot;&gt;증명&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이산형&lt;br /&gt;
$P[X\in A, Y\in B]$&lt;br /&gt;
$=\sum_{y\in B}\sum_{x\in A}p(x,y)$&lt;br /&gt;
$=\sum_{y\in B}\sum_{x\in A}p_X(x)p_Y(y)$&lt;br /&gt;
$=\sum_{x\in A}p_X(x)\sum_{y\in B}p_Y(y)$&lt;br /&gt;
$=P[X\in A]P[Y\in B]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;연속형&lt;br /&gt;
$P[X\in A, Y\in B]$&lt;br /&gt;
$=\int_{y\in B}\int_{x\in A}f(x,y)\;dxdy$&lt;br /&gt;
$=\int_{y\in B}\int_{x\in A}f_X(x)f_Y(y)\;dxdy$&lt;br /&gt;
$=\int_{x\in A}f_X(x)\;dx\int_{y\in B}f_Y(y)\;dy$&lt;br /&gt;
$=P[X\in A]P[Y\in B]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;independent-random-variables---example&quot;&gt;Independent Random Variables - Example&lt;/h2&gt;
&lt;p&gt;$X$, $Y$의 결합 확률 분포가 다음과 같다. $X\sim Geometric(\theta)$, $Y\sim Bernoulli(\theta)$를 따른다고 할 때, $X$, $Y$는 독립인가?&lt;/p&gt;

&lt;p&gt;$P_{XY}(x,y)$&lt;br /&gt;
$=\theta^{y+1}(1-\theta)^{x-y}$&lt;br /&gt;
$=\theta^{y}\theta(1-\theta)^{x-1+1-y}$&lt;br /&gt;
$=\theta^{y}\theta(1-\theta)^{x-1+1-y}$&lt;br /&gt;
$=(1-\theta)^{x-1}\theta\cdot\theta^{y}(1-\theta)^{1-y}$&lt;br /&gt;
$=p_X(x)\cdot p_Y(y)$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry></feed>