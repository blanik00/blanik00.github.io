<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-23T07:41:59-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JeongUk Jang</title><subtitle>Github Pages template for academic personal websites, forked from mmistakes/minimal-mistakes</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><entry><title type="html">Geometric, Negative Binomial, Hypergeometric distribution</title><link href="http://localhost:4000/posts/2020/05/23/geo_neg_hyp" rel="alternate" type="text/html" title="Geometric, Negative Binomial, Hypergeometric distribution" /><published>2020-05-23T00:00:00-07:00</published><updated>2020-05-23T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/23/geo_neg_hyp</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/23/geo_neg_hyp">&lt;iframe src=&quot;https://www.youtube.com/embed/IYJFMjYRMgM&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;geometric-distribution기하-분포&quot;&gt;Geometric Distribution(기하 분포)&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : 첫 번째 성공이 일어날 때 까지 필요한 시행 횟수&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;베르누이 시행 활용&lt;/li&gt;
  &lt;li&gt;예를 들어, 다섯 번째 시행에서 첫 성공이 일어났다면, 확률은 $(1-p)^4p$이다.&lt;/li&gt;
  &lt;li&gt;$P(X=n)=(1-p)^{n-1}p, \;\;n=1,2,3,…$&lt;/li&gt;
  &lt;li&gt;기하 확률 함수의 합은 1이 될까?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\sum_{n=1}^{\infty}P(X=n)=p\sum_{n=1}^{\infty}(1-p)^{n-1}={p\over1-(1-p)}=1$&lt;/p&gt;

&lt;h2 id=&quot;mean-and-variance-of-geometric-distribution&quot;&gt;Mean and Variance of Geometric Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$E[X]=1/p,\;\;V[X]=(1-p)/p^2$&lt;/li&gt;
  &lt;li&gt;$E[X]$&lt;br /&gt;
$=\sum_{i=1}^{\infty}i(1-p)^{i-1}p$&lt;br /&gt;
$=p\sum_{i=1}^{\infty}i(1-p)^{i-1}$&lt;br /&gt;
$Let\;\; 1-p=k$&lt;br /&gt;
$=p\sum_{i=1}^{\infty}ik^{i-1}$&lt;br /&gt;
$=p\sum_{i=1}^{\infty}{d\over dk}k^i$&lt;br /&gt;
$=p{d\over dk}\sum_{i=1}^{\infty}k^i$&lt;br /&gt;
$=p{d\over dk}{k\over 1-k}$&lt;br /&gt;
$=p{1-k+k\over (1-k)^2}$&lt;br /&gt;
$=p{1\over p^2}$&lt;br /&gt;
$={1\over p}$&lt;/li&gt;
  &lt;li&gt;$E[X^2]={2-p\over p^2}$&lt;/li&gt;
  &lt;li&gt;$V[X]=E[X^2]-E[X]^2={1-p\over p^2}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;geometric-distribution---example&quot;&gt;Geometric Distribution - Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;항아리에 $N$개의 흰공과 $M$개의 검은공이 있다. 검은공을 뽑을 때 까지 반복해서 뽑는 작업을 한다. (공을 뽑고 나면 다시 항아리에 넣어야 한다)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;RV X : 검은 공을 뽑을 때 까지 뽑기를 한 횟수&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;$P(W)={N\over N+M}$&lt;/li&gt;
  &lt;li&gt;$P(B)={M\over N+M}$&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$P(X=n)$
$=P(W)^{n-1}P(B)=({N\over N+M})^{n-1}{M\over N+M}={MN^{n-1}\over (N+M)^{n}}$&lt;/li&gt;
  &lt;li&gt;검은공이 뽑힐 때 까지 적어도 $k$번의 시행이 필요할 확률
$P(X\ge k)$&lt;br /&gt;
$={M\over N+M}\sum_{n=k}^{\infty}({N\over N+M})^{n-1}$&lt;br /&gt;
$={M\over N+M}({N\over N+M})^{k-1}/(1-{N\over N+M})$&lt;br /&gt;
$=({N\over N+M})^{k-1}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;즉, “k-1번 동안 흰공만 뽑는 확률”과 “검은공을 처음으로 뽑은 것이 k번째인 확률”은 동일하다.&lt;/p&gt;

&lt;h2 id=&quot;negative-binomial-distribution음이항-분포&quot;&gt;Negative Binomial Distribution(음이항 분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;기하 분포를 일반화 시킨 분포&lt;/li&gt;
  &lt;li&gt;확률 함수가 이항 분포와 유사해 이렇게 이름을 지은 것으로 보임&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : $r$번째 성공을 보기 위해 베르누이 시행을 몇 번 해야 하는가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;예를 들어, 5번째 베르누이 시행에서 3번째 성공을 볼 확률은, 앞의 네 번 동안 2번의 성공이 있어야 하며, 마지막인 다섯 번째 시행에서 성공하면 된다. ${4 \choose 2}p^2(1-p)^2p={4 \choose 2}p^3(1-p)^2$&lt;/li&gt;
  &lt;li&gt;parameter는 $r$과 성공 확률인 $p$이다.&lt;/li&gt;
  &lt;li&gt;$P(X=n)={n-1 \choose r-1}p^r(1-p)^{n-r},\;\; n=r,r+1,r+2,…$&lt;/li&gt;
  &lt;li&gt;$r=1$이면 앞에서 배운 기하분포와 같다.&lt;/li&gt;
  &lt;li&gt;$E[X]={r\over p}$&lt;/li&gt;
  &lt;li&gt;$V[X]={r(1-p)\over p^2}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;확률 변수의 기대값과 분산은 확률 함수의 파라미터의 함수 꼴로 표현된다.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;negative-binomial-distribution---example&quot;&gt;Negative Binomial Distribution - Example&lt;/h2&gt;
&lt;p&gt;주사위를 6이 세 번 나올 때 까지 던진다고 하자. 5번째 시행에서 세 번째 6이 나올 확률은 얼마인가?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RV X : 6이 세 번 나올 때 까지 주사위를 던진 횟수($x\ge3$)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;4번 중 2번은 6이 나와야 하고, 2번은 6이 나오면 안된다. 마지막인 다섯 번째 시행에서는 6이 나와야 한다.&lt;/p&gt;

&lt;p&gt;$P(X=5)$&lt;br /&gt;
$={4\choose 2}(1/6)^2(5/6)^{4-2}(1/6)$&lt;br /&gt;
$={4\choose 2}(1/6)^3(5/6)^{4-2}$&lt;br /&gt;
$={4\choose 2}(1/6)^3(5/6)^{4-2}$&lt;/p&gt;

&lt;h2 id=&quot;negative-binomial-distribution---hiring-example&quot;&gt;Negative Binomial Distribution - Hiring Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;회사에서 직원을 3명 고용하려고 한다. 지원자와 인터뷰를 해서 합격시킬 확률이 0.6이라고 할 때, 몇 번의 인터뷰가 필요한가?&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;RV X : 3명을 뽑을 때 까지 필요한 인터뷰 횟수($x \ge 3$)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;$P(X=6)={5\choose 2}0.6^30.4^3=0.138$&lt;/li&gt;
  &lt;li&gt;회사가 6명을 인터뷰할 예산만 가지고 있을 때, 6번 인터뷰 동안 3명을 고용할 수 있는 확률을 구하라.
$P(X\le 6)=P(X=3)+P(X=4)+P(X=5)+P(X=6)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;hypergeometric-distribution초기하-분포&quot;&gt;Hypergeometric Distribution(초기하 분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;주머니에 $N$개의 공이 있는데, $m$개는 흰 공이고, $N-m$개는 빨간 공이다. 주머니에서 $n$개를 비복원 추출로 뽑는다. 이 때 흰색 공의 갯수가 몇 개인지 나타내는 것이 초기하 분포이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;RV X : 샘플링한 $n$개 중 흰색 공의 갯수(X=0,1,2,…n)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;$P(X=i)={m\choose i}{N-m\choose n-i} / {N \choose n},\;\;i=0,1,2,…n$&lt;/li&gt;
  &lt;li&gt;분모 - $N$개 중 $n$개를 뽑는다.&lt;/li&gt;
  &lt;li&gt;분자 - $m$개 중 $i$개가 흰색 공이고, $N-m$개 중 $n-i$가 빨간 공이다.&lt;/li&gt;
  &lt;li&gt;파라미터는 $N$, $n$, $m$이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mean-and-variance-of-hypergeometric-distribution&quot;&gt;Mean and Variance of Hypergeometric Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$E[X]=nm/N$&lt;/li&gt;
  &lt;li&gt;$V[X]=n{m\over N}{(N-m)\over N}{N-n\over N-1}$&lt;/li&gt;
  &lt;li&gt;기대값과 분선은 파라미터의 함수인 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypergeometric-distribution---bad-unit-example&quot;&gt;Hypergeometric Distribution - Bad Unit Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;공장에서는 제품을 30개씩 제조한다. 여기에 5개의 불량품이 포함되어 있다고 하자. 30개 중 4개를 추출하여 품질 테스트를 수행한다.
    &lt;blockquote&gt;
      &lt;p&gt;RV X : 4개의 샘플 중 불량품의 갯수&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;불량품이 2개일 확률은 얼마인가?&lt;br /&gt;
$P(X=2)={5\choose 2}{30-5\choose 4-2} / {30\choose 4}$&lt;/li&gt;
  &lt;li&gt;불량품이 2개 미만일 확률은 얼마인가?&lt;br /&gt;
$P(X&amp;lt;2)=P(X=0)+P(X=1)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;hypergeometric-distribution---lot-defect-example&quot;&gt;Hypergeometric Distribution - Lot Defect Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;한 소비자가 물건을 사려고 한다. 이 소비자는 사려고 하는 10개의 물건 중 3개를 추출하여 이들이 모두 불량품이 아니라면 구매한다. 물건의 30%는 10개 중 4개의 불량이 있고, 70%는 10개 중 1개의 불량이 있다. 소비자가 물건을 구매하지 않을 확률을 구하라.&lt;/li&gt;
  &lt;li&gt;구매할 확률을 구해 1에서 빼주는 것이 계산하기 편하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;RV X : 3개 중 불량품의 갯수&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$P(X=0)=P(defective=4)P(X=0|defective=4)+P(defective=1)P(X=0|defective=1)$&lt;br /&gt;
$=54/100$&lt;br /&gt;
$\therefore 1-P(X=0)=46/100$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Poisson Distribution</title><link href="http://localhost:4000/posts/2020/05/22/poisson_distribution" rel="alternate" type="text/html" title="Poisson Distribution" /><published>2020-05-22T00:00:00-07:00</published><updated>2020-05-22T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/22/poisson_distribution</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/22/poisson_distribution">&lt;iframe src=&quot;https://www.youtube.com/embed/HJZ5Ev_p8Uo&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;복습&quot;&gt;복습&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sample Space(표본 공간)&lt;/strong&gt; : 실험으로 나오는 모든 결과를 담고 있는 집합&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Random Variable(확률 변수)&lt;/strong&gt; : sample space의 원소들을 숫자로 바꿔주는 함수&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Probability Function(확률 함수)&lt;/strong&gt; : Random Variable을 확률로 변환하는 함수&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Probability Distribution(확률 분포)&lt;/strong&gt; : Probability Function으로부터 나온 확률들의 전반적인 패턴&lt;/p&gt;

&lt;p&gt;확률 분포는 확률 함수의 input에 따라 두 가지로 나뉜다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;확률 함수의 input&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;확률 함수&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;확률 분포&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;discrete&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PMF(확률질량함수)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Discrete Probability Distribution(이산형 확률분포)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;continous&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PDF(확률밀도함수)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Continous Probability Distribution(연속형 확률분포)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;확률질량함수에서 나온 확률 분포. 즉, 이산형 확률분포는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Bernoulli Distribution&lt;/li&gt;
  &lt;li&gt;Binomial Distribution&lt;/li&gt;
  &lt;li&gt;Poisson Distribution&lt;/li&gt;
  &lt;li&gt;Geometric Distribution&lt;/li&gt;
  &lt;li&gt;Negative Binomial Distribution&lt;/li&gt;
  &lt;li&gt;Hypergeometric Distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;지난 시간에 Bernoulli, Binomial Distribution에 대해 배웠으며, &lt;strong&gt;이번에는 Poisson Distribution에 대해 배운다.&lt;/strong&gt; 또한 다음 시간에는 나머지 이산형 확률분포에 대해 배운다.&lt;/p&gt;

&lt;p&gt;확률밀도함수에서 나온 확률 분포. 즉, 연속형 확률분포는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Uniform Distribution&lt;/li&gt;
  &lt;li&gt;Normal Distribution&lt;/li&gt;
  &lt;li&gt;Exponential Distribution&lt;/li&gt;
  &lt;li&gt;Gamma Distribution&lt;/li&gt;
  &lt;li&gt;Beta Distribution&lt;/li&gt;
  &lt;li&gt;Chi-square Distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연속형 확률 분포는 나중에 다룬다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;지난 시간에 배운 Bernoulli, Binomial Distribution을 복습하자.&lt;/p&gt;

&lt;h3 id=&quot;bernoulli-distribution&quot;&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : 0 혹은 1만을 가지는 단순한 확률 변수&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;0과 1을 확률로 변환하려면 확률 함수(Probability Function)가 필요하다. 여기서 확률 함수의 input은 이산형이므로 Bernoulli Distribution은 이산형 확률 분포가 된다.&lt;/p&gt;

&lt;p&gt;확률 함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(x;p)=p^x(1-p)^{1-x}, x=0 \; or \; 1&lt;/script&gt;

&lt;p&gt;즉, Bernoulli Distribution은 Bernoulli 확률 함수로부터 생성되는 확률들의 분포를 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;binomial-distribution&quot;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Binomial Distribution은 Bernoulli Distribution으로부터 나온다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : Bernoulli trials를 $n$번 했을 때, 성공 횟수
parameter는 $n$, $p$이다.(parameter는 확률 분포의 모양의 결정할 때 중요한 값)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;성공 횟수를 확률로 변환하려면 확률 함수(Probability Function)가 필요하다. 여기서 확률 함수의 input은 이산형이므로 Binomial Distribution은 이산형 확률 분포가 된다.&lt;/p&gt;

&lt;p&gt;확률 함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)={n\choose x}p^x(1-p)^{1-x}\quad for \;x=0,1,2,...,n&lt;/script&gt;

&lt;p&gt;즉, Binomial Distribution은 Binomial 확률 함수로부터 생성되는 확률들의 분포를 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;poisson-distribution&quot;&gt;Poisson Distribution&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : 단위 시간 혹은 단위 공간 안에 특정 사건이 몇 번 발생할 것인가&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;한 번도 발생 안할 수도 있고(0), 1번 발생할 수도 있고, 2번 발생할 수도 있다. 즉, discrete하다.&lt;/p&gt;

&lt;p&gt;발생 횟수를 확률로 변환하려면 확률 함수(Probability Function)가 필요하다. 여기서 확률 함수의 input은 이산형이므로 Poisson Distribution은 이산형 확률 분포가 된다.&lt;/p&gt;

&lt;p&gt;확률 함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X=i)={e^{-\lambda}\lambda^i\over i!}, \quad i=0,1,2,...&lt;/script&gt;

&lt;p&gt;이것이 확률 함수가 되기 위해서는 더해서 1이 되어야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=0}^{\infty}P(X=i)=e^{-\lambda}\sum_{i=0}^{\infty}{\lambda^i\over i!}=e^{-\lambda}e^{\lambda}=1\; (\sum_{i=0}^{\infty}{\lambda^i\over i!}=e^{\lambda} \; by \; taylor \; series)&lt;/script&gt;

&lt;p&gt;parameter는 $\lambda$이다.&lt;/p&gt;

&lt;h2 id=&quot;poisson-approximation-to-binomial&quot;&gt;Poisson Approximation to Binomial&lt;/h2&gt;
&lt;p&gt;Binomial Distribution에서 parameter는 $n$과 $p$이다. $n$이 굉장히 크고, $p$가 굉장히 작을 때, Binomial Distribution을 parameter $\lambda=np$인 Poisson Distribution으로 근사할 수 있다.&lt;/p&gt;

&lt;p&gt;즉, $Binomial(i;n,p)\approx Poisson(i;\lambda), \quad \lambda = np, p={\lambda \over n}$&lt;/p&gt;

&lt;p&gt;$P(X=i)={n!\over(n-i)!i!}p^i(1-p)^{n-i}\approx e^{-\lambda} {\lambda^i\over i!}$
$={n!\over(n-i)!i!}({\lambda \over n})^i(1-{\lambda \over n})^{n-i}$
$={n(n-1)…(n-i+1)\over n^i}{\lambda^i\over i!}{(1-{\lambda \over n})^{n}\over (1-{\lambda \over n})^{i}}$&lt;/p&gt;

&lt;p&gt;마지막 식을 세 부분으로 나누어 푼다. $n$이 아주 크다고 했으므로 $n$을 무한대로 보내보자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;${n(n-1)…(n-i+1)\over n^i}$&lt;br /&gt;
$\lim_{n \to \infty}{n(n-1)…(n-i+1)\over n^i}=1$&lt;/li&gt;
  &lt;li&gt;$(1-{\lambda \over n})^{i}$&lt;br /&gt;
$\lim_{n \to \infty}(1-{\lambda \over n})^{i}=1$&lt;/li&gt;
  &lt;li&gt;$(1-{\lambda \over n})^{n}$&lt;br /&gt;
$\lim_{n \to \infty}(1-{\lambda \over n})^{n}=\lim_{n \to \infty}((1-{\lambda \over n})^{n\over \lambda})^{\lambda}=\lim_{n \to \infty}(e^{-1})^{\lambda}=\lim_{n \to \infty}(e^{-\lambda})$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;결과를 대입해보면 ${e^{-\lambda}\lambda^i\over i!}$가 나오며, 이는 포아송 분포의 확률 함수와 같다.&lt;/p&gt;

&lt;h2 id=&quot;example-of-poisson-random-variable&quot;&gt;Example of Poisson Random Variable&lt;/h2&gt;
&lt;p&gt;Poisson 분포는 단위 시간 혹은 단위 공간 내에 특정 사건이 몇 번 발생했는지를 나타낸다고 했는데, 조금 더 추가하자면 특정 사건은 자주 발생하는 것이 아니라 드물게 발생해야 포아송 분포로 모델링하기 좋다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 마을에서 100세 이상까지 사는 사람의 수&lt;/li&gt;
  &lt;li&gt;책의 한 페이지에서 오타의 개수&lt;/li&gt;
  &lt;li&gt;하룻동안 잘못 전화를 건 횟수&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mean-and-variance-of-poisson-distribution&quot;&gt;Mean and Variance of Poisson Distribution&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;$E[X]=V[X]=\lambda$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$E[X]=\sum_{i=0}^{\infty}{ie^{-\lambda}\lambda^{i}\over i!}$&lt;br /&gt;
$=\lambda\sum_{i=1}^{\infty}{e^{-\lambda}\lambda^{i-1}\over (i-1)!}$&lt;br /&gt;
$Let \;\;k=i-1$&lt;br /&gt;
$=\lambda\sum_{k=0}^{\infty}{e^{-\lambda}\lambda^{k}\over k!}$&lt;br /&gt;
$=\lambda e^{-\lambda}\sum_{k=0}^{\infty}{\lambda^{k}\over k!}$&lt;br /&gt;
$=\lambda e^{-\lambda}e^{\lambda}$&lt;br /&gt;
$=\lambda$&lt;/p&gt;

&lt;p&gt;아래도 비슷하게 풀 수 있다.&lt;br /&gt;
$E[X^2]=\lambda(\lambda+1)$&lt;br /&gt;
$V[X]=E[X^2]-{E[X]}^2=\lambda^2 + \lambda - \lambda^2=\lambda$&lt;/p&gt;

&lt;h2 id=&quot;poisson-distribution---quality-example&quot;&gt;Poisson Distribution - Quality Example&lt;/h2&gt;
&lt;p&gt;유리 제조사의 품질 관리자가 유리에 결함이 있는지 검사하고 있다. 유리에 있는 결함의 수를 $\lambda=0.5$인 Poisson Distribution으로  가정했을 때, 유리에 결함이 하나도 없을 확률은 얼마인가?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : Number of imperfections in a glass&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;우리는 확률을 구해야 하니 확률 함수를 이용해야 한다. 즉, 이 문제는 $P(X=0)$을 구하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;$P(X=0)={e^{-0.5}0.5^0\over 0!}=e^{-0.5}=0.607$&lt;/p&gt;

&lt;h2 id=&quot;poisson-distribution---radioactive-particle-example&quot;&gt;Poisson Distribution - Radioactive Particle Example&lt;/h2&gt;
&lt;p&gt;방사선 입자는 ms(millisecond) 동안 4개가 나온다. 2.5ms 동안 나오는 방사선 입자의 개수가 15일 확률을 구하라.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : Number of Radioactive Particle over 2.5ms period&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;우리는 확률을 구해야 하니 확률 함수를 이용해야 한다. 즉, 이 문제는 $P(X=15)$을 구하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;우리가 알고 있는 것은 ms 동안 4개의 입자가 나온다는 것이다. 이를 2.5ms로 바꾸면 10개의 입자가 나온다는 것이 된다. 즉, $\lambda=\lambda’t=4\times 2.5=10$이다.(보통 Poisson Distribution 문제를 풀 때 단위 시간을 1로 주는데 이 문제는 1이 아니라는 점을 주의해야 한다)&lt;/p&gt;

&lt;p&gt;$P(X=15)=f(15;10)={e^{-10}10^{15}\over 15!}=0.0348$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Bernoulli, Binomial Distribution</title><link href="http://localhost:4000/posts/2020/05/21/bern_bin" rel="alternate" type="text/html" title="Bernoulli, Binomial Distribution" /><published>2020-05-21T00:00:00-07:00</published><updated>2020-05-21T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/21/bern_bin</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/21/bern_bin">&lt;iframe src=&quot;https://www.youtube.com/embed/fP_udxUJez8&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;bernoulli-distribution&quot;&gt;Bernoulli Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;확률 변수가 두 개의 실수값(1,0)만 갖는 확률 분포&lt;/li&gt;
  &lt;li&gt;확률 변수를 확률로 바꾸려면 확률 함수가 필요(확률 질량 함수)&lt;/li&gt;
  &lt;li&gt;$f_X(x;p)=p^x(1-p)^{(1-x)},\; x=0,1$&lt;/li&gt;
  &lt;li&gt;즉, 베르누이 분포는 베르누이 확률 함수로부터 생성되는 확률들의 패턴&lt;/li&gt;
  &lt;li&gt;모수는 $p$이다.&lt;/li&gt;
  &lt;li&gt;$E[X]=\sum_{x=0,1} xp^x(1-p)^{(1-x)}=p$&lt;/li&gt;
  &lt;li&gt;$V[X]$
$E[X^2]=\sum_{x=0,1} x^2p^x(1-p)^{(1-x)}=p$&lt;br /&gt;
$V[X]=E[X]^2-E[X^2]=p^2-p=p(1-p)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;binomial-distribution&quot;&gt;Binomial Distribution&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;베르누이 시행을 독립적으로 $n$번 수행&lt;/li&gt;
  &lt;li&gt;확률 변수 X는 $n$번 수행해서 성공한 횟수이다.&lt;/li&gt;
  &lt;li&gt;$X=0,1,2,3,…n$&lt;/li&gt;
  &lt;li&gt;베르누이 분포는 이항 분포의 $n=1$인 특수한 케이스다.&lt;/li&gt;
  &lt;li&gt;이 실수값들을 확률로 바꿀 확률 함수를 정의해야 한다.&lt;/li&gt;
  &lt;li&gt;$p(x)={n \choose x}p^x(1-p)^{n-x}$&lt;/li&gt;
  &lt;li&gt;즉, 이항 분포는 이항 확률 함수로부터 나온 확률들의 패턴&lt;/li&gt;
  &lt;li&gt;모수는 $n$, $p$이다.&lt;/li&gt;
  &lt;li&gt;$\sum_{i=0}^{n}{n \choose x}p^x(1-p)^{n-x}=(p+1-p)^n=1^n=1$
$(\because (x+y)^n=\sum_{i=0}^{n}{n \choose i}x^iy^{n-i})$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;binomial-distribution-with-different-parameters&quot;&gt;Binomial Distribution with different parameters&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;모수 : 확률 분포의 모양을 결정하는 중요한 수&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82673486-e92ade80-9c7c-11ea-80c2-783030e73242.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mean--variance-of-binomial-distribution&quot;&gt;Mean &amp;amp; Variance of Binomial Distribution&lt;/h2&gt;
&lt;p&gt;$E[X]$&lt;br /&gt;
$=\sum_{i=0}^nx{n \choose i}p^i(1-p)^{n-i}$&lt;br /&gt;
$=\sum_{i=0}^n{in! \over (n-i)!i!}p^i(1-p)^{n-i}$&lt;br /&gt;
$=np\sum_{x=1}^n{i(n-1)! \over (n-i)!i!}p^{i-1}(1-p)^{n-i}$
$=np\sum_{x=1}^n{(n-1)! \over (n-i)!(i-1)!}p^{i-1}(1-p)^{n-i}$&lt;br /&gt;
$k=i-1$로 치환
$=np\sum_{k=1}^n{(n-1)! \over (n-k-1)!k!}p^{k}(1-p)^{n-1-k}$&lt;br /&gt;
$=np(p+(1-p)^{n-1})^{n-1}$&lt;br /&gt;
$(\because (x+y)^n=\sum_{i=0}^{n}{n \choose i}x^iy^{n-i})$&lt;br /&gt;
$=np$&lt;/p&gt;

&lt;p&gt;$V[X]=np(1-p)$&lt;/p&gt;

&lt;h2 id=&quot;binomial-distribution---defect-example-1&quot;&gt;Binomial Distribution - Defect Example 1&lt;/h2&gt;
&lt;p&gt;검사를 수행한 제품의 수가 $n=10$이고, 제품이 불량일 확률은 $p=0.1$이다. 10개 중 2개가 불량일 확률은 얼마인가?&lt;/p&gt;

&lt;p&gt;RV X : 불량인 제품의 수&lt;br /&gt;
확률을 구하고 싶으면 확률 함수가 필요&lt;/p&gt;

&lt;p&gt;$p(x)={n \choose x}p^x(1-p)^{n-x}$&lt;/p&gt;

&lt;p&gt;$P[X=2]={10 \choose 2}(0.1)^{2}(0.9)^{8}=0.1937$&lt;/p&gt;

&lt;p&gt;$E[X]=np=(10)(0.1)=1$&lt;br /&gt;
$V[X]=np(1-p)=(10)(0.1)(0.9)=0.9$&lt;/p&gt;

&lt;h2 id=&quot;binomial-distribution---defect-example-2&quot;&gt;Binomial Distribution - Defect Example 2&lt;/h2&gt;
&lt;p&gt;$n=50$, $p=0.06$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;What is Random Variable?
RV X : 불량인 제품의 수&lt;/li&gt;
  &lt;li&gt;What is the probability of 3 defective units in the sample?
$P(X=3)={50 \choose 3}0.06^{3}(1-0.06)^{47}$&lt;/li&gt;
  &lt;li&gt;What is the probability of more than 3 defective units in the sample?&lt;/li&gt;
  &lt;li&gt;$P(X&amp;gt;3)=1-P(X=0)-P(X=1)-P(X=2)-P(X=3)$&lt;br /&gt;
$=1-\sum_{i=0}^{3}{n \choose i}p^i(1-p)^{50-i}$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;binomial-distribution---proposition&quot;&gt;Binomial Distribution - Proposition&lt;/h2&gt;
&lt;p&gt;이항 분포에서 확률은 증가했다가 감소하는 패턴을 보인다. 이 때, $k$가 $k\le(n+1)p$이며, 정수일 때 가장 높은 확률을 보인다.&lt;/p&gt;

&lt;h3 id=&quot;ex&quot;&gt;ex&lt;/h3&gt;
&lt;p&gt;$n=10, p=1/2$인 이항분포가 있을 때, 언제 확률이 가장 높은가?&lt;/p&gt;

&lt;p&gt;$k\le (n+1)p$&lt;br /&gt;
$=(10+1)0.5$&lt;br /&gt;
$=11\times 0.5$&lt;br /&gt;
$=5.5$&lt;/p&gt;

&lt;p&gt;$k=5$일 때 가장 큰 확률을 가진다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Random Variables</title><link href="http://localhost:4000/posts/2020/05/17/random_variables" rel="alternate" type="text/html" title="Random Variables" /><published>2020-05-17T00:00:00-07:00</published><updated>2020-05-17T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/17/random_variables</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/17/random_variables">&lt;iframe src=&quot;https://www.youtube.com/embed/GqDy0sInGJ0&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;random-variables&quot;&gt;Random Variables&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;표본 공간의 원소들(실험의 결과들)을 입력으로 받아 실수로 바꿔주는 함수&lt;/li&gt;
  &lt;li&gt;$Real \;Number=f(Elements \;of \;the \;sample \;space)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;random-variables---coin-example&quot;&gt;Random Variables - Coin Example&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;tossing 3 coins&lt;/li&gt;
  &lt;li&gt;Sample Space = {(HHH),(HHT),(HTH),(THH),(HTT),(THT),(TTH),(TTT)}&lt;/li&gt;
  &lt;li&gt;Random Variable Y : Number of heads&lt;/li&gt;
  &lt;li&gt;Y={0,1,2,3}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82135062-78796180-9839-11ea-8b4b-12e0951ffe58.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 일반적으로 사용하는 데이터는 다음과 같은 형태를 가지고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82135114-d4dc8100-9839-11ea-9fda-00abf4f23c29.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 확률 변수의 개념을 적용하면 “샘플들이 $X_1$이라는 확률 변수를 만나면 $x_{11},x_{21},…x_{n1}$이라는 실수로 바뀌고, $X_p$이라는 확률 변수를 만나면 $x_{1p},x_{2p},…x_{np}$이라는 실수로 바뀐다.”고 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;discrete--continous-random-variables&quot;&gt;Discrete / Continous Random Variables&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Random Variable의 결과는 실수인데, 실수는 두 가지 종류가 있다.&lt;/li&gt;
  &lt;li&gt;Discrete(이산형) : 셀 수 있다(동전을 세 개 던졌을 때 앞면이 나온 경우의 수, 코로나 바이러스 확진자 수)&lt;/li&gt;
  &lt;li&gt;Continous(연속형) : 셀 수 없다(시간과 돈이 대표적인 연속형 데이터. 서울 주민들의 평균 연봉)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-function&quot;&gt;Probability Function&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;확률 변수로부터 나온 실수를 입력으로 받아 확률로 바꿔주는 함수&lt;/li&gt;
  &lt;li&gt;$p=f(Real \; Number)$&lt;/li&gt;
  &lt;li&gt;Random Variables의 결과가 Discrete, Continous 두 가지가 있다고 했다. 이것인 즉슨 Probability Function의 입력도 Discrete, Continous 두 가지가 있다는 것이다. 이 때문에 Probability Function은 입력의 종류에 따라 두 가지로 나뉘게 된다.&lt;/li&gt;
  &lt;li&gt;입력이 Discrete한 경우 : Probability Mass Function(PMF, 확률질량함수)&lt;/li&gt;
  &lt;li&gt;입력이 Continous한 경우 : Probability Density Function(PDF, 확률밀도함수)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-mass-function&quot;&gt;Probability Mass Function&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Discrete Random Variable $X$을 입력으로 받음&lt;/li&gt;
  &lt;li&gt;$x$를 $X$의 원소라고 하자.&lt;/li&gt;
  &lt;li&gt;이산형 실수인 $x$에 대해 확률이 부여됨&lt;/li&gt;
  &lt;li&gt;$p(x)=P[X=x]$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82135065-7adbbb80-9839-11ea-9f09-7649b020585a.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;X축에 있는 것은 $X$라는 확률 변수가 가질 수 있는 이산형 값들이 되며, 거기에 해당하는 확률이며, 이를 막대 그래프로 표현한 것이다.&lt;/li&gt;
  &lt;li&gt;$0\le p(x) \le 1$   for all $x$&lt;/li&gt;
  &lt;li&gt;$\sum_xp(x)=1$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-mass-function---coin-example&quot;&gt;Probability Mass Function - Coin Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;tossing 2 coins&lt;/li&gt;
  &lt;li&gt;Sample Space S={(HH), (HT), (TH), (TT)}&lt;/li&gt;
  &lt;li&gt;Random Variable X = Number of heads&lt;/li&gt;
  &lt;li&gt;Possible values of X = {0, 1, 2}&lt;/li&gt;
  &lt;li&gt;What is probability mass function?&lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;$P(X=0)=1/4$&lt;/li&gt;
      &lt;li&gt;$P(X=1)=1/2$&lt;/li&gt;
      &lt;li&gt;$P(X=2)=1/4$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-mass-function---accident-example&quot;&gt;Probability Mass Function - Accident Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;한 공장에서는 매년 낙상 사고가 일어난다.&lt;/li&gt;
  &lt;li&gt;Random Variable X : 일 년 동안 일어나는 낙상 사고의 수&lt;/li&gt;
  &lt;li&gt;$X=0,1,2,3,…$&lt;/li&gt;
  &lt;li&gt;$P(X=x)={1\over 2^{x+1}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 때, $P(X=x)$는 Probability Mass Function이라고 할 수 있는가?&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;확률 변수 X는 셀 수 있으므로 이산형이 맞다.&lt;/li&gt;
  &lt;li&gt;$0\le p(x)\le 1$&lt;br /&gt;
$P(X=0)=1/2$&lt;br /&gt;
$P(X=1)=1/2^2$&lt;br /&gt;
$P(X=2)=1/2^3$&lt;br /&gt;
…이므로 모든 확률은 0과 1 사이에 있다.&lt;/li&gt;
  &lt;li&gt;$\sum_{x}P(X=x)=1$
$1/2+1/2^2+1/2^3+…={1/2 \over 1-{1/2}}=1$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;모든 조건을 만족하므로 Probability Mass Function이라고 할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;probability-mass-function---coin-example-2&quot;&gt;Probability Mass Function - Coin Example 2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;동전을 던져 앞면이 나오거나 던진 횟수가 $n$이 되면 동전 던지기를 멈춘다고 하자.&lt;/li&gt;
  &lt;li&gt;$P(head)=p$&lt;/li&gt;
  &lt;li&gt;Random Variable $X$ : 동전을 던진 횟수&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$X$는 Random Variable이 될 수 있는가?&lt;br /&gt;
X={1,2,3,…n}&lt;br /&gt;
x=1 =&amp;gt; 동전 던지기 한 번에 앞 면이 나옴&lt;br /&gt;
x=2 =&amp;gt; 동전 던지기 첫 번째에는 뒷 면이 나오고, 두 번째에는 앞 면이 나옴.&lt;br /&gt;
x=3 =&amp;gt; 동전 던지기 두 번째까지 뒷 면이 나오고, 세 번째에는 앞 면이 나옴.&lt;br /&gt;
x=n =&amp;gt; 동전 던지기 (n-1)까지 뒷 면이 나오고, n 번째에는 앞 면이든 상관없다.(어차피 멈춰야 함)&lt;br /&gt;
원소들을 실수로 바꿨으므로 확률변수라고 할 수 있다.&lt;/li&gt;
  &lt;li&gt;X의 원소를 확률에 대응시켜라&lt;br /&gt;
$P(X=1)=p(H)=p$&lt;br /&gt;
$P(X=2)=p(TH)=(1-p)p$&lt;br /&gt;
$P(X=3)=p(TTH)=(1-p)^2p$&lt;br /&gt;
$P(X=n-1)=(1-p)^{n-2}p$&lt;br /&gt;
$P(X=n)=(1-p)^{n-1}p+(1-p)^{n-1}(1-p)$   이 부분 주의해야 함&lt;/li&gt;
  &lt;li&gt;$\sum_{i=1}^{n}P(X=i)=1$을 만족하는지 확인하라&lt;br /&gt;
$\sum_{i=1}^{n-1}P(X=i)+P(X=n)$&lt;br /&gt;
$=\sum_{i=1}^{n-1}(1-p)^{i-1}p+(1-p)^{n-1}$&lt;br /&gt;
$=p{1-(1-p)^{n-1}\over 1-(1-p)}+(1-p)^{n-1}$&lt;br /&gt;
$=1$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;probability-mass-function-with-infinitely-many-possible-values&quot;&gt;Probability Mass Function with Infinitely Many Possible Values&lt;/h2&gt;
&lt;p&gt;PMF : $p(X=i)=c\lambda^i/i!, \;\; i=0,1,2,…, \;\; \lambda는 양수$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$c$를 구하라.&lt;br /&gt;
$\sum_{i=1}^{\infty}P(X=i)=c\sum_{i=1}^{\infty}{\lambda^i\over i!}=c(1+\lambda+{\lambda^2\over 2!}+{\lambda^3\over 3!}+…)=ce^\lambda=1$&lt;br /&gt;
$c=e^{-\lambda}$&lt;/li&gt;
  &lt;li&gt;$P(X=0)$&lt;br /&gt;
$P(X=0)=e^{-\lambda}\lambda^0/0!=e^{-\lambda}$&lt;/li&gt;
  &lt;li&gt;$P(X&amp;gt;2)$&lt;br /&gt;
$=1-P(X\le 2)$&lt;br /&gt;
$=1-(P(X=0)+P(X=1)+P(X=2))$&lt;br /&gt;
$=e^{-\lambda}+e^{-\lambda}\lambda^1/1!+e^{-\lambda}\lambda^2/2!$&lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">How to deal with Absence of Negative Feedback</title><link href="http://localhost:4000/posts/2020/05/17/absence_of_negative_feedback" rel="alternate" type="text/html" title="How to deal with Absence of Negative Feedback" /><published>2020-05-17T00:00:00-07:00</published><updated>2020-05-17T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/17/absence_of_negative_feedback</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/17/absence_of_negative_feedback">&lt;p&gt;Implicit Feedback을 활용한 추천 시스템은 아마 unobserved data를 어떻게든 해결해야 할 것이다. 이 문제는 Explicit Feedback과 비교하면 명백하게 알 수 있다.&lt;/p&gt;

&lt;p&gt;점수가 1~5점 사이에 분포해 있는 Explicit Feedback의 경우 1점은 “유저가 아이템을 좋아하지 않는다”는 것을 나타내며, 5점은 “유저가 아이템을 좋아한다”는 것을 나타낸다.&lt;/p&gt;

&lt;p&gt;하지만 Implicit Feedback의 경우 0점을 기록(ex. 유저가 해당 아이템을 구매한 적이 없음)하더라도 “유저가 해당 아이템을 좋아하지 않는다”고 말할 수 없다. 유저가 실제로 아이템을 좋아하지 않는 경우 이외에도 유저가 아이템을 인지하지 못했던 경우가 있기 때문이다. 이러한 문제를 부정적인 피드백의 부재(absence of negative feedback)라고 한다.&lt;/p&gt;

&lt;p&gt;만약 부정적인 피드백를 모두 포함시켜 학습을 시킨다면 부정적인 피드백의 수가 긍정적인 피드백보다 훨씬 많아 긍정적인 피드백 무시하는 방향으로 학습이 진행될 것이다. 반대로 부정적인 피드백를 모두 제외시킨다면 긍정적인 피드백만 학습이 진행되어 우리의 최종 목표인 binary classification을 수행하지 못할 것이다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위한 두 가지 방법을 제안한다. 하나는 Neural Collaborative Filtering에서 사용한 방법이고, 다른 하나는 Collaborative Filtering for Implicit Feedback Datasets에서 사용한 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;1-negative-sampling&quot;&gt;1. Negative Sampling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05031.pdf&quot;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;유저가 긍정적인 피드백을 준 것 이외에 유저가 아직 피드백을 주지 않은 k개의 아이템을 포함시켜 Training Set을 구성한다.(k는 하이퍼 파라미터이며, 이 논문에서는 $k=4$)&lt;/li&gt;
  &lt;li&gt;긍정적인 피드백이 $n$개라면 Training Set은 $n(k+1)$개가 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback의 일부만 샘플링해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-confidence-level&quot;&gt;2. Confidence Level&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://yifanhu.net/PUB/cf.pdf&quot;&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Confidence Level이라는 개념을 도입한다. 유저가 아이템과 연관성이 있는지 정도를 수치적으로 표현하기 위한 것이다.&lt;/li&gt;
  &lt;li&gt;$c_{ui}=1+\alpha r_{ui}$로 계산할 수 있다. $c_ui$가 클수록 유저와 아이템은 연관성이 큰 것이다.&lt;/li&gt;
  &lt;li&gt;$r_{ui}$는 긍정적인 피드백의 수이며, $r_{ui}$가 증가할수록 $c_ui$도 커진다. (증가하는 정도를 나타낸 것이 $\alpha$다. $\alpha$는 하이퍼 파라미터이며, 이 논문에서는 $\alpha=40$)&lt;/li&gt;
  &lt;li&gt;$c_{ui}$는 모두 1 이상의 값을 갖게 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback도 모두 사용해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">Implicit Feedback을 활용한 추천 시스템은 아마 unobserved data를 어떻게든 해결해야 할 것이다. 이 문제는 Explicit Feedback과 비교하면 명백하게 알 수 있다.</summary></entry><entry><title type="html">기대값, 분산(Expectation, Variance)</title><link href="http://localhost:4000/posts/2020/05/19/expectation_variance" rel="alternate" type="text/html" title="기대값, 분산(Expectation, Variance)" /><published>2020-05-17T00:00:00-07:00</published><updated>2020-05-17T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/19/expectation_variance</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/19/expectation_variance">&lt;iframe src=&quot;https://www.youtube.com/embed/1YDBHTM-e_I&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;center---expectation-of-discrete-random-variable&quot;&gt;Center - Expectation of Discrete Random Variable&lt;/h2&gt;
&lt;p&gt;다음 수들의 평균을 구해보자.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;1, 2, 3, 4, 5&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;정답은 3이다. 평균을 구하는 공식에 의해 ${1+2+3+4+5\over 5}=3$이 된다. 하지만 여기에는 각각의 숫자가 나올 확률이 동일하다는 가정이 숨어있다.&lt;/p&gt;

&lt;p&gt;확률 분포의 기대값(=평균)을 구하기 위해서는 각각의 값에 그 값이 나올 확률을 곱해 더하면 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[X]=\sum_{i}x_if_X(x_i)&lt;/script&gt;

&lt;p&gt;즉, 산술 평균은 모든 값이 나올 확률이 동일하다는 가정을 가진 기대값의 특수한 케이스라고 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;예제&quot;&gt;예제&lt;/h3&gt;
&lt;p&gt;게임에서 이기면 100을 벌고, 지면 100,000을 잃는다. 게임에서 이길 확률을 0.99라고 할 때, $X$를 게임에서 벌거나 잃는 돈이라고 하자. $E[X]$는 얼마인가?&lt;/p&gt;

&lt;p&gt;$E[X]=100\times 0.99+100000\times 0.01=-901$&lt;/p&gt;

&lt;h2 id=&quot;expectation-of-discrete-random-variable---coin-example-1&quot;&gt;Expectation of Discrete Random Variable - Coin Example 1&lt;/h2&gt;
&lt;p&gt;RV X : 동전을 두 개 던져 나오는 앞면의 수
앞 면이 나올 확률은 1/2&lt;/p&gt;

&lt;p&gt;$p(0)={1\over 4}, p(1)={1\over 2}, p(2)={1\over 4}$&lt;/p&gt;

&lt;p&gt;$E[X]=(0)({1\over 4})+(1)({1\over 2})+(2)({1\over 4})=1$&lt;/p&gt;

&lt;h2 id=&quot;expectation-of-discrete-random-variable---coin-example-2&quot;&gt;Expectation of Discrete Random Variable - Coin Example 2&lt;/h2&gt;
&lt;p&gt;RV X : 동전을 두 개 던져 나오는 앞면의 수&lt;br /&gt;
앞 면이 나올 확률은 $p$&lt;/p&gt;

&lt;p&gt;$P(X=0)=P(t,t)=(1-p)^2$&lt;br /&gt;
$P(1)=P(h,t)+P(t,h)=2p(1-p)$&lt;br /&gt;
$P(2)=P(h,h)=p^2$&lt;/p&gt;

&lt;p&gt;$E[X]=(0)((1-p)^2)+(1)(2p(1-p))+(2)(p^2)=1$&lt;/p&gt;

&lt;h2 id=&quot;expectation-of-a-function-of-a-random-variable&quot;&gt;Expectation of a Function of a Random Variable&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;지금까지 배웠던 것은 확률변수의 기대값&lt;/li&gt;
  &lt;li&gt;이번에 배울 것은 확률변수의 함수의 기대값&lt;/li&gt;
  &lt;li&gt;별 다를건 없음&lt;/li&gt;
  &lt;li&gt;function of X를 g(X)라고 하자.&lt;/li&gt;
  &lt;li&gt;$g(x_i)$와 그것이 발생할 확률을 곱해 모두 더하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[g(X)]=\sum_ig(x_i)p(x_i)&lt;/script&gt;

&lt;h2 id=&quot;expectation-of-a-function-of-a-random-variable---example&quot;&gt;Expectation of a Function of a Random Variable - Example&lt;/h2&gt;
&lt;p&gt;RV X : 동전을 두 개 던져 나오는 앞면의 수
앞 면이 나올 확률은 1/2&lt;/p&gt;

&lt;p&gt;$p(0)={1\over 4}, p(1)={1\over 2}, p(2)={1\over 4}$&lt;/p&gt;

&lt;p&gt;$g(x)={3\over x+1}$&lt;/p&gt;

&lt;p&gt;$E[g(x)]=E[{3\over x+1}]=({3\over 0+1})({1\over 4})+({3\over 1+1})({1\over 2})+({3\over 2+1})({1\over 4})={7\over 4}$&lt;/p&gt;

&lt;h2 id=&quot;expectation-of-a-function-of-a-random-variable-1&quot;&gt;Expectation of a Function of a Random Variable&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$c, d$는 상수&lt;/li&gt;
  &lt;li&gt;$E[c]=\sum cp(x)=c\sum p(x)=c\cdot 1=c$&lt;/li&gt;
  &lt;li&gt;$E[cX]=\sum cxp(x)=c\sum xp(x)=c\cdot E[X]$&lt;/li&gt;
  &lt;li&gt;$E[cX+d]=\sum(cx+d)p(x)=c\sum xp(x)+d\sum p(x)=cE[X]+d$&lt;/li&gt;
  &lt;li&gt;그렇다면 $E[X^n]$은 무엇일까?&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[X^n]=\sum_xx^np(x)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$E[X^n]$를 확률 변수 X의 “n차 적률(nth moment)”이라고 한다.&lt;/li&gt;
  &lt;li&gt;$E[X]$는 first moment이고, $E[X^2]$는 second moment이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;expected-value-of-sums-of-random-variables&quot;&gt;Expected Value of Sums of Random Variables&lt;/h2&gt;
&lt;p&gt;확률 변수 $X_1, X_2, …, X_n$은 다음 수식을 만족한다.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;E[\sum_{i=1}^{n}X_i]=\sum_{i=1}^{n}E[X_i]&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;E[X_1+X_2+...+X_n]=E[X_1]+E[X_2]+...+E[X_n]&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;expected-value-of-sums-of-random-variables---example&quot;&gt;Expected Value of Sums of Random Variables - Example&lt;/h2&gt;
&lt;p&gt;RV X : 주사위를 $N$번 던졌을 때 값들의 합&lt;br /&gt;
$E[X_i]=\sum_{i=1}^{6}i\cdot({1\over 6})=3.5$&lt;br /&gt;
$E[X]=E[\sum_{i=1}^{N}X_i]=\sum_{i=1}^{N}E[X_i]=3.5n$&lt;/p&gt;

&lt;h2 id=&quot;variability---variance&quot;&gt;Variability - Variance&lt;/h2&gt;
&lt;p&gt;RV $X$가 있고, $X$의 기대값이 $E[X]$일 때, $X$의 분산은 다음과 같이 정의한다.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;V(X)=E[(X-E[X])^2]&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;평균을 기점으로 얼마만큼 떨어져있는지를 제곱의 스케일로 표현한 측도&lt;/li&gt;
  &lt;li&gt;음수가 될 수 없다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$V(X)=E[(X-E[X])^2]$&lt;br /&gt;
$=E(X^2)-{E[X]}^2$ (이 형태가 많이 쓰임)&lt;/p&gt;

&lt;h2 id=&quot;variance---example&quot;&gt;Variance - Example&lt;/h2&gt;
&lt;p&gt;RV X : 동전을 두 개 던져 나오는 앞면의 수
앞 면이 나올 확률은 1/2&lt;/p&gt;

&lt;p&gt;$p(0)={1\over 4}, p(1)={1\over 2}, p(2)={1\over 4}$&lt;/p&gt;

&lt;p&gt;$E[X^2]=\sum x^2p(x)=3/2$&lt;br /&gt;
$V(X)=E(X^2)-{E[X]}^2=3/2-1^2=1/2$&lt;/p&gt;

&lt;h3 id=&quot;variance-of-a-function-of-a-random-variable&quot;&gt;Variance of a Function of a Random Variable&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;V(g(X))=E[(g(X)-\mu_{g(X)})^2]&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$c, d$는 상수&lt;/li&gt;
  &lt;li&gt;$V(c)=E[(c-E[c])^2]=E[(c-c)^2]=0$&lt;/li&gt;
  &lt;li&gt;$V(cX)=E[(cX-cE[X])^2]=c^2E[(X-E[X])^2]=c^2V(X)$&lt;/li&gt;
  &lt;li&gt;$V(cX+d)=E[(cX+d-E[cX+d])^2]=E[(cX+d-cE[X]-d)^2]=c^2E[(X-E[X])^2]=c^2V(X)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;standard-deviation&quot;&gt;Standard Deviation&lt;/h2&gt;
&lt;p&gt;분산의 제곱근&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;SD[X]=\sqrt{V[X]}&lt;/script&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Independence</title><link href="http://localhost:4000/posts/2020/05/16/independence" rel="alternate" type="text/html" title="Independence" /><published>2020-05-16T00:00:00-07:00</published><updated>2020-05-16T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/16/independence</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/16/independence">&lt;iframe src=&quot;https://www.youtube.com/embed/dHTkIna_hFk&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;independence&quot;&gt;Independence&lt;/h2&gt;
&lt;p&gt;$A$, $B$ 사건이 다음과 같은 조건을 만족하면 독립이라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B|A)=P(B)&lt;/script&gt;

&lt;p&gt;이 수식을 해석하면, “사건 $B$가 발생할 확률은 $A$의 발생 여부에 영향을 받지 않는다”이다.&lt;/p&gt;

&lt;p&gt;식을 다시 써보면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(AB)=P(A)P(B|A)=P(A)P(B)&lt;/script&gt;

&lt;p&gt;즉, 조건부 확률에 해당하는 부분을 없앨 수 있다.&lt;/p&gt;

&lt;p&gt;이를 응용하면 &lt;a href=&quot;https://blanik00.github.io/posts/2020/05/14/conditional_probability&quot;&gt;Conditional Probability&lt;/a&gt;에서 봤던 Multiplicative Rules in Probability을 단순화시킬 수 있다.&lt;/p&gt;

&lt;p&gt;$P(A_1A_2…A_n)$&lt;br /&gt;
$=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)…P(A_n|A_1A_2…A_{n-1})$&lt;br /&gt;
$=P(A_1)P(A_2)…P(A_n)$&lt;/p&gt;

&lt;h2 id=&quot;independence-events---example-1&quot;&gt;Independence Events - Example 1&lt;/h2&gt;
&lt;p&gt;A마을에는 소방차 한 대와 구급차 한 대가 있다. 소방차가 필요할 때 소방차를 사용할 수 있는 확률은 0.98이고, 구급차가 필요할 때 구급차를 사용할 수 있는 확률은 0.92이다. 이 둘을 동시에 호출했을 때 둘 다 사용할 수 있는 확률은 얼마인가?(두 사건은 독립이다)&lt;/p&gt;

&lt;p&gt;A: 소방차&lt;br /&gt;
B: 앰뷸런스&lt;/p&gt;

&lt;p&gt;$P(AB)=P(A)P(B)=0.9016$&lt;/p&gt;

&lt;h2 id=&quot;independence-events---example-2&quot;&gt;Independence Events - Example 2&lt;/h2&gt;
&lt;p&gt;두 개의 주사위를 던진다.&lt;br /&gt;
$E_1= \{ sum\; is\; 6 \} = \{ (1,5),(2,4),(3,3),(4,2),(5,1) \} $&lt;br /&gt;
$E_2= \{ sum\; is\; 7 \} = \{ (1,6),(2,5),(3,4),(4,3),(5,2),(6,1) \} $&lt;br /&gt;
$E_3= \{ first\; dice\; is\;4 \} = \{ (4,1),(4,2),(4,3),(4,4),(4,5),(4,6) \} $&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$E_1$과 $E_2$는 독립인가?&lt;br /&gt;
$E_1$과 $E_2$는 mutually exclusive하기 때문에 $P(E_1E_2)=0$이다.&lt;br /&gt;
$P(E_1E_2)\neq P(E_1)P(E_2)$&lt;br /&gt;
mutually exclusive와 independence가 다르다는 점을 주의해야 한다.&lt;/li&gt;
  &lt;li&gt;$E_1$과 $E_3$는 독립인가? dependent&lt;/li&gt;
  &lt;li&gt;$E_2$과 $E_3$는 독립인가? independent&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;또한 사건 $E$, $F$가 독립이라면, $E$, $F^C$ 또한 독립이다.&lt;/p&gt;

&lt;p&gt;$P(E)=P(EF)+P(EF^C)$&lt;br /&gt;
$=P(E)P(F)+P(EF^C)$&lt;/p&gt;

&lt;p&gt;$P(EF^C)=P(E)(1-P(F))$&lt;br /&gt;
$=P(E)P(F^C)$&lt;/p&gt;

&lt;p&gt;즉, $E$, $F$가 독립이라면, $E$가 발생할 확률은 $F$의 발생 여부에 영향을 받지 않는다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;세 개의 사건 $E$, $F$, $G$가 독립이기 위해서는 다음 조건을 만족해야 한다.&lt;/p&gt;

&lt;p&gt;$P(EFG)=P(E)P(F)P(G)$&lt;br /&gt;
$P(EF)=P(E)P(F)$&lt;br /&gt;
$P(EG)=P(E)P(G)$&lt;br /&gt;
$P(FG)=P(F)P(G)$&lt;/p&gt;

&lt;p&gt;세 사건이 독립이라면, $E$는 $F$와 $G$를 어떻게 조합해도 그 조합한 것과 독립이다. 예를 들어, $E$는 $F\cup G$과 독립이다.&lt;/p&gt;

&lt;p&gt;$P[E(F\cup G)]=P(EF\cup EG)$&lt;br /&gt;
$=P(EF)+P(EG)-P(EFG)$&lt;br /&gt;
$=P(E)P(F)+P(E)P(G)-P(E)P(FG)$&lt;br /&gt;
$=P(E)[P(F)+P(G)-P(FG)]$&lt;br /&gt;
$=P(E)P(F\cup G)$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Conditional Probability</title><link href="http://localhost:4000/posts/2020/05/14/conditional_probability" rel="alternate" type="text/html" title="Conditional Probability" /><published>2020-05-15T00:00:00-07:00</published><updated>2020-05-15T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/14/conditional_probability</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/14/conditional_probability">&lt;iframe src=&quot;https://www.youtube.com/embed/Cj25K_leYZw&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;conditional-probability&quot;&gt;Conditional Probability&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$F$라는 이벤트가 일어났다는 조건 하에 이벤트 $E$가 일어날 확률&lt;/li&gt;
  &lt;li&gt;$P(E|F)={P(EF)\over P(F)}, \quad P(F) &amp;gt; 0$&lt;/li&gt;
  &lt;li&gt;“Probability of E given F” 혹은 “Probability of E conditional on F”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;if $E$ and $F$ are mutually exclusive&lt;br /&gt;
$P(E|F)={P(EF)\over P(F)}={0\over P(F)}=0$&lt;/p&gt;

&lt;p&gt;if $F\subset E$&lt;br /&gt;
$P(E|F)={P(EF)\over P(F)}={P(F)\over P(F)}=1$&lt;/p&gt;

&lt;h2 id=&quot;conditional-probability---example-1&quot;&gt;Conditional Probability - Example 1&lt;/h2&gt;
&lt;p&gt;한 마을에 있는 인구를 성별과 고용 상태에 따라 나눈 것이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Employed&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Unemployed&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Totals&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;460&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;F&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;140&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;260&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Totals&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;900&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$P(M|U)={P(MU)\over P(U)}={40 \over 300}$&lt;/p&gt;

&lt;h2 id=&quot;conditional-probability---example-2&quot;&gt;Conditional Probability - Example 2&lt;/h2&gt;
&lt;p&gt;$P(E|F)={P(EF)\over P(F)}$&lt;br /&gt;
$P(EF)=P(F)P(E|F)$&lt;br /&gt;
$P(E|FG)={P(EFG)\over P(FG)}$&lt;/p&gt;

&lt;p&gt;Celine는 French 혹은 Chemistry 수업 중 어떤 것을 들을지 고민하고 있다. 그녀가 French를 수강하고 A학점을 받을 확률은 $1/2$이며, Chemistry를 수강하고 A학점을 받을 확률은 $2/3$이다. 만약 그녀가 동전던지기를 통해 어떤 수업을 들을지 결정한다면, 그녀가 Chemistry 수업을 듣고, A학점을 받게될 확률은 무엇인가?&lt;/p&gt;

&lt;p&gt;event C : Chemistry 수업을 수강하는 사건&lt;br /&gt;
event A : A학점을 받는 사건&lt;/p&gt;

&lt;p&gt;구하고자 하는 확률은 $P(CA)$이다.&lt;/p&gt;

&lt;p&gt;$P(CA)=P(C)P(A|C)=({1\over 2})({2\over 3})={1\over3}$&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-rules-in-probability&quot;&gt;Multiplicative Rules in Probability&lt;/h2&gt;
&lt;p&gt;조건부 확률의 정의로부터 다음을 도출할 수 있다.&lt;br /&gt;
$P(AB)=P(A)P(B|A)=P(B)P(A|B)$&lt;br /&gt;
$P(ABC)=P(AB)P(C|AB)=P(A)P(B|A)P(C|AB)$&lt;br /&gt;
$P(ABCD)=P(ABC)P(D|ABC)=P(A)P(B|A)P(C|AB)P(D|ABC)$&lt;/p&gt;

&lt;p&gt;이것을 일반화하면 사건 $A_1$, $A_2$, … $A_k$에 대해 &lt;script type=&quot;math/tex&quot;&gt;P(A_1A_2...A_k)=P(A_1)P(A_2\|A_1)P(A_3\|A_1A_2)...P(A_k\|A_1A_2...A_{k-1})&lt;/script&gt;을 만족한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고&lt;/strong&gt; 각 사건이 독립이라는 조건이 있다면 위 연산이 간단해진다.&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule&quot;&gt;Bayes’ Rule&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82064341-9777d680-9707-11ea-9727-4353740824c2.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$A_1$, $A_2$, $A_3$은 표본 공간 S의 Partition이다(Mutually Exclusive &amp;amp; Collectively Exhaustive).&lt;/li&gt;
  &lt;li&gt;$B$는 event이다.&lt;br /&gt;
$P(B)=P(A_1B)+P(A_2B)+P(A_3B)$&lt;br /&gt;
$\qquad=P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)$&lt;br /&gt;
$\qquad=\sum_{i=1}^{3}P(A_i)P(B|A_i)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$P(B)$ 즉, 우리가 관심있는 이벤트에 대한 확률은 &lt;strong&gt;조건부 확률의 합&lt;/strong&gt;으로 표현할 수 있다.(Law of Total Probability) 이것이 Bayes’ Rule의 시작점이다.&lt;/p&gt;

&lt;p&gt;이제 본격적으로 Bayes’ Rule에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;다음과 같은 확률들을 알고있다고 가정하자.&lt;br /&gt;
$P(A_1)$, $P(A_2)$, $P(A_3)$&lt;br /&gt;
$P(B|A_1)$, $P(B|A_2)$, $P(B|A_3)$&lt;/p&gt;

&lt;p&gt;이 정보들을 가지고 다음과 같은 확률을 알아내야 한다고 하자.&lt;/p&gt;

&lt;p&gt;$P(A_1|B)$, $P(A_2|B)$, $P(A_3|B)$&lt;/p&gt;

&lt;p&gt;이 작업을 해내는 것이 Bayes’ Rule이다.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A_1|B)={P(A_1)P(B|A_1)\over P(B)}={P(A_1)P(B|A_1)\over P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A_2|B)={P(A_2)P(B|A_2)\over P(B)}={P(A_2)P(B|A_2)\over P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A_3|B)={P(A_3)P(B|A_3)\over P(B)}={P(A_3)P(B|A_3)\over P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;용어-정리&quot;&gt;용어 정리&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82064328-947ce600-9707-11ea-9271-482fb3a038ac.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;정리&quot;&gt;정리&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Prior Probability와 Data Probability를 기반으로 Posterior Probability를 구하는 법칙&lt;/li&gt;
  &lt;li&gt;$P(A_1|B)$과 $P(A_1)$은 모두 $A_1$에 대한 확률이다. 다만 앞의 것은 $B$가 주어졌을 때의 확률이다. 무엇인가 주어졌다는 것은 정보가 주어졌다는 것이다. 즉, $B$는 정보다. $P(A_1|B)$는 $P(A_1)$이 정보 $B$에 의해 업데이트된 것이다. 그래서 $P(A_1)$은 정보를 받기 전이라서 Prior이고, $P(A_1|B)$은 정보를 받은 후라서 Posterior이다. 정리하자면 Posterior Probability는 Prior Probability의 업데이트된 버전이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bayes-rule---supplier-example&quot;&gt;Bayes’ Rule - Supplier Example&lt;/h2&gt;
&lt;p&gt;완성차 제조업체 A는 두 타이어 업체로부터 타이어를 공급받는다. 타이어 업체는 1, 2 두 가지가 있으며, 업체 1에서 공급받은 타이어의 10%가 불량품이며, 업체 2에서 공급받은 타이어의 10%가 불량품이다. 현재 A에서 보유하고 있는 타이어 재고의 40%가 1에서 공급받은 것이다. 이 때 불량 타이어가 발견되었을 때, 그것이 업체 1에서 공급받은 타이어일 확률을 구하라.&lt;/p&gt;

&lt;p&gt;$P(S_1 | D)$&lt;/p&gt;

&lt;p&gt;우선 Prior Probability와 Data Probability를 알아야 한다.
$P(S_1)=0.4, P(S_2)=0.6$
$P(D | S_1)=0.1, P(D| S_2)=0.05$&lt;/p&gt;

&lt;p&gt;$P(S_1 | D)={P(S_1 \cap D)\over P(D)}$&lt;br /&gt;
$\qquad \qquad={P(S_1 \cap D)\over P(S_1 \cap D) + P(S_2 \cap D)}$&lt;br /&gt;
$\qquad \qquad={P(S_1)P(D|S_1)\over P(S_1)P(D|S_1) + P(S_2)P(D|S_2)}$&lt;br /&gt;
$\qquad \qquad=0.57$&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule---criminal-investigation&quot;&gt;Bayes’ Rule - Criminal Investigation&lt;/h2&gt;
&lt;p&gt;경찰관이 수사를 통해 60%의 확신을 가지고 용의자를 추려냈는데, 이 상황에서 새로운 증거가 발견되었다. 그것은 범인이 대머리라는 사실이다. 만약 전체 인구의 20%가 대머리일 때, 대머리인 사람이 범인일 확률을 구하라.&lt;/p&gt;

&lt;p&gt;마찬가지로 Prior Probability와 Data Probability를 알아야 하는데, 조금 헷갈리는 부분이 있어 잘 생각해봐야 한다.&lt;/p&gt;

&lt;p&gt;$P(S)$ : 유죄일 확률&lt;br /&gt;
$P(\sim S)$ : 무죄일 확률&lt;br /&gt;
$P(B| S)$ : 유죄이면서 대머리일 확률&lt;br /&gt;
$P(B| \sim S)$ : 무죄이면서 대머리일 확률&lt;/p&gt;

&lt;p&gt;$P(S)=0.6, P(\sim S)=0.4$&lt;br /&gt;
$P(B| S)=1, P(B| \sim S)=0.2$&lt;/p&gt;

&lt;p&gt;처음에는 전체 인구의 20%가 대머리라고 해서 $P(B| S)$과 $P(B| \sim S)$의 확률이 모두 0.2이라고 생각했다. 하지만 문제에서 범인이 대머리라는 사실이 밝혀졌다고 명시했으므로 $P(B| S)=1$이다. 즉, 범인은 반드시 대머리다.&lt;/p&gt;

&lt;p&gt;$P(S | B)={P(S \cap B)\over P(B)}$&lt;br /&gt;
$\qquad \qquad={P(B|S)P(S)\over P(B|S)P(S) + P(B|\sim S)P(\sim S)}$&lt;br /&gt;
$\qquad \qquad=0.882$&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule---disease-example&quot;&gt;Bayes’ Rule - Disease Example&lt;/h2&gt;
&lt;p&gt;전체 인구의 1%가 희귀병에 앓고 있다. A사가 개발한 혈액 테스트는 97%의 확률로 이 희귀병을 진단할 수 있다(희귀병에 걸린 사람에게 희귀병에 걸렸다고 진단). 하지만 건강한 사람이 희귀병을 앓고 있다고 잘못 진단할 확률도 6% 있다.&lt;/p&gt;

&lt;p&gt;우선 Prior Probability와 Data Probability를 알아두자.&lt;/p&gt;

&lt;p&gt;$D$ : 희귀병&lt;br /&gt;
$P$ : 양성&lt;br /&gt;
$N$ : 음성&lt;/p&gt;

&lt;p&gt;$P(D)=0.01, P(\sim D)=0.99$&lt;br /&gt;
$P(P| D)=0.97, P(N| D)=0.03$&lt;br /&gt;
$P(P| \sim D)=0.06, P(N| \sim D)=0.94$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$P(P)$&lt;br /&gt;
$P(P)=P(D\cap P) + P(\sim D\cap P)$&lt;br /&gt;
$\qquad=P(D)P(P|D)+P(\sim D)P(P|\sim D)$&lt;br /&gt;
$\qquad=0.0691$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(D| P)$&lt;br /&gt;
$P(D| P)={P(D\cap P)\over P(P)}=0.14$  (위에서 다 구했던 것들임)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(\sim D| N)$&lt;br /&gt;
$P(\sim D| N)={P(\sim D\cap N)\over P(N)}={P(\sim D)P(N|\sim D)\over 1-P(P)}=0.999$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">YOLO, SSD, YOLO 9000</title><link href="http://localhost:4000/posts/2020/05/11/yolo" rel="alternate" type="text/html" title="YOLO, SSD, YOLO 9000" /><published>2020-05-11T00:00:00-07:00</published><updated>2020-05-11T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/11/yolo</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/11/yolo">&lt;h2 id=&quot;yolo&quot;&gt;YOLO&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;You only look once!&lt;/li&gt;
  &lt;li&gt;성능 자체는 Faster R-CNN과 비슷하나, 속도가 훨씬 빠름&lt;/li&gt;
  &lt;li&gt;기존의 방법에서는 bounding box를 만들어서 그것을 Neural Network에 넣어 분류를 하는 방법을 사용했다면 YOLO에서는 bounding box를 찾는 것과 classification을 동시에 수행한다. (detection 문제를 regression 문제로 바꿈)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;flow&quot;&gt;Flow&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;이미지를 입력받으면 $S\times S$로 줄인다.(Max Pooling을 통해서 줄임, 여기서 $S$=7) 각 칸을 그리드라고 부른다.&lt;/li&gt;
  &lt;li&gt;각 그리드마다  class probability를 C개씩, bounding box를 B개씩 부여한다. 각 bounding box는 [x, y, w, h, conf]로 이루어져 있다. 마지막에 있는 conf(confidence)는 이 bounding box가 쓸모있는 것인지 아닌지를 나타낸다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;만약 S=7, B=2, C=20이라면 마지막 layer는 $7\times 7\times 30$이다.($30=20+2\times 5$)&lt;/p&gt;

&lt;h3 id=&quot;yolo의-한계&quot;&gt;YOLO의 한계&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;각 그리드가 B개 만의 bounding box만을 만드므로 조그마한 물체들이 뭉쳐 있을 때는 좋은 성능을 발휘하지 못한다.&lt;/li&gt;
  &lt;li&gt;bounding box의 정보가 정확하지 않을 수 있다.&lt;/li&gt;
  &lt;li&gt;Loss Function이 작은 box와 큰 box를 동일하게 다뤄 에러를 측정하므로 점수를 메길 때 불리하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ssd&quot;&gt;SSD&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Single Shot MultiBox Detector&lt;/li&gt;
  &lt;li&gt;YOLO의 컨셉(Single Shot)과 Faster R-CNN의 Region Proposal(MultiBox)을 합친 것&lt;/li&gt;
  &lt;li&gt;Predicting category scores from each cell of multiple convolutional feature maps.&lt;/li&gt;
  &lt;li&gt;Predicting Box offsets for a fixed set of default boxes from each cell of multiple convolutional feature maps.&lt;/li&gt;
  &lt;li&gt;즉, 각 feature map의 셀 마다 k개의 default box(anchor box)를 가지며, 각 box는 c개의 class score와 4개의 offset을 가지고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;yolo-9000&quot;&gt;YOLO 9000&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;SSD보다 좋은 성능을 내기 위해 이것저것 많은 시도를 한 논문. 실용적인 내용이 많다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;yolo가-잘-안됐던-이유&quot;&gt;YOLO가 잘 안됐던 이유&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Localization Errors&lt;/li&gt;
  &lt;li&gt;Low recall compared region proposal based method
box를 적게 뽑았으니 recall 성능이 낮았다는 말임.
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81780462-85443f80-9531-11ea-9085-b056bb24514d.jpg&quot; alt=&quot;1&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;개선-방법faster&quot;&gt;개선 방법(Faster)&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Batch Normalization
2% 성능 개선(웬만하면 Batch Normalization 쓰는 것이 좋음)&lt;/li&gt;
  &lt;li&gt;High Resolution
original YOLO는 $224\times 224$를 썼는데, 여기서는 $448\times 448$을 썼다.&lt;/li&gt;
  &lt;li&gt;Anchor Boxes
Region Proposal Network에서 사용된 anchor box와 비슷한 것을 사용
anchor box마다 class와 objectiveness를 예측&lt;/li&gt;
  &lt;li&gt;Dimension Cluster
기존에는 anchor box의 pre-defined 크기를 정할 때 실제 데이터셋의 box 크기르 고려하지 않고 사람이 임의로 정했다. YOLO 9000에서는 실제 데이터셋에서 bounding box를 클러스터링해서 거기에 많이 쓰이는 bounding box 크기를 사용&lt;/li&gt;
  &lt;li&gt;Location Prediction
YOLO 9000은 box offset과 scaling을 예측한다. 즉, 사이즈를 몇 배 키울지 혹은 줄일지를 예측&lt;/li&gt;
  &lt;li&gt;Fine-grained Features&lt;/li&gt;
  &lt;li&gt;Multi-scale Training&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;개선-방법stronger&quot;&gt;개선 방법(Stronger)&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Hierarchical classification
YOLO 9000은 9000개의 클래스를 구분할 수 있다는 말이다. 9000개 중 하나를 찾는 것은 매우 힘든 일이다. 그래서 여기서는 Hierarchical classification을 적용한다. 예를 들어, 요커셔 테리어를 생각해보면 “physical object” - “animal” - “mammal” - … - “hunting dog” - “terrior” - Yorkshire terriror”라는 식으로 parent에 해당하는 라벨도 생각해낼 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81780457-837a7c00-9531-11ea-8b3c-177b763d78ae.PNG&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림처럼  위계마다 하나의 클래스를 선택한다. 결과적으로는 한 이미지를 입력하면 여러 개의 클래스를 가지는 결과가 나온다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">YOLO You only look once! 성능 자체는 Faster R-CNN과 비슷하나, 속도가 훨씬 빠름 기존의 방법에서는 bounding box를 만들어서 그것을 Neural Network에 넣어 분류를 하는 방법을 사용했다면 YOLO에서는 bounding box를 찾는 것과 classification을 동시에 수행한다. (detection 문제를 regression 문제로 바꿈)</summary></entry><entry><title type="html">R-CNN, SPP, Fast R-CNN, Faster R-CNN</title><link href="http://localhost:4000/posts/2020/05/10/r-cnn" rel="alternate" type="text/html" title="R-CNN, SPP, Fast R-CNN, Faster R-CNN" /><published>2020-05-10T00:00:00-07:00</published><updated>2020-05-10T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/10/r-cnn</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/10/r-cnn">&lt;h2 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771298-92573380-951d-11ea-8aea-25f440c43aec.PNG&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최초로 딥러닝을 활용해 detection 문제에 접근&lt;/li&gt;
  &lt;li&gt;매우 간단&lt;/li&gt;
  &lt;li&gt;이미지 안에서 여러 개의 bounding box를 뽑아냄.(region proposal) 이 부분은 딥러닝을 사용하지 않았으며, 속도가 느리기 때문에 bottle neck이 된다. selective search 방법을 사용한다.&lt;/li&gt;
  &lt;li&gt;모든 bounding box를 같은 사이즈로 resize하고, resize된 box를 CNN에 넣어 feature를 추출한다. (모든 box에 대해 CNN을 수행하므로 속도가 느림)&lt;/li&gt;
  &lt;li&gt;추출된 feature를 SVM으로 분류한다.&lt;/li&gt;
  &lt;li&gt;분류 카테고리가 $n$개라면 실제로는 “background”까지 포함해 $n+1$개가 되어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;spp&quot;&gt;SPP&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771299-93886080-951d-11ea-81d7-9e532edc17ed.PNG&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RCNN은 한 이미지의 region proposals의 수만큼 CNN이 돌아야 했다. 이 부분 때문에 bottle neck이 생겼는데, SPP는 한 번만 돌면 되도록 개선시켰다.&lt;/li&gt;
  &lt;li&gt;SPP는 convolution 연산을 한 후에 나오는 convolutional feature map 위에서 원하는 정보를 찾는다. convolutional feature map에서 뽑히는 박스의 위치는 이미지에서 뽑힌 bounding box의 위치에서 뽑는다.(이 때도 딥러닝을 사용하지는 않는다) box를 뽑았다면 Spatial Pyramid Pooling을 사용한다. Spatial Pyramid Pooling의 목적은 fixed-length representation을 찾는 것이다.&lt;/li&gt;
  &lt;li&gt;Spatial Pyramid Pooling이란 무엇일까?
만약 우리가 뽑은 convolution feature map에서 뽑은 박스의 사이즈가 $2\times 4$라고 하자.&lt;br /&gt;
우리가 구하고자 하는 fixed-length가 $1\times 1$이라고 한다면 $2\times 4$의 값들을 평균내어 사용한다.&lt;br /&gt;
또한 우리가 구하고자 하는 fixed-length가 $2\times 2$라고 한다면 $2\times 4$를 네 부분으로 나누어 각각을 평균내어 사용한다. 즉, Pooling을 통해 같은 사이즈로 바꾸는 작업이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fast-r-cnn&quot;&gt;Fast R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771301-9420f700-951d-11ea-988e-1f6f110c6c02.PNG&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fast R-CNN과 SPP는 구조가 거의 같다.&lt;/li&gt;
  &lt;li&gt;굳이 차이점이 있다면 fixed-length representation을 찾는 방식에 차이가 있다. SPP는 Spatial Pyramid Pooling을 사용하였고, Fast R-CNN은 ROI Pooling을 통해 찾는다. 사실 이 둘도 비슷하긴 하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;flow&quot;&gt;Flow&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;입력 이미지를 받고, 입력 이미지로부터 bounding box를 뽑는다.(딥러닝을 사용하지 않는다.)&lt;/li&gt;
  &lt;li&gt;CNN을 통해 convolutional feature map을 생성한다.&lt;/li&gt;
  &lt;li&gt;ROI Pooling을 통해 각 bounding box의 fixed-length feature vector를 만든다.&lt;/li&gt;
  &lt;li&gt;결과는 두 가지가 되는데, 하나는 “background”를 고려한 (k+1) classification이고, 다른 하나는 bounding box regression이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;roi-pooling&quot;&gt;ROI Pooling&lt;/h3&gt;
&lt;p&gt;입력을 미리 정해둔 사이즈로 쪼갠 후, 해당 하는 부분을 평균내서 벡터로 만든다. 예를 들어, $3\times 3$으로 쪼개는 것으로 정해뒀다면 입력을 $3\times 3$으로 쪼개고, 각각을 평균낸 후 이어붙여 길이가 9인 벡터로 만든다.&lt;/p&gt;

&lt;h2 id=&quot;faster-r-cnn&quot;&gt;Faster R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771304-94b98d80-951d-11ea-9505-6f5ac079ca4d.PNG&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;지금까지 배운 것은 나이브한 방법에 딥러닝을 바로 적용한 detection 방법론이었다. 하지만 여기부터는 딥러닝을 활용한 region proposal이 시작된다. (기존에는 이 부분이 bottleneck이었음)&lt;/li&gt;
  &lt;li&gt;Faster R-CNN = Region Proposal Network + Fast R-CNN&lt;/li&gt;
  &lt;li&gt;어떠한 이미지의 사이즈와 스케일이 다를 때 고려할 수 있는 방법은 세 가지가 있다. 첫 번째 방법은 Pyramids of images이며, Spatial Pyramid Pooling이 여기에 해당된다. 이미지 자체의 사이즈를 줄여서 줄어든 multiple scaled image를 한 번에 고려하는 것이다. 두 번째 방법은 Pyramids of filters이며, 이 방법은 필터의 사이즈를 바꾸는 것이다. 세 번째 방법이 &lt;strong&gt;Pyramids of anchors&lt;/strong&gt;이다. Anchor는 미리 정해져있는 bounding box의 크기를 의미한다. 미리 bounding box의 크기가 어느정도 될 것인지를 생각을 해두고, 이 box를 얼만큼 바꿔야지 target box가 되는지에 해당하는 offset을 학습한다.&lt;/li&gt;
  &lt;li&gt;Region Proposal Network는 이미지나 convolutional feature map을 입력으로 받는다. 각 픽셀마다 Anchor를 정의한다. 각 점을 중앙으로 하는 k개의 Anchor box를 만든다. 각 Anchor box마다 2개의 점수를 할당하는데(2k scores) 앞의 것이 크면 positive(쓸만한 Anchor box), 뒤의 것이 크면 negative(쓸모없는 Anchor box)이다. 또한 4k coordinates는 각 Anchor boxes를 어떻게 움직여야 target box에 가까워질 것인지에 대한 숫자가 들어있다.&lt;/li&gt;
  &lt;li&gt;즉, Region Proposal Network는 총 $k\times (4+2)$ 사이즈의 벡터를 출력한다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">R-CNN 최초로 딥러닝을 활용해 detection 문제에 접근 매우 간단 이미지 안에서 여러 개의 bounding box를 뽑아냄.(region proposal) 이 부분은 딥러닝을 사용하지 않았으며, 속도가 느리기 때문에 bottle neck이 된다. selective search 방법을 사용한다. 모든 bounding box를 같은 사이즈로 resize하고, resize된 box를 CNN에 넣어 feature를 추출한다. (모든 box에 대해 CNN을 수행하므로 속도가 느림) 추출된 feature를 SVM으로 분류한다. 분류 카테고리가 $n$개라면 실제로는 “background”까지 포함해 $n+1$개가 되어야 한다.</summary></entry></feed>