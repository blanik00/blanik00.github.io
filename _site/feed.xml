<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-30T09:08:01-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JeongUk Jang</title><subtitle>Github Pages template for academic personal websites, forked from mmistakes/minimal-mistakes</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><entry><title type="html">Neural Collaborative Filtering</title><link href="http://localhost:4000/paper_review/2020/04/24/Neural_Collaborative_Filtering" rel="alternate" type="text/html" title="Neural Collaborative Filtering" /><published>2020-04-24T00:00:00-07:00</published><updated>2020-04-24T00:00:00-07:00</updated><id>http://localhost:4000/paper_review/2020/04/24/Neural_Collaborative_Filtering</id><content type="html" xml:base="http://localhost:4000/paper_review/2020/04/24/Neural_Collaborative_Filtering">&lt;h1 id=&quot;요약&quot;&gt;요약&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Implicit Feedback을 활용한 Collaborative Filtering을 다룬다.&lt;/li&gt;
  &lt;li&gt;output이 1이면 User와 Item은 연관성이 있는 것이며, 0이면 User와 Item은 연관성이 없는 것이다. 즉, binary classification 문제이다.&lt;/li&gt;
  &lt;li&gt;Collaborative Filtering에서 Matrix Factorization은 많이 쓰이는 기법이다.&lt;/li&gt;
  &lt;li&gt;기존의 Matrix Factorization은 User Latent Factor와 Item Latent Factor를 구하고, 두 Latent Factor를 내적하는 방법을 통해 Rating Matrix를 복원한다.&lt;/li&gt;
  &lt;li&gt;하지만 이 방법은 User-Item Interaction을 충분히 표현하지 못한다.&lt;/li&gt;
  &lt;li&gt;저자는 &lt;strong&gt;내적&lt;/strong&gt;이라는 지나치게 단순한 방법으로 Rating Matrix를 복원했기 때문이라고 본다.&lt;/li&gt;
  &lt;li&gt;대안으로 &lt;strong&gt;뉴럴 네트워크&lt;/strong&gt;의 사용을 권한다.&lt;/li&gt;
  &lt;li&gt;뉴럴 네트워크를 도입함으로써 Matrix Factorization이 가지는 Linearity와 뉴럴 네트워크가 가지는 Non-Linearity를 모두 가질 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;문제-제시&quot;&gt;문제 제시&lt;/h1&gt;
&lt;p&gt;기존의 Matrix Factorization은 SGD나 ALS 등의 방법을 사용해서 User-Item Interaction($M\times N$)을 User Latent Factor($M\times K$)와 Item Latent Factor($K\times N$)로 분해한다(K « M, N).&lt;/p&gt;

&lt;p&gt;그 후 두 Latent Factor를 내적해 Rating Matrix를 복원하는 방법을 통해 User가 Item을 얼마나 선호하는지 등을 예측한다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499831-6a59c300-89a8-11ea-9e96-de5ff1654354.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;내적의 의미를 생각해보면 내적을 한다는 것은 User Latent Factor와 Item Latent Factor를 선형 결합해서 Rating Matrix를 복원한다는 것이다(Linear). 하지만 Linear한 방법은 단순한 예제에서는 잘 작동하지만, User-Item Interaction과 같이 복잡한 관계를 가지는 것을 잘 표현하지 못한다는 단점이 있다.&lt;/p&gt;

&lt;h1 id=&quot;대안-제시&quot;&gt;대안 제시&lt;/h1&gt;
&lt;p&gt;저자는 이 문제를 해결하기 위해 두 가지 방법이 있다고 말한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Latent Factor의 Dimension(K)을 늘리는 것&lt;/li&gt;
  &lt;li&gt;Non-Linear한 방법을 통해 모델 구축&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;둘 다 좋은 방법이지만 1번 방법의 경우 오버피팅이 발생한다는 문제가 있어 2번 방법을 채택했으며, Non-Linearity를 추가하기 위한 방법으로 Neural Network를 도입한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;뉴럴-네트워크-도입으로-인해-달라지는-것들&quot;&gt;뉴럴 네트워크 도입으로 인해 달라지는 것들&lt;/h1&gt;

&lt;h2 id=&quot;1-embedding-layer&quot;&gt;1. Embedding Layer&lt;/h2&gt;
&lt;p&gt;Embedding Layer를 한 문장으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Turns positive integers (indexes) into dense vectors of fixed size.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, categorical한 input을 고정된 사이즈의 dense한 벡터로 바꿔주는 Layer이다. 이 논문에서는 User Latent Factor와 Item Latent Factor를 표현하는 데 사용한다.&lt;/p&gt;
&lt;h2 id=&quot;2-필요한-모델&quot;&gt;2. 필요한 모델&lt;/h2&gt;
&lt;h3 id=&quot;0-neural-collaborative-filtering-framework&quot;&gt;(0) Neural Collaborative Filtering Framework&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499827-69c12c80-89a8-11ea-920a-969720de8bec.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒤에 나오는 모델들의 뼈대가 되는 것이다. 다음과 같은 과정을 거친다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Input Layer&lt;/li&gt;
  &lt;li&gt;Embedding Layer&lt;/li&gt;
  &lt;li&gt;Neural CF Layers&lt;/li&gt;
  &lt;li&gt;Output Layer&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1-mlpmulti-layer-perceptron&quot;&gt;(1) MLP(Multi-Layer Perceptron)&lt;/h3&gt;

&lt;p&gt;앞의 Neural Collaborative Filtering Framework을 충실히 재현한 모델이다. User Latent Factor와 Item Latent Factor를 여러 Layer를 거치게 해 결과를 낸다.(Layer의 activation function을 relu로 해 Non-Linearity를 얻는다.)&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499829-6a59c300-89a8-11ea-97fb-11ba105acc76.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$W_x$ : weight matrix for x-th layer&lt;br /&gt;
$b_x$ : bias vector for x-th layer&lt;br /&gt;
$a_x$ : activation function for x-th layer&lt;br /&gt;
$h$ : edge weights of the output layer&lt;/p&gt;

&lt;p&gt;수식에서 볼 수 있듯이 첫 번째 layer는 User Latent Factor와 Item Latent Factor를 Concatenate한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;2-gmfgeneralized-matrix-factorization&quot;&gt;(2) GMF(Generalized Matrix Factorization)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499823-69289600-89a8-11ea-959a-3d6377197ead.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 앞에서 본 Neural Collaborative Filtering Framework의 특별한 케이스 중 하나를 다룰 것이다.&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499822-688fff80-89a8-11ea-8ca8-fa287685f3d2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$a_{out}$ : activation function&lt;br /&gt;
$h$ : edge weights of the output layer&lt;br /&gt;
$\odot$ : element-wise product&lt;/p&gt;

&lt;p&gt;만약 activation function으로 non-linear한 함수를 사용하면 non-linear한 	모델로도 사용할 수 있다. 하지만 논문에서 저자는 GMF 모델이 Linearity를 가지게 한다(저자가 작성한 &lt;a href=&quot;https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/GMF.py&quot;&gt;코드&lt;/a&gt;를 보면 확인할 수 있다).&lt;/p&gt;

&lt;p&gt;$a_{out}$이 identity function($y=x$)이며, $h$가 1로 이루어진 벡터라면, Matrix Factorization과 똑같아진다.&lt;/p&gt;

&lt;p&gt;실제로는 $a_{out}$과 $h$를 없애고, $p_u^G\odot p_u^M$만을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;3-neumfneural-matrix-factorization&quot;&gt;(3) NeuMF(Neural Matrix Factorization)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499819-67f76900-89a8-11ea-9f46-525a6837a031.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NeuMF는 GMF와 MLP에서 얻은 최종 Layer를 Concatenate하여 결과를 낸다. GMF와 MLP를 결합해 Linearity와 Non-Linearity 모두를 얻고자 하는 모델이다.&lt;/p&gt;

&lt;p&gt;수식으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499815-67f76900-89a8-11ea-984f-43480da9eb07.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$p_u^G$ : User Embedding for GMF&lt;br /&gt;
$p_u^M$ : User Embedding for MLP&lt;br /&gt;
$q_i^G$ : Item Embedding for GMF&lt;br /&gt;
$q_i^M$ : Item Embedding for MLP&lt;/p&gt;

&lt;h1 id=&quot;결과&quot;&gt;결과&lt;/h1&gt;
&lt;h2 id=&quot;1-다른-모델과의-비교&quot;&gt;1. 다른 모델과의 비교&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499809-675ed280-89a8-11ea-8e8d-49c6c72d7ea3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-pre-training의-성과&quot;&gt;2. Pre-training의 성과&lt;/h2&gt;
&lt;p&gt;Pre-training에 대한 내용은 뒤에 나온다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499805-662da580-89a8-11ea-97a6-29afd932fd3d.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-3가지-모델의-성능-비교&quot;&gt;3. 3가지 모델의 성능 비교&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499803-65950f00-89a8-11ea-925d-f29d1bca75af.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-negative-sample의-수에-따른-성능-비교&quot;&gt;4. Negative Sample의 수에 따른 성능 비교&lt;/h2&gt;
&lt;p&gt;Negative Sampling에 대한 내용은 뒤에 나온다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499801-6463e200-89a8-11ea-9ad3-1ff6c31a7290.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-mlp-layer의-수에-따른-성능-비교&quot;&gt;5. MLP Layer의 수에 따른 성능 비교&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80499792-62018800-89a8-11ea-92d8-3ed1cd21bbf9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;기타&quot;&gt;기타&lt;/h1&gt;
&lt;h2 id=&quot;1-pre-training&quot;&gt;1. Pre-training&lt;/h2&gt;
&lt;p&gt;NCF의 Loss Function은 non-convex하기 때문에 GD를 사용하면 Local Optimum에 수렴할 가능성이 있다. 이러한 경우 초기값을 어떻게 주는지가 딥러닝 모델의 성능에 큰 영향을 미친다. 저자는 미리 훈련된(pretrained) GMF, MLP 모델을 사용하여 NeuMF의 weights를 초기화하는 방식을 제안한다.&lt;/p&gt;

&lt;p&gt;방법은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;GMF와 MLP 모델을 수렴할 때 까지 훈련시킨다.&lt;/li&gt;
  &lt;li&gt;GMF와 MLP의 파라미터를 NeuMF의 해당하는 부분에 초기값으로 대입한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2-how-to-deal-with-absence-of-negative-feedback&quot;&gt;2. How to deal with Absence of Negative Feedback&lt;/h2&gt;
&lt;p&gt;Implicit Feedback을 활용한 추천 시스템은 아마 unobserved data를 어떻게든 해결해야 할 것이다. 이 문제는 Explicit Feedback과 비교하면 명백하게 알 수 있다.&lt;/p&gt;

&lt;p&gt;점수가 1~5점 사이에 분포해 있는 Explicit Feedback의 경우 1점은 “유저가 아이템을 좋아하지 않는다”는 것을 나타내며, 5점은 “유저가 아이템을 좋아한다”는 것을 나타낸다.&lt;/p&gt;

&lt;p&gt;하지만 Implicit Feedback의 경우 0점을 기록(ex. 유저가 해당 아이템을 구매한 적이 없음)하더라도 “유저가 해당 아이템을 좋아하지 않는다”고 말할 수 없다. 유저가 실제로 아이템을 좋아하지 않는 경우 이외에도 유저가 아이템을 인지하지 못했던 경우가 있기 때문이다. 이러한 문제를 부정적인 피드백의 부재(absence of negative feedback)라고 한다.&lt;/p&gt;

&lt;p&gt;만약 부정적인 피드백를 모두 포함시켜 학습을 시킨다면 부정적인 피드백의 수가 긍정적인 피드백보다 훨씬 많아 긍정적인 피드백 무시하는 방향으로 학습이 진행될 것이다. 반대로 부정적인 피드백를 모두 제외시킨다면 긍정적인 피드백만 학습이 진행되어 우리의 최종 목표인 binary classification을 수행하지 못할 것이다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위한 두 가지 방법을 제안한다. 하나는 이번 논문인 Neural Collaborative Filtering에서 사용한 방법이고, 다른 하나는 Collaborative Filtering for Implicit Feedback Datasets이라는 논문에서 사용한 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;1-negative-sampling&quot;&gt;1. Negative Sampling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Neural Collaborative Filtering&lt;/li&gt;
  &lt;li&gt;유저가 긍정적인 피드백을 준 것 이외에 유저가 아직 피드백을 주지 않은 k개의 아이템을 포함시켜 Training Set을 구성한다.(k는 하이퍼 파라미터이며, 이 논문에서는 $k=4$)&lt;/li&gt;
  &lt;li&gt;긍정적인 피드백이 $n$개라면 Training Set은 $n(k+1)$개가 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback의 일부만 샘플링해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-confidence-level&quot;&gt;2. Confidence Level&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/li&gt;
  &lt;li&gt;Confidence Level이라는 개념을 도입한다. 유저가 아이템과 연관성이 있는지 정도를 수치적으로 표현하기 위한 것이다.&lt;/li&gt;
  &lt;li&gt;$c_{ui}=1+\alpha r_{ui}$로 계산할 수 있다. $c_ui$가 클수록 유저와 아이템은 연관성이 큰 것이다.&lt;/li&gt;
  &lt;li&gt;$r_{ui}$는 긍정적인 피드백의 수이며, $r_{ui}$가 증가할수록 $c_ui$도 커진다. (증가하는 정도를 나타낸 것이 $\alpha$다. $\alpha$는 하이퍼 파라미터이며, 이 논문에서는 $\alpha=40$)&lt;/li&gt;
  &lt;li&gt;$c_{ui}$는 모두 1 이상의 값을 갖게 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback도 모두 사용해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-loss-function&quot;&gt;3. Loss Function&lt;/h2&gt;
&lt;p&gt;이 모델에서 output은 0과 1이므로 Bernoulli Distribution을 따른다.&lt;/p&gt;

&lt;p&gt;베르누이 분포에서 가능도는 다음과 같이 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80507963-659a0c80-89b2-11ea-9c1a-a85714fa00d1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가능도의 negative logarithm은 다음과 같다. 이 식을 활용해서 loss를 최소화하는 방향으로 학습을 진행시킬 수 있다(식이 binary cross-entropy와 같다).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80507967-6763d000-89b2-11ea-8d63-b4fc7d23c660.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05031.pdf&quot;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;http://yifanhu.net/PUB/cf.pdf&quot;&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;review&quot;]" /><summary type="html">요약 Implicit Feedback을 활용한 Collaborative Filtering을 다룬다. output이 1이면 User와 Item은 연관성이 있는 것이며, 0이면 User와 Item은 연관성이 없는 것이다. 즉, binary classification 문제이다. Collaborative Filtering에서 Matrix Factorization은 많이 쓰이는 기법이다. 기존의 Matrix Factorization은 User Latent Factor와 Item Latent Factor를 구하고, 두 Latent Factor를 내적하는 방법을 통해 Rating Matrix를 복원한다. 하지만 이 방법은 User-Item Interaction을 충분히 표현하지 못한다. 저자는 내적이라는 지나치게 단순한 방법으로 Rating Matrix를 복원했기 때문이라고 본다. 대안으로 뉴럴 네트워크의 사용을 권한다. 뉴럴 네트워크를 도입함으로써 Matrix Factorization이 가지는 Linearity와 뉴럴 네트워크가 가지는 Non-Linearity를 모두 가질 수 있다.</summary></entry><entry><title type="html">Recommender System for Ecommerce</title><link href="http://localhost:4000/project/2020/04/23/Recommender_System_for_Ecommerce" rel="alternate" type="text/html" title="Recommender System for Ecommerce" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>http://localhost:4000/project/2020/04/23/Recommender_System_for_Ecommerce</id><content type="html" xml:base="http://localhost:4000/project/2020/04/23/Recommender_System_for_Ecommerce"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><summary type="html"></summary></entry><entry><title type="html">확률 및 통계 7강</title><link href="http://localhost:4000/lecture/2020/04/16/prob_stat_07" rel="alternate" type="text/html" title="확률 및 통계 7강" /><published>2020-04-16T00:00:00-07:00</published><updated>2020-04-16T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/16/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%207%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/16/prob_stat_07">&lt;h2 id=&quot;chebyshev-inequality&quot;&gt;Chebyshev Inequality&lt;/h2&gt;

&lt;p&gt;$P(|X-E[X]|\ge a)\le {\sigma_X^2 \over a^2}$&lt;/p&gt;

&lt;p&gt;어떤 랜덤한 $X$와 $X$의 평균의 차이의 정도를 $a$라 보고, $a$보다 차이가 더 많이 날 확률은 ${\sigma_X^2 \over a^2}$보다 작다.($\sigma_X^2$는 X의 분산)&lt;/p&gt;

&lt;p&gt;즉, 임의로 선택한 $X$는 $X$의 평균과 차이를 보이기 마련인데, 우리가 예측한 것(a)보다 더 크게 차이를 보일 확률은 얼마나 되는지 수치적으로 표현한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;chapter-4-special-distribution&quot;&gt;Chapter 4. Special Distribution&lt;/h1&gt;
&lt;h2 id=&quot;42-bernoulli-distribution베르누이-분포&quot;&gt;4.2 Bernoulli Distribution(베르누이 분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RV X : binary&lt;/li&gt;
  &lt;li&gt;보통 하나를 success, 다른 하나를 failure라고 한다.&lt;/li&gt;
  &lt;li&gt;$P(success) = p, P(failure) = 1-p$&lt;/li&gt;
  &lt;li&gt;보통 success한 경우를 1로 두고, failure한 경우를 0으로 둔다.&lt;/li&gt;
  &lt;li&gt;$E[X]=p$&lt;/li&gt;
  &lt;li&gt;$\sigma_X^2=p(1-p)$&lt;/li&gt;
  &lt;li&gt;Bernoulli Distribution를 바탕으로 많은 Distribution이 만들어진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;43-binomial-distribution이항분포&quot;&gt;4.3 Binomial Distribution(이항분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RV X : number of successes out of n Bernoulli trails(베르누이 시행을 n 번 반복해서 성공한 횟수)&lt;/li&gt;
  &lt;li&gt;$x=0,1,2,…,n$. 즉, discrete하다.&lt;/li&gt;
  &lt;li&gt;$P_X(x)={n \choose x}p^x(1-p)^{n-x}$&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$\sum_{x=0}^nP_X(x)=\sum_{x=0}^n{n \choose x}p^x(1-p)^{n-x}=(p+(1-p))^n=1(\because (a+b)^n=\sum_{x=0}^n{n \choose x}a^xb^{n-x})$&lt;/li&gt;
  &lt;li&gt;$E[X]=\sum_{x=0}^nx{n \choose x}p^x(1-p)^{n-x}=np$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{xn! \over (n-x)!x!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n! \over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n(n-1)! \over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n(n-1)!p \over (n-1-(x-1))!(x-1)!}p^{x-1}(1-p)^{(n-1-(x-1))}$&lt;br /&gt;
$\qquad =\sum_{x=0}^n{n(n-1)!p \over (n-1-(x-1))!(x-1)!}p^{x-1}(1-p)^{(n-1-(x-1))}$&lt;br /&gt;
$\qquad x’=x-1$로 치환&lt;br /&gt;
$\qquad =\sum_{x=0}^{n-1}np{(n-1)!px’(1-p)^{n-1-x’} \over (n-1-x’)!(x’)!}$&lt;br /&gt;
$\qquad =np(p+(1-p))^{n-1}$&lt;br /&gt;
$\qquad =np$&lt;/li&gt;
  &lt;li&gt;$\sigma_X^2[X]=E[X^2]-(E[X])^2$&lt;br /&gt;
$E[X^2]=\sum_{x=0}^n{x^2n!\over (n-x)!x!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=1}^n{xn!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=1}^n{(x-1)n!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}+\sum_{x=1}^n{n!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}$&lt;br /&gt;
$\qquad =\sum_{x=1}^n{(x-1)n!\over (n-x)!(x-1)!}p^x(1-p)^{n-x}+np$&lt;br /&gt;
$\qquad =\sum_{x=2}^n{n!\over (n-x)!(x-2)!}p^x(1-p)^{n-x}+np$&lt;br /&gt;
$\qquad =\sum_{x=2}^n{n(n-1)(n-2)!\over ((n-2)-(x-2))!(x-2)!}p^2p^{x-2}(1-p)^{(n-x)-(x-2)}+np$&lt;br /&gt;
$\qquad x’=x-2$로 치환&lt;br /&gt;
$\qquad =\sum_{x=2}^n{n-2 \choose x’}p^{x’}(1-p)^{n-2-x’}n(n-1)p^2+np$&lt;br /&gt;
$\qquad =(p+(1-p))^{n-2}n(n-1)p^2+np$&lt;br /&gt;
$\qquad \therefore \sigma_X^2=E[X^2]-n^2p^2$&lt;br /&gt;
$\qquad =np(1-p)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;44-geometrid-distribution기하분포&quot;&gt;4.4 Geometrid Distribution(기하분포)&lt;/h2&gt;
&lt;p&gt;RV X : Number of Bernoulli trials until first success&lt;br /&gt;
$P_X(x)=(1-p)^{x-1}p$, $x=1,2,3,…$&lt;br /&gt;
$E[X]={1\over p}$&lt;br /&gt;
$\sigma_X^2={1-p\over p^2}$&lt;/p&gt;

&lt;h3 id=&quot;forgetfulnessmemoryless&quot;&gt;Forgetfulness(Memoryless)&lt;/h3&gt;
&lt;p&gt;6이 나올 때 까지 주사위를 던지는 확률 변수라고 하자.&lt;br /&gt;
10번을 던졌는데 6이 안나왔다. 이 경우에 다섯 번을 더 던져 6이 나올 확률은 얼마일까? 이것은 기하분포 공식을 통해 계산해낼 수 있다. $(1-p)^4p$이다.&lt;br /&gt;
이번에는 15번을 던졌는데 6이 안나왔다. 이 경우에도 다섯 번을 더 던져 6이 나올 확률은 얼마인지 계산해보자. $(1-p)^4p$이다.&lt;br /&gt;
이 예시는 기하 분포가 과거 기록에 영향을 받지 않는다는 것을 보여준다. 이런 성질을 Forgetfulness라고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Consider K additional trials until the first success, given n trials fail.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$P(X=n+k|X&amp;gt;n)$&lt;br /&gt;
$={P(X=n+k\cap X&amp;gt;n)\over P(X&amp;gt;n)}$&lt;br /&gt;
$={P(X=n+k)\over P(X&amp;gt;n)}$&lt;br /&gt;
$={(1-p)^{n+k-1}p\over \sum_{x=n+1}^{\infty}(1-p)^{x-1}p}$&lt;br /&gt;
$={(1-p)^{n+k-1}p\over {p(1-p)^n\over 1-(1-p)}}$&lt;br /&gt;
$=p(1-p)^{k-1}$&lt;br /&gt;
$=P(X=k)$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">Chebyshev Inequality</summary></entry><entry><title type="html">Collaborative Filtering for Implicit Feedback Datasets</title><link href="http://localhost:4000/paper_review/2020/04/16/Collaborative_Filtering_for_Implicit_Feedback_Datasets" rel="alternate" type="text/html" title="Collaborative Filtering for Implicit Feedback Datasets" /><published>2020-04-16T00:00:00-07:00</published><updated>2020-04-16T00:00:00-07:00</updated><id>http://localhost:4000/paper_review/2020/04/16/Collaborative_Filtering_for_Implicit_Feedback_Datasets</id><content type="html" xml:base="http://localhost:4000/paper_review/2020/04/16/Collaborative_Filtering_for_Implicit_Feedback_Datasets"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;review&quot;]" /><summary type="html"></summary></entry><entry><title type="html">미분적분학 1강</title><link href="http://localhost:4000/lecture/2020/04/14/calculus_01" rel="alternate" type="text/html" title="미분적분학 1강" /><published>2020-04-14T00:00:00-07:00</published><updated>2020-04-14T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/14/%EB%AF%B8%EB%B6%84%EC%A0%81%EB%B6%84%ED%95%99%201%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/14/calculus_01">&lt;h2 id=&quot;1-함수의-정의&quot;&gt;1. 함수의 정의&lt;/h2&gt;
&lt;h3 id=&quot;함수의-정의&quot;&gt;함수의 정의&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$f:X\to Y$&lt;/li&gt;
  &lt;li&gt;X라는 집합에 있는 원소를 어떻게 Y로 보내느냐, 그 규칙을 결정하는 것이 함수&lt;/li&gt;
  &lt;li&gt;X를 정의역, Y를 공역이라 한다. 즉, $x \to f(x)=y$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;함수의-조건&quot;&gt;함수의 조건&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;모든 x가 y로 가야 한다.&lt;/li&gt;
  &lt;li&gt;각 x는 하나의 y로만 가야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;예시&quot;&gt;예시&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;아래 함수들은 앞으로의 예시에서 계속 등장하니 자주 참고해야 한다.&lt;/li&gt;
  &lt;li&gt;정의역 X는 실수 전체&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$y=e^x$&lt;/li&gt;
  &lt;li&gt;$y=lnx$&lt;/li&gt;
  &lt;li&gt;$y=sinx$&lt;/li&gt;
  &lt;li&gt;$y=x^3$&lt;/li&gt;
  &lt;li&gt;$x^2+y^2=1$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371682-3e1d4400-88cd-11ea-821f-c97ee6594d23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2번 : 함수가 아니다. $x&amp;lt;0$에서 y값을 대응시킬 수 없다. 즉, 1번 조건을 만족시키지 못한다. 단, 정의역을 $x&amp;gt;0$이라고 제한한다면 함수라고 할 수 있다.&lt;/li&gt;
  &lt;li&gt;5번 : 하나의 x가 두 개의 y를 가질 수 있다. 즉, 2번 조건을 만족시키지 못한다. 단, $y&amp;gt;=0$이라고 제한한다면 함수라고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-일대일-대응함수&quot;&gt;2. 일대일 대응함수&lt;/h2&gt;
&lt;h3 id=&quot;조건&quot;&gt;조건&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;one-to-one : y축과 평행한 선을 그어 한 점과만 만나야 한다.&lt;/li&gt;
  &lt;li&gt;onto : 치역과 공역이 같아야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;3번 : 일대일 대응이 아니다. y축과 평행한 선을 그어보면 여러 점에서 만난다. 즉, 1번 조건을 만족시키지 못한다.&lt;/li&gt;
  &lt;li&gt;1번 : $y&amp;gt;0$에서만 정의된다. 즉, onto 조건을 만족시키지 못한다. 단, $y&amp;gt;0$이라는 조건이 있다면 onto 조건을 만족시킨다.&lt;/li&gt;
  &lt;li&gt;4번 : 일대일 대응 조건을 만족&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;참고&quot;&gt;참고&lt;/h3&gt;
&lt;p&gt;x값을 제한하면 one-to-one 조건을 만족할 수 있도록 만들 수 있으며, y값을 제한하면 onto 조건을 만족할 수 있도록 만들 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;일대일-대응의-의미&quot;&gt;일대일 대응의 의미&lt;/h3&gt;
&lt;p&gt;$y=f(x)$가 일대일 대응이다 $\Leftrightarrow$ 역함수 존재&lt;/p&gt;

&lt;h2 id=&quot;3-역함수와-도함수&quot;&gt;3. 역함수와 도함수&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$y=f(x)$의 역함수 : $x=f(y) \Leftrightarrow y=f^{-1}(x)$&lt;/li&gt;
  &lt;li&gt;X가 치역, Y가 정의역이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---yex&quot;&gt;예제 - $y=e^x$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $-\infty &amp;lt; x &amp;lt; \infty, y&amp;gt;0$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=e^y\Leftrightarrow y=lnx$  ($-\infty &amp;lt; y &amp;lt; \infty, x&amp;gt;0$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=e^x$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=lnx \Rightarrow y={1 \over x}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---ysinx&quot;&gt;예제 - $y=sinx$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $-1 \le y \le 1, -{\pi \over 2} \le x \le {\pi \over 2}$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=siny \Leftrightarrow y=arcsinx$  ($-1 \le x \le 1, -{\pi \over 2} \le y \le {\pi \over 2}$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=cosx$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=arcsinx \Rightarrow y’={1 \over \sqrt{1-x^2}}$&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371685-3f4e7100-88cd-11ea-9f23-a9d189454215.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$y=arcsinx \Rightarrow y’={1 \over \sqrt{1-x^2}}$&lt;br /&gt;
이 식은 어떻게 도출된 것일까?&lt;br /&gt;
$y=arcsinx\Leftrightarrow x=siny$
$\Rightarrow 1=cosy \cdot y’$(양 변을 $x$로 미분)&lt;br /&gt;
$\Rightarrow y’={1\over cosy}$&lt;br /&gt;
$={1\over \sqrt{1-sin^2y}}$($\because sin^2x+cos^2x=1$)&lt;br /&gt;
$={1\over \sqrt{1-x^2}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---ycosx&quot;&gt;예제 - $y=cosx$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $0\le x\le \pi, -1 \le y \le 1$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=cosy \Leftrightarrow y=arccosx$  ($0\le y\le \pi, -1 \le x \le 1$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=-sinx$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=arccosx \Rightarrow y’=-{1 \over \sqrt{1-x^2}}$&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371674-3c538080-88cd-11ea-9101-e466cda44a4c.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$y=arccosx \Rightarrow y’=-{1 \over \sqrt{1-x^2}}$&lt;br /&gt;
이 식은 어떻게 도출된 것일까?&lt;br /&gt;
$y=arccosx\Leftrightarrow x=cosy$
$\Rightarrow 1=-siny \cdot y’$(양 변을 $x$로 미분)&lt;br /&gt;
$\Rightarrow y’=-{1\over siny}$&lt;br /&gt;
$=-{1\over \sqrt{1-cos^2y}}$($\because sin^2x+cos^2x=1$)&lt;br /&gt;
$=-{1\over \sqrt{1-x^2}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예제---ytanx&quot;&gt;예제 - $y=tanx$&lt;/h3&gt;
&lt;p&gt;일대일 대응을 위해 $-{1\over 2}\pi \le x\le {1\over 2}\pi$ 조건을 붙임.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;역함수 : $x=tany \Leftrightarrow y=arctanx$  ($-{1\over 2}\pi \le y\le {1\over 2}\pi$)&lt;/li&gt;
  &lt;li&gt;도함수 : $y’=sec^2x$&lt;/li&gt;
  &lt;li&gt;역함수의 도함수 : $y=arctanx \Rightarrow y’={1 \over 1+x^2}$&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80371671-3b225380-88cd-11ea-87c3-72991eb83b3f.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$y=arctanx \Rightarrow y’={1 \over 1+x^2}$&lt;br /&gt;
이 식은 어떻게 도출된 것일까?&lt;br /&gt;
$y=arctanx \Leftrightarrow x=tany$
$\Rightarrow 1=sec^2y \cdot y’$(양 변을 $x$로 미분)&lt;br /&gt;
$\Rightarrow y’={1\over sec^2y}$&lt;br /&gt;
$={1\over \sqrt{1+tan^2y}}$($\because sec^2x-tan^2x=1$)&lt;br /&gt;
$={1\over \sqrt{1+x^2}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;예제---역함수와-도함수&quot;&gt;예제 - 역함수와 도함수&lt;/h2&gt;

&lt;h2 id=&quot;4-쌍곡선함수와-그-도함수&quot;&gt;4. 쌍곡선함수와 그 도함수&lt;/h2&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="미분적분학" /><summary type="html">1. 함수의 정의 함수의 정의 $f:X\to Y$ X라는 집합에 있는 원소를 어떻게 Y로 보내느냐, 그 규칙을 결정하는 것이 함수 X를 정의역, Y를 공역이라 한다. 즉, $x \to f(x)=y$</summary></entry><entry><title type="html">확률 및 통계 6강</title><link href="http://localhost:4000/lecture/2020/04/14/prob_stat_06" rel="alternate" type="text/html" title="확률 및 통계 6강" /><published>2020-04-14T00:00:00-07:00</published><updated>2020-04-14T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/14/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%206%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/14/prob_stat_06">&lt;h2 id=&quot;예시---geometric-distribution기하분포&quot;&gt;예시 - Geometric Distribution(기하분포)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;RV K: Number of trials until first success&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예시---tossing-a-dice-until-first-6&quot;&gt;예시 - Tossing a dice until first 6&lt;/h3&gt;
&lt;p&gt;p: 6이 나올 확률&lt;br /&gt;
(1-p): 6 이외의 수가 나올 확률&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;첫 번째 시도에서 성공&lt;br /&gt;
$k=1, p$&lt;/li&gt;
  &lt;li&gt;두 번째 시도에서 성공&lt;br /&gt;
$k=2, (1-p)p$&lt;/li&gt;
  &lt;li&gt;세 번째 시도에서 성공&lt;br /&gt;
$k=3, (1-p)(1-p)p$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;즉, $P_K(k)=(1-p)^{k-1}p\quad(k=1,2,3,4,…)$&lt;/p&gt;

&lt;h3 id=&quot;mean&quot;&gt;Mean&lt;/h3&gt;
&lt;p&gt;$E[K]=\sum_{k=1}^{\infty}kp(1-p)^{k-1}$&lt;br /&gt;
$\sum_{k=1}^{\infty}(1-p)^{k}={1-p \over p}$&lt;br /&gt;
${d \over dp}\sum_{k=1}^{\infty}(1-p)^{k}={d \over dp}({1-p \over p})$&lt;br /&gt;
$=-\sum_{k=1}^{\infty}k(1-p)^{k-1}=-{1 \over p^2}$&lt;br /&gt;
$=\sum_{k=1}^{\infty}k(1-p)^{k-1}={1 \over p^2}$&lt;br /&gt;
$\therefore E[K]=p{1 \over p^2}={1 \over p}$&lt;/p&gt;

&lt;h3 id=&quot;variance&quot;&gt;Variance&lt;/h3&gt;
&lt;p&gt;$\sigma_k^2=E[K^2]-\mu^2$&lt;br /&gt;
$E[K^2]=\sum_{k=1}^{\infty}k^2p(1-p)^{k-1}$&lt;br /&gt;
$\Rightarrow -{d \over dp}\sum_{k=1}^{\infty}k(1-p)^{k-1}=-{d \over dp}{1 \over p^2}$(Mean 구하는 공식에서 가져옴)&lt;br /&gt;
$\qquad=-\sum_{k=1}^{\infty}k(k-1)(1-p)^{k-2}=-{2\over p^3}$&lt;br /&gt;
$\qquad=\sum_{k=1}^{\infty}k(k-1)(1-p)^{k-2}={2\over p^3}$&lt;br /&gt;
$\Rightarrow \sum_{k=1}^{\infty}k^2(1-p)^{k-2}=\sum_{k=1}^{\infty}k(1-p)^{k-2}+{2\over p^3}$&lt;br /&gt;
$\qquad =\sum_{k=1}^{\infty}k^2(1-p)^{k-2}p(1-p)=(\sum_{k=1}^{\infty}k(1-p)^{k-2}+{2\over p^3})p(1-p)$&lt;br /&gt;
$\qquad =\sum_{k=1}^{\infty}k(1-p)^{k-1}p+{2(1-p)\over p^2}$&lt;br /&gt;
$\qquad ={1\over p}+{2(1-p)\over p^2}$&lt;br /&gt;
$\therefore \sigma_k^2=E[K^2]-\mu^2={1\over p}+{2(1-p)\over p^2}-{1\over p^2}={1-p \over p^2}$&lt;/p&gt;

&lt;h2 id=&quot;conditional-mean&quot;&gt;Conditional Mean&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$E[X|A]=\sum_{x_i\in A}x_ip(x_i|A)$&lt;/li&gt;
  &lt;li&gt;A의 조건을 만족하는 변수값들에 대해서만 평균값을 구한 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;이산확률변수의 경우&lt;/strong&gt;&lt;br /&gt;
$\sum_{x_i\in A}x_i{p(x_i \cap A) \over p(A)}$&lt;br /&gt;
$\qquad =\sum_{x_i\in A}x_i{p(x_i) \over p(A)}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;연속확률변수의 경우&lt;/strong&gt;&lt;br /&gt;
$E[X|A]=\int_{x\in A}xf_X(x|A)$
연속형이기 때문에 density를 구해줘야 하는데, 이 경우에는 conditional density($f_X(x|A)$)를 구해야 한다.&lt;br /&gt;
이것을 바로 구할 수는 없고, conditional CDF를 활용해 구한다.&lt;br /&gt;
$f_X(x|A)={d \over dx}F_X(x|A)$&lt;br /&gt;
$\qquad\qquad ={d \over dx}P(X\le x|A)$&lt;br /&gt;
$\qquad\qquad ={d \over dx}{P(X\le x\cap A)\over P(A)}$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">예시 - Geometric Distribution(기하분포) RV K: Number of trials until first success</summary></entry><entry><title type="html">Taylor Series</title><link href="http://localhost:4000/posts/2020/04/13/taylor_series" rel="alternate" type="text/html" title="Taylor Series" /><published>2020-04-13T00:00:00-07:00</published><updated>2020-04-13T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/04/13/taylor_series</id><content type="html" xml:base="http://localhost:4000/posts/2020/04/13/taylor_series">&lt;h2 id=&quot;테일러-급수&quot;&gt;테일러 급수&lt;/h2&gt;
&lt;p&gt;테일러 급수(Taylor Series) 또는 테일러 전개(Taylor Expansion)은 어떤 함수 $f(x)$를 우리가 다루기 쉬운 다항함수 형태로 바꾸어 준다.&lt;br /&gt;
  &lt;br /&gt;
$f(x)=p_{\infty}(x)$  &lt;br /&gt;
$p_n(x)=f(a)+f’(a)(x-a)+{f''(a)\over 2!}(x-a)^2+…+{f^{(n)}(a)\over n!}(x-a)^n$  &lt;br /&gt;
$\qquad\quad=\Sigma_{k=0}^{n}{f^{(k)}(a)\over k!}(x-a)^k$  &lt;br /&gt;
  &lt;br /&gt;
근사다항식의 차수가 높으면 높을수록 $p_n(x)$는 $f(x)$를 잘 근사하게 된다.  &lt;br /&gt;
  &lt;br /&gt;
주의해야 할 사항은 모든 $x$에 대해서 잘 근사하는 것이 아니라 $x=a$ 근처에서만 잘 근사한다는 것이다. 즉, $x$가 $a$로부터 멀어지면 멀어질수록 $f(x)=p(x)$는 큰 오차를 갖게 된다.  &lt;br /&gt;
  &lt;br /&gt;
그렇다면 위에서 봤던 $p_n(x)$는 어떻게 구한 것일까  &lt;br /&gt;
  &lt;br /&gt;
테일러 급수는 미적분의 기본 정리로부터 출발한다.  &lt;br /&gt;
  &lt;br /&gt;
$\int_a^xf’(t)dt=f(x)-f(a)$  &lt;br /&gt;
$f(x)=f(a)+\int_a^xf’(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
이제 우변의 마지막 항인 $\int_a^xf’(t)dt$를 부분적분을 활용해 바꿔줄 것이다.  &lt;br /&gt;
  &lt;br /&gt;
$\int_a^xf’(t)dt=\int_a^x1\cdot f’(t)dt$  &lt;br /&gt;
$\qquad\qquad =[(t+c)f’(t)dt]_a^x-\int_a^x(t+c)f''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
여기서 $c$는 적분상수이기 때문에 우리가 원하는 값을 넣을 수 있다. $c$에 $-x$를 대입한다.  &lt;/p&gt;

&lt;p&gt;$\qquad\qquad =[(t-x)f’(t)dt]_a^x-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
$\qquad\qquad =(x-x)f’(x)-(a-x)f’(a)-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
$\qquad\qquad =(x-a)f’(a)-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
정리하면 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_1(x)=f(a)+(x-a)f’(a)-\int_a^x(t-x)f''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
마찬가지로 마지막 항인 $-\int_a^x(t-x)f''(t)dt$를 부분적분을 활용해 바꿔줄 것이다.  &lt;br /&gt;
$-\int_a^x(t-x)f''(t)dt=-([{(t-x)^2\over 2}f''(t)]_a^x-\int_a^x{(t-x)^2\over 2}f'''(t)dt)$  &lt;br /&gt;
$\qquad\qquad\qquad\qquad ={(x-a)^2 \over 2}f''(a)+\int_a^x{(t-x)^2\over 2}f'''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
정리하면 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_2(x)=f(a)+(x-a)f’(a)+{(x-a)^2 \over 2}f''(a)+\int_a^x{(t-x)^2\over 2}f'''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
마찬가지로 마지막 항인 $\int_a^x{(t-x)^2\over 2}f'''(t)dt$를 부분적분을 활용해 바꿔줄 것이다.  &lt;br /&gt;
  &lt;br /&gt;
$\int_a^x{(t-x)^2\over 2}f'''(t)dt=[{(t-x)^3\over 2\cdot 3}f'''(t)]_a^x-\int_a^x{(t-x)^3\over 2\cdot 3}f''''(t)dt$  &lt;br /&gt;
$\qquad\qquad\qquad\quad ={(x-a)^3 \over 2\cdot 3}f'''(a)-\int_a^x{(t-x)^2\over 2\cdot 3}f''''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
정리하면 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_3(x)=f(a)+(x-a)f’(a)+{(x-a)^2 \over 2}f''(a)+{(x-a)^3 \over 2\cdot 3}f'''(a)-\int_a^x{(t-x)^2\over 2\cdot 3}f''''(t)dt$  &lt;br /&gt;
  &lt;br /&gt;
이 정도 해보면 패턴이 눈에 보일 것이다. $p_n(x)$는 다음과 같다.  &lt;br /&gt;
  &lt;br /&gt;
$p_n(x)=f(a)+f’(a)(x-a)+{f''(a)\over 2!}(x-a)^2+…+{f^{(n)}(a)\over n!}(x-a)^n$  &lt;br /&gt;
$\qquad\quad=\Sigma_{k=0}^{n}{f^{(k)}(a)\over k!}(x-a)^k$  &lt;/p&gt;

&lt;p&gt;참고로 테일러 급수는 보통 $a$에 0을 넣어서 사용하는데, 이를 매클로린 급수(Maclaurin Series)라고 한다.  &lt;/p&gt;

&lt;p&gt;또한, 일반적으로 테일러 급수는 1차 또는 2차까지만 하는 경우가 많다. 1차, 2차 다항함수로 근사할 경우에는  &lt;br /&gt;
  &lt;br /&gt;
$f(x)\approx p_1=f(a)+(x-a)f’(a)+Q_2(x)$  &lt;br /&gt;
$f(x)\approx p_2=f(a)+(x-a)f’(a)+{(x-a)^2 \over 2}f''(a)+Q_3(x)$  &lt;br /&gt;
  &lt;br /&gt;
와 같이 놓고, $Q(x)$를 0으로 간주해 무시한다. 이 경우, $f(x)$를 무한차수 다항함수로 근사하는 것 보다는 근사오차가 크지만, $x$가 충분히 $a$에 가까운 경우에는 근사오차가 거의 없다.&lt;/p&gt;

&lt;h2 id=&quot;테일러급수가필요한이유&quot;&gt;테일러 급수가 필요한 이유&lt;/h2&gt;

&lt;p&gt;1. 잘 모르거나 복잡한 함수를 다루기 쉽고 이해하기 쉬운 다항함수로 대체&lt;br /&gt;
2. 함수의 특성을 분석하기 용이&lt;/p&gt;

&lt;h2 id=&quot;예시---e&quot;&gt;예시 - $e$&lt;/h2&gt;
&lt;p&gt;$e^x = 1 + {x \over 1!} + {x^2 \over 2!} + {x^3 \over 3!} + …$&lt;br /&gt;
$\quad =\sum_{k=0}^{\infty}{x^k \over k!}$&lt;/p&gt;

&lt;h2 id=&quot;정리&quot;&gt;정리&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;매클로린 급수&lt;br /&gt;
$f(x)=\sum_{k=0}^{\infty}{f^{(k)}(0) \over k!}x^k$&lt;/li&gt;
  &lt;li&gt;테일러 급수&lt;br /&gt;
$f(x-a)=\sum_{k=0}^{\infty}{f^{(k)}(a) \over k!}(x-a)^k$&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">테일러 급수 테일러 급수(Taylor Series) 또는 테일러 전개(Taylor Expansion)은 어떤 함수 $f(x)$를 우리가 다루기 쉬운 다항함수 형태로 바꾸어 준다.    $f(x)=p_{\infty}(x)$   $p_n(x)=f(a)+f’(a)(x-a)+{f''(a)\over 2!}(x-a)^2+…+{f^{(n)}(a)\over n!}(x-a)^n$   $\qquad\quad=\Sigma_{k=0}^{n}{f^{(k)}(a)\over k!}(x-a)^k$      근사다항식의 차수가 높으면 높을수록 $p_n(x)$는 $f(x)$를 잘 근사하게 된다.      주의해야 할 사항은 모든 $x$에 대해서 잘 근사하는 것이 아니라 $x=a$ 근처에서만 잘 근사한다는 것이다. 즉, $x$가 $a$로부터 멀어지면 멀어질수록 $f(x)=p(x)$는 큰 오차를 갖게 된다.      그렇다면 위에서 봤던 $p_n(x)$는 어떻게 구한 것일까      테일러 급수는 미적분의 기본 정리로부터 출발한다.      $\int_a^xf’(t)dt=f(x)-f(a)$   $f(x)=f(a)+\int_a^xf’(t)dt$      이제 우변의 마지막 항인 $\int_a^xf’(t)dt$를 부분적분을 활용해 바꿔줄 것이다.      $\int_a^xf’(t)dt=\int_a^x1\cdot f’(t)dt$   $\qquad\qquad =[(t+c)f’(t)dt]_a^x-\int_a^x(t+c)f''(t)dt$      여기서 $c$는 적분상수이기 때문에 우리가 원하는 값을 넣을 수 있다. $c$에 $-x$를 대입한다.  </summary></entry><entry><title type="html">확률 및 통계 5강</title><link href="http://localhost:4000/lecture/2020/04/13/prob_stat_05" rel="alternate" type="text/html" title="확률 및 통계 5강" /><published>2020-04-13T00:00:00-07:00</published><updated>2020-04-13T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/13/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%205%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/13/prob_stat_05">&lt;h2 id=&quot;32-expectation&quot;&gt;3.2 Expectation&lt;/h2&gt;
&lt;p&gt;$E[X]=\sum_{i}x_iP(x_i)=\int_{-\infty}^{\infty}xf_X(x)dx$&lt;/p&gt;

&lt;h3 id=&quot;예시---poisson-distribution포아송-분포&quot;&gt;예시 - Poisson Distribution(포아송 분포)&lt;/h3&gt;
&lt;p&gt;Discrete Random Variable $P_K(k)={\lambda^k \over k!}e^{-\lambda}$(k = 0, 1, 2, 3, …)&lt;/p&gt;

&lt;p&gt;이렇게 생긴 것을 Poisson Distribution이라고 한다.&lt;/p&gt;

&lt;p&gt;언제 쓸까?&lt;br /&gt;
은행이나 고객들이 방문한다고 하자. 이 때 단위 시간(time interval) 동안 몇 명의 고객이 방문했는지를 가지고 그 횟수를 확률 변수로 정의한 것이다.&lt;/p&gt;

&lt;p&gt;만약 은행에 한 시간에 평균 10명의 고객이 방문하는데 직원 한 명이 한 시간동안 처리할 수 있는 고객의 수가 5명이라면 두 명의 직원이 고객 응대 일을 해야 한다.&lt;/p&gt;

&lt;p&gt;$E[K]=\sum_{k=0}^{\infty}k{\lambda^k \over k!}e^{-\lambda}$&lt;br /&gt;
$=\sum_{k=1}^{\infty}k{\lambda^k \over k!}e^{-\lambda}$&lt;br /&gt;
$=\sum_{k=1}^{\infty}{\lambda^k \over (k-1)!}e^{-\lambda}$&lt;br /&gt;
$=\sum_{k=1}^{\infty}{\lambda \lambda^{k-1} \over (k-1)!}e^{-\lambda}$&lt;br /&gt;
$=\lambda e^{-\lambda} \sum_{k=1}^{\infty}{\lambda^{k-1} \over (k-1)!}$&lt;br /&gt;
$=\lambda e^{-\lambda} \sum_{k=0}^{\infty}{\lambda^k \over k!}$&lt;br /&gt;
$=\lambda$ ($\because \sum_{k=0}^{\infty}{\lambda^k \over k!}=e^\lambda$ by &lt;a href=&quot;https://blanik00.github.io/posts/taylor_series&quot;&gt;taylor series&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;즉, 단위 시간 동안 방문한 고객의 평균 수가 $\lambda$다. 그러므로 은행은 고객이 평균 몇 명 정도 방문하는지 경험적인 데이터로 $\lambda$를 구하고 포아송 분포로 모델링하면 된다. 예를 들어, k가 25 이상이 될 확률이 5% 미만이라면, 그 정도의 고객만 응대할 수 있도록 근무 시간표를 조절하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;예시---continuous-rv&quot;&gt;예시 - Continuous RV&lt;/h3&gt;
&lt;p&gt;$f_X(x)=
\begin{cases}
\lambda e^{-\lambda x} \qquad x\ge 0 \\
0 \qquad \qquad x&amp;lt;0
\end{cases}$&lt;/p&gt;

&lt;p&gt;이러한 것을 Exponential Distribution이라고 한다. 이 분포는 시스템의 Lifetime과 관련이 많다. 어느 시스템이 어느 순간에서도 잘 동작하게 될 확률과 관련된 것이다. 시간이 오래될수록 생존 확률이 낮아진다. 즉, 지수 분포로 시스템을 설계하게 되면 “이 시스템은 95%의 Reliability를 가지고 3년 동안은 잘 동작할 것이다”와 같은 분석을 할 수 있게 된다.&lt;/p&gt;

&lt;p&gt;$E[X]=\int_{-\infty}^{\infty}xf_X(x)dx$&lt;br /&gt;
$=\int_{0}^{\infty}x\lambda e^{-\lambda x}dx$&lt;br /&gt;
$={1 \over \lambda}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고&lt;/strong&gt;&lt;br /&gt;
예시 3.3, 3.4에서 본 것들은 $\lambda$를 알면 확률분포를 알 수 있다. 이런 것을 Parametric probability density estimation이라고 한다. 앞으로 배울 분포들은 파라미터를 알면 그 분포를 알 수 있는 것과 알 수 없는 것으로 나뉜다.&lt;/p&gt;

&lt;h2 id=&quot;34-moments-of-rv&quot;&gt;3.4 Moments of RV&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;nth-order Moments&lt;br /&gt;
$E[X^n]=\sum_{i}x_i^nP(x_i)=\int_{-\infty}^{\infty}x^nf_X(x)dx$&lt;br /&gt;
$n = 1$이라면 $E[X]$는 mean($\mu$)과 같다.&lt;/li&gt;
  &lt;li&gt;Central Moments&lt;br /&gt;
$E[(X-\mu)^n]=\sum_{i}(x_i-\mu)^nP(x_i)=\int_{-\infty}^{\infty}(x-\mu)^nf_X(x)dx$
    &lt;ul&gt;
      &lt;li&gt;$n = 1$&lt;br /&gt;
$E[(X-\mu)]=\int_{-\infty}^{\infty}(x-\mu)f_X(x)dx=\int_{-\infty}^{\infty}xf_X(x)dx+\mu\int_{-\infty}^{\infty}f_X(x)dx=\mu-\mu\times 1=0$&lt;br /&gt;
평균을 중심으로 각각의 RV의 차이값을 평균내면 당연히 0이 나온다. 이 예시는 Continous한 경우지만, Discrete한 경우에도 적용된다.&lt;/li&gt;
      &lt;li&gt;$n = 2$&lt;br /&gt;
$E[(X-\lambda)^2]$. 이는 분산이라고 한다.($=\sigma_X^2$) 우리가 흔히 아는 표준편차의 제곱이다. 분산은 0 이상의 값을 가지며, 모든 값이 같을 때 0의 분산을 가진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;예시&lt;/strong&gt;&lt;br /&gt;
RV X를 가지고 새로운 함수 $g_1(X), g_2(X)$를 만들었다고 하자. 그렇다면 $E[ag_1(X)+bg_2(X)]$는 어떻게 구할까?&lt;/p&gt;

&lt;p&gt;$E[ag_1(X)+bg_2(X)]$&lt;br /&gt;
$=\int (ag_1(x)+bg_2(x))f_X(x)dx$&lt;br /&gt;
$=a\int g_1(x)f_X(x)dx+b\int g_2(x)f_X(x)dx$&lt;br /&gt;
$=aE[g_1(x)]+bE[g_2(x)]$&lt;/p&gt;

&lt;p&gt;이런 것을 선형성(Linearity)을 가지고 있다고 한다.&lt;/p&gt;

&lt;p&gt;아래는 Expectation의 선형성을 활용해 풀 수 있다.&lt;br /&gt;
$\sigma_X^{2}=E[(X-\mu)^2]$&lt;br /&gt;
$=E[X^2-2\mu X+\mu^2]$&lt;br /&gt;
$=E[X^2]-2\mu E[X]+\mu^2$&lt;br /&gt;
$=E[X^2]-\mu^2\quad(\because E[X]=\mu)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예시 - 포아송 분포의 분산&lt;/strong&gt;&lt;br /&gt;
$P_K(k)={\lambda^k \over k!}e^{-\lambda}$&lt;br /&gt;
$E[X]=\lambda$&lt;/p&gt;

&lt;p&gt;$E[K^2]=\sum_{k=0}^{\infty}k^2{\lambda^k \over k!}e^{-\lambda}$&lt;br /&gt;
$=e^{-\lambda}\sum_{k=1}^{\infty}k^2{\lambda^k \over k!}$&lt;br /&gt;
$=e^{-\lambda}\sum_{k=1}^{\infty}{k\lambda^k \over (k-1)!}$&lt;br /&gt;
$=e^{-\lambda}\sum_{k=1}^{\infty}{(k-1+1)\lambda^k \over (k-1)!}$&lt;br /&gt;
$=e^{-\lambda}\sum_{k=1}^{\infty}{(k-1)\lambda^k \over (k-1)!}+e^{-\lambda}\sum_{k=1}^{\infty}{1\lambda^k \over (k-1)!}$&lt;br /&gt;
$=e^{-\lambda}\sum_{k=2}^{\infty}{\lambda^k \over (k-2)!}+e^{-\lambda}\sum_{k=1}^{\infty}{\lambda^k \over (k-1)!}$&lt;br /&gt;
$=e^{-\lambda}\sum_{k=2}^{\infty}{\lambda^2 \lambda^{k-2} \over (k-2)!}+e^{-\lambda}\sum_{k=1}^{\infty}{\lambda \lambda^{k-1} \over (k-1)!}$&lt;br /&gt;
$=\lambda^2 e^{-\lambda}\sum_{k=2}^{\infty}{ \lambda^{k-2} \over (k-2)!}+\lambda e^{-\lambda}\sum_{k=1}^{\infty}{\lambda^{k-1} \over (k-1)!}$&lt;br /&gt;
$=\lambda^2 e^{-\lambda}\sum_{k=0}^{\infty}{ \lambda^{k} \over k!}+\lambda e^{-\lambda}\sum_{k=0}^{\infty}{\lambda^{k} \over k!}$&lt;br /&gt;
$=\lambda^2+\lambda$  ($\because \sum_{k=0}^{\infty}{\lambda^k \over k!}=e^\lambda$ by &lt;a href=&quot;https://blanik00.github.io/posts/taylor_series&quot;&gt;taylor series&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;$\sigma_K^2=E[K^2]-\mu^2=\mu$&lt;/p&gt;

&lt;p&gt;즉, 평균과 분산이 $\mu$로 같다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">3.2 Expectation $E[X]=\sum_{i}x_iP(x_i)=\int_{-\infty}^{\infty}xf_X(x)dx$</summary></entry><entry><title type="html">확률 및 통계 4강</title><link href="http://localhost:4000/lecture/2020/04/12/prob_stat_04" rel="alternate" type="text/html" title="확률 및 통계 4강" /><published>2020-04-12T00:00:00-07:00</published><updated>2020-04-12T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/12/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%204%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/12/prob_stat_04">&lt;h2 id=&quot;26continousrandomvariable&quot;&gt;2.6 Continous Random Variable&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;outcome들을 특정 구간 안에 있는 모든 real number에 대응시키는 것이다. 즉, outcome은 무한하며, 무한한 outcome을 만들기 위해 Sample Space도 무한해야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbpbeXd%2FbtqCZZSh9c5%2Fej5eae49ORaKE0Zegwiri1%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Continous Random Variable이 [0, 1] 사이에 존재하며, 이 중에서 0.5를 뽑아낸다고 하자. 분자에는 0.5 하나가 들어가고, 분모에는 무한히 많은 숫자가 들어간다. 즉, $P(0.5)={1 \over \infty}=0$이다. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fb7dWmo%2FbtqC3qVnXVz%2FZsb4yZ48Zhl5kZc1uf0hfK%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것은 Continous Random Variable에서 특정 값에서의 확률($P(X=x_i)$)을 정의하기가 힘들다는 것을 보여준다. 그렇기 때문에 새로운 방법으로 Continous Random Variable라는 확률 변수에 대한 확률 분포를 나타내는 함수를 정의해줘야 한다. 그래서 누적분포함수(CDF)를 활용한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;for Continous RVs, $P(x)\to 0$. It is not possible to define a probability value for each $x$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;아주 작은 구간의 확률을 구하는 문제로 바꿔보자. 즉, $\lim_{\Delta x \to 0}P(x&amp;lt;X\le x+\Delta x)=F_X(x+\Delta x)-F_X(x)$를 구하면 된다. 하지만 이것도 $\Delta x$가 0으로 수렴 할수록 0으로 수렴할 것이다. 0으로 수렴하는 것을 방지하기 위해 $\Delta x$로 나눠준다. 즉, $\lim_{\Delta x \to 0}{P(x&amp;lt;X\le x+\Delta x)\over \Delta x}=\lim_{\Delta x \to 0}{F_X(x+\Delta x)-F_X(x) \over \Delta x}$로 하면 결과가 특정 값으로 수렴해간다.&lt;/p&gt;

&lt;p&gt;이를 $f_X(x)=\lim_{\Delta x \to 0}{F_X(x+\Delta x)-F_X(x) \over \Delta x}=F_X’(x)$로 표현한다. $\lim_{\Delta x \to 0}{P(x&amp;lt;X\le x+\Delta x)\over \Delta x}$의 물리적 의미를 생각해보자. 분자는 확률(probability)이며, 분모는 단위 길이(density)다. 그래서 $f(x)$를 Probability Density Function(PDF)이라고 부른다. 이것으로 우리는 Continous RV의 확률분포를 정의해볼 수 있다.&lt;/p&gt;

&lt;p&gt;정리하자면 $f_X(x)=F’_X(x)$이며, 역으로 $F_X(x)=\int_{-\infty}^xf(\tilde{x})d\tilde{x}=P(X \le x)$이다.($\tilde{x}$는 $\int_{-\infty}^x$의 $x$와 구분하기 위해 ~를 붙인 것이다.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PDF의 성질&lt;/strong&gt;&lt;br /&gt;
- CDF를 미분한 것이다. CDF가 non-decreasing 함수이므로, PDF $F’_X(x)=f_X(x)\ge 0$이다. 즉, non-negative이다.&lt;br /&gt;
- $f_X(x)$는 확률값이므로 $\int_{-\infty}^{\infty}f_X(x)dx=1$&lt;br /&gt;
- $P(a&amp;lt;X\le b)=F_X(b)-F_X(a)=\int_{-\infty}^{b}f_X(x)dx-\int_{-\infty}^{a}f_X(x)dx=\int_{a}^{b}f_X(x)dx$&lt;br /&gt;
- $P(X &amp;lt; a)=P(X\le a)$ ($\because P(X=a)\to 0$)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예제 1&lt;/strong&gt;&lt;br /&gt;
$f_X(x)=&lt;br /&gt;
\begin{cases}&lt;br /&gt;
A(2x-x^2) \qquad 0 &amp;lt; x &amp;lt; 2 \\&lt;br /&gt;
0 \qquad \qquad \qquad else&lt;br /&gt;
\end{cases}$&lt;/p&gt;

&lt;p&gt;(1) A = ?&lt;br /&gt;
PDF가 되기 위해서는 $\int_{-\infty}^{\infty}f_X(x)dx=1$을 만족해야 한다. (2번 성질)  
$\int_{0}^{2}A(2x-x^2)dx=1$을 풀면 $A={3 \over 4}$가 나온다.&lt;/p&gt;

&lt;p&gt;(2) $P(X&amp;gt;1) = ?$&lt;br /&gt;
$=\int_{1}^{\infty}f_X(x)dx$&lt;br /&gt;
$=\int_{1}^{2}f_X(x)dx$&lt;br /&gt;
$=\int_{1}^{2}{3 \over 4}(2x-x^2)dx$&lt;br /&gt;
$={1 \over 2}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예제 2 - Uniform Distribution&lt;/strong&gt;&lt;br /&gt;
$f_T(t)=&lt;br /&gt;
\begin{cases}&lt;br /&gt;
{1 \over 6} \qquad 2\le t\le 8 \\&lt;br /&gt;
0 \qquad  else&lt;br /&gt;
\end{cases}$&lt;/p&gt;

&lt;p&gt;구간 내의 확률값이 모두 동일하다.&lt;/p&gt;

&lt;p&gt;$F_T(t)$&lt;br /&gt;
$=\int_{-\infty}^{t}f_T(\tilde{t})d\tilde{t}$&lt;br /&gt;
$=\begin{cases}&lt;br /&gt;
0 \qquad \qquad \qquad \qquad \quad t&amp;lt;2 \\&lt;br /&gt;
\int_{2}^{t}{1\over 6}d\tilde{t}={1\over 6}(t-2) \qquad 2\le t\le 8 \\&lt;br /&gt;
1 \qquad  \qquad \qquad \qquad \quad t &amp;gt; 8 \\&lt;br /&gt;
\end{cases}$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">2.6 Continous Random Variable outcome들을 특정 구간 안에 있는 모든 real number에 대응시키는 것이다. 즉, outcome은 무한하며, 무한한 outcome을 만들기 위해 Sample Space도 무한해야 한다.</summary></entry><entry><title type="html">확률 및 통계 3강</title><link href="http://localhost:4000/lecture/2020/04/11/prob_stat_03" rel="alternate" type="text/html" title="확률 및 통계 3강" /><published>2020-04-11T00:00:00-07:00</published><updated>2020-04-11T00:00:00-07:00</updated><id>http://localhost:4000/lecture/2020/04/11/%ED%99%95%EB%A5%A0%20%EB%B0%8F%20%ED%86%B5%EA%B3%84%203%EA%B0%95</id><content type="html" xml:base="http://localhost:4000/lecture/2020/04/11/prob_stat_03">&lt;h1 id=&quot;chapter-2-random-variable&quot;&gt;Chapter 2 Random Variable&lt;/h1&gt;

&lt;h2 id=&quot;22-definition-of-random-variable&quot;&gt;2.2 Definition of Random Variable&lt;/h2&gt;
&lt;p&gt;Random Variable: Mapping each outcome of random experiment to a &lt;strong&gt;real numbers&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;예시---tossing-a-coin&quot;&gt;예시 - tossing a coin&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;X: H, T&lt;/li&gt;
  &lt;li&gt;H를 1로, T를 0으로 매핑한다면, 1, 0은 확률면수가 된다.&lt;/li&gt;
  &lt;li&gt;$P(H)=P(1)={1 \over 2}$&lt;/li&gt;
  &lt;li&gt;지금까지는 확률을 구할 때 P(A)와 같이 사건을 대입했지만 지금부터는 확률변수인 $X$를 넣어 P(X)로 할 수 있다.(함수처럼 다룰 수 있다)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;예시---tossing-two-coins&quot;&gt;예시 - tossing two coins&lt;/h3&gt;
&lt;p&gt;RV: number of heads&lt;br /&gt;
0 =&amp;gt; {TT}&lt;br /&gt;
1 =&amp;gt; {HT, TH}&lt;br /&gt;
2 =&amp;gt; {HH}&lt;br /&gt;
$P(0)=P(\{TT\})={1\over 4}$&lt;br /&gt;
$P(1)=P(\{HT, TH\})={1\over 2}$&lt;br /&gt;
$P(2)=P(\{HH\})={1\over 4}$&lt;br /&gt;
확률변수는 $X$, $Y$, $Z$와 같이 대문자를 써서 표현하며, 확률변수의 특정 값은 $x$, $y$, $z$와 같이 표현한다.&lt;/p&gt;

&lt;h2 id=&quot;23-event-defined-by-rv&quot;&gt;2.3 Event defined by RV&lt;/h2&gt;
&lt;p&gt;$A_x$를 outcome의 집합이라 하자. 그렇다면 $A_x=\{w|X(w)=x\}$처럼 표현할 수 있다. 동전 두 개 던지는 예를 다시 가져와보면 $A_0=\{TT\}$, $A_1=\{TH, HT\}$, $A_2=\{HH\}$이다. $A_x$의 확률은 $P(A_x)=P(X=x)$다. 이제 우리는 $P(A_x)$와 같은 표현 보다는 $P(X=x)$ 혹은 x에 값을 넣어서 $P(1)$과 같은 표현을 많이 쓸 것이다.&lt;/p&gt;

&lt;h2 id=&quot;24-distribution-functions&quot;&gt;2.4 Distribution Functions&lt;/h2&gt;
&lt;p&gt;RV를 X, RV의 값을 x라 하자. 이 때 Culmulative Distribution Function(CDF)는 $F_X(x)=P(X\le x)$이다. 예를 들어, $F_X(1)=P(X \le 1)$이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FclrmhK%2FbtqCZZ5g5qv%2FRKwS6KoxI8ApsiGdjEwbzk%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CDF는 다음과 같은 성질을 가진다.&lt;/p&gt;

&lt;p&gt;1. if $x_1 &amp;lt; x_2, F_X(x_1)\le F_X(x_2)$&lt;br /&gt;
2. $0\le F_X(x)\le1$&lt;br /&gt;
3. $F_X(\infty)=\lim_{n \to \infty}F_X(x)=1$&lt;br /&gt;
4. $F_X(-\infty)=\lim_{n \to -\infty}F_X(x)=0$&lt;br /&gt;
5. $P(a \le x \le b)=F_X(b)-F_X(a)$&lt;br /&gt;
6. $P(x&amp;gt;a)=1-F_X(a)$ (equality는 정의에 의해 $&amp;lt;$에만 붙여야 한다)&lt;/p&gt;

&lt;h3 id=&quot;예시--&quot;&gt;예시 -&lt;/h3&gt;
&lt;p&gt;$F_X(x)=&lt;br /&gt;
\begin{cases}&lt;br /&gt;
0 \quad\quad\quad\quad (x&amp;lt;0) \\&lt;br /&gt;
x+{1 \over 2}\quad\quad (0 \le x \le {1 \over 2}) \\&lt;br /&gt;
1 \quad \quad\quad\quad (x &amp;gt; {1 \over 2}) \\&lt;br /&gt;
\end{cases}$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2F0GXFp%2FbtqC1HiLH1H%2FPusSacFRIgsxPt1aqFKDk1%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$P(X&amp;gt;{1\over 4})=1-P(X \le {1\over 4})=1-F_X({1\over 4})={1\over 4}$&lt;/li&gt;
  &lt;li&gt;$P(X&amp;gt;0)=1-P(X \le 0)=1-F_X(0)=1-{1\over 2}={1\over 2}$&lt;/li&gt;
  &lt;li&gt;$P(X\ge0)=1$&lt;/li&gt;
  &lt;li&gt;$P(X=0)={1\over 2}$&lt;/li&gt;
  &lt;li&gt;$P(X\ge{1\over 4})=1-P(X&amp;lt;{1\over 4})=1-\lim_{x \to {1\over 4}}F_X(x)=1-{3\over 4}={1\over 4}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;25-discrete-rv&quot;&gt;2.5 Discrete RV&lt;/h2&gt;

&lt;p&gt;discrete: non-continuous(ex. integer)&lt;/p&gt;

&lt;p&gt;Probability Mass Function(PMF)&lt;/p&gt;

&lt;p&gt;$P_X(x)=P(X=x)$&lt;/p&gt;

&lt;p&gt;$F_X(x)=P(X \le x)=\sum_{x_i \le x}P_X(x_i)$&lt;/p&gt;

&lt;h3 id=&quot;예시---tossing2coins&quot;&gt;예시 - tossing 2 coins&lt;/h3&gt;
&lt;p&gt;RV: number of heads&lt;br /&gt;
0 =&amp;gt; $P_X(0)={1 \over 4}$&lt;br /&gt;
1 =&amp;gt; $P_X(1)={1 \over 2}$&lt;br /&gt;
2 =&amp;gt; $P_X(2)={1 \over 4}$&lt;/p&gt;

&lt;p&gt;$F_X(1)=P(X\le 1)=P(X=0)+P(X=1)={3 \over 4}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CDF와 PMF의 관계&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FdewxRf%2FbtqC4tDCyEB%2FzVAisoGsBCSksCi1tmi011%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CDF를 알면 PMF를 알 수 있고, PMF를 알면 CDF를 알 수 있다.&lt;/p&gt;

&lt;p&gt;PMF의 표현은 다음과 같다.&lt;/p&gt;

&lt;p&gt;$x$가 0이고, 높이가 1일 때 $\delta(x)$라고 한다. 그렇다면 $x$가 1이고, 높이가 ${1\over 2}$이라면 ${1\over 2}\delta(x-1)$이다. 또한 그렇다면 $x$가 -2이고, 높이가 ${3\over 2}$이라면 ${3\over 2}\delta(x+2)$이다.&lt;br /&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcCZ43H%2FbtqCYGLV43J%2FSQp2fOOEPSmtEaHmsfoz31%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 모든 것들을 하나의 함수처럼 쓰고 싶다면 $f(x)=\delta(x)+{1\over 2}\delta(x-1)+{3\over 2}\delta(x+2)$로 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;예시---tossing-two-coins-1&quot;&gt;예시 - tossing two coins&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FWOukZ%2FbtqCX9m9kdD%2FBpFaADCEEcCAtjl4w0Uvb0%2Fimg.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
$P(x)=&lt;br /&gt;
\begin{cases}  &lt;br /&gt;
{1\over 4} \quad\quad (x=0) \\  &lt;br /&gt;
{1\over 2} \quad\quad (x=1) \\  &lt;br /&gt;
{1\over 4} \quad \quad (x=2) \\  &lt;br /&gt;
\end{cases}$&lt;/p&gt;

&lt;p&gt;위 식 대신에 ${1\over 4}\delta(x)+{1\over 2}\delta(x-1)+{1\over 4}\delta(x-2)$로 표현할 수 있다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;lecture&quot;]" /><category term="확률 및 통계" /><summary type="html">Chapter 2 Random Variable</summary></entry></feed>