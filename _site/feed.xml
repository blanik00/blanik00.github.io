<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-18T06:10:39-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JeongUk Jang</title><subtitle>Github Pages template for academic personal websites, forked from mmistakes/minimal-mistakes</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><entry><title type="html">Random Variables</title><link href="http://localhost:4000/posts/2020/05/17/random_variables" rel="alternate" type="text/html" title="Random Variables" /><published>2020-05-17T00:00:00-07:00</published><updated>2020-05-17T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/17/random_variables</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/17/random_variables">&lt;iframe src=&quot;https://www.youtube.com/embed/GqDy0sInGJ0&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;random-variables&quot;&gt;Random Variables&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;표본 공간의 원소들(실험의 결과들)을 입력으로 받아 실수로 바꿔주는 함수&lt;/li&gt;
  &lt;li&gt;$Real \;Number=f(Elements \;of \;the \;sample \;space)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;random-variables---coin-example&quot;&gt;Random Variables - Coin Example&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;tossing 3 coins&lt;/li&gt;
  &lt;li&gt;Sample Space = {(HHH),(HHT),(HTH),(THH),(HTT),(THT),(TTH),(TTT)}&lt;/li&gt;
  &lt;li&gt;Random Variable Y : Number of heads&lt;/li&gt;
  &lt;li&gt;Y={0,1,2,3}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82135062-78796180-9839-11ea-8b4b-12e0951ffe58.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 일반적으로 사용하는 데이터는 다음과 같은 형태를 가지고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82135114-d4dc8100-9839-11ea-9fda-00abf4f23c29.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 확률 변수의 개념을 적용하면 “샘플들이 $X_1$이라는 확률 변수를 만나면 $x_{11},x_{21},…x_{n1}$이라는 실수로 바뀌고, $X_p$이라는 확률 변수를 만나면 $x_{1p},x_{2p},…x_{np}$이라는 실수로 바뀐다.”고 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;discrete--continous-random-variables&quot;&gt;Discrete / Continous Random Variables&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Random Variable의 결과는 실수인데, 실수는 두 가지 종류가 있다.&lt;/li&gt;
  &lt;li&gt;Discrete(이산형) : 셀 수 있다(동전을 세 개 던졌을 때 앞면이 나온 경우의 수, 코로나 바이러스 확진자 수)&lt;/li&gt;
  &lt;li&gt;Continous(연속형) : 셀 수 없다(시간과 돈이 대표적인 연속형 데이터. 서울 주민들의 평균 연봉)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-function&quot;&gt;Probability Function&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;확률 변수로부터 나온 실수를 입력으로 받아 확률로 바꿔주는 함수&lt;/li&gt;
  &lt;li&gt;$p=f(Real \; Number)$&lt;/li&gt;
  &lt;li&gt;Random Variables의 결과가 Discrete, Continous 두 가지가 있다고 했다. 이것인 즉슨 Probability Function의 입력도 Discrete, Continous 두 가지가 있다는 것이다. 이 때문에 Probability Function은 입력의 종류에 따라 두 가지로 나뉘게 된다.&lt;/li&gt;
  &lt;li&gt;입력이 Discrete한 경우 : Probability Mass Function(PMF, 확률질량함수)&lt;/li&gt;
  &lt;li&gt;입력이 Continous한 경우 : Probability Density Function(PDF, 확률밀도함수)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-mass-function&quot;&gt;Probability Mass Function&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Discrete Random Variable $X$을 입력으로 받음&lt;/li&gt;
  &lt;li&gt;$x$를 $X$의 원소라고 하자.&lt;/li&gt;
  &lt;li&gt;이산형 실수인 $x$에 대해 확률이 부여됨&lt;/li&gt;
  &lt;li&gt;$p(x)=P[X=x]$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82135065-7adbbb80-9839-11ea-9f09-7649b020585a.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;X축에 있는 것은 $X$라는 확률 변수가 가질 수 있는 이산형 값들이 되며, 거기에 해당하는 확률이며, 이를 막대 그래프로 표현한 것이다.&lt;/li&gt;
  &lt;li&gt;$0\le p(x) \le 1$   for all $x$&lt;/li&gt;
  &lt;li&gt;$\sum_xp(x)=1$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-mass-function---coin-example&quot;&gt;Probability Mass Function - Coin Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;tossing 2 coins&lt;/li&gt;
  &lt;li&gt;Sample Space S={(HH), (HT), (TH), (TT)}&lt;/li&gt;
  &lt;li&gt;Random Variable X = Number of heads&lt;/li&gt;
  &lt;li&gt;Possible values of X = {0, 1, 2}&lt;/li&gt;
  &lt;li&gt;What is probability mass function?&lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;$P(X=0)=1/4$&lt;/li&gt;
      &lt;li&gt;$P(X=1)=1/2$&lt;/li&gt;
      &lt;li&gt;$P(X=2)=1/4$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probability-mass-function---accident-example&quot;&gt;Probability Mass Function - Accident Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;한 공장에서는 매년 낙상 사고가 일어난다.&lt;/li&gt;
  &lt;li&gt;Random Variable X : 일 년 동안 일어나는 낙상 사고의 수&lt;/li&gt;
  &lt;li&gt;$X=0,1,2,3,…$&lt;/li&gt;
  &lt;li&gt;$P(X=x)={1\over 2^{x+1}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 때, $P(X=x)$는 Probability Mass Function이라고 할 수 있는가?&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;확률 변수 X는 셀 수 있으므로 이산형이 맞다.&lt;/li&gt;
  &lt;li&gt;$0\le p(x)\le 1$&lt;br /&gt;
$P(X=0)=1/2$&lt;br /&gt;
$P(X=1)=1/2^2$&lt;br /&gt;
$P(X=2)=1/2^3$&lt;br /&gt;
…이므로 모든 확률은 0과 1 사이에 있다.&lt;/li&gt;
  &lt;li&gt;$\sum_{x}P(X=x)=1$
$1/2+1/2^2+1/2^3+…={1/2 \over 1-{1/2}}=1$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;모든 조건을 만족하므로 Probability Mass Function이라고 할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;probability-mass-function---coin-example-2&quot;&gt;Probability Mass Function - Coin Example 2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;동전을 던져 앞면이 나오거나 던진 횟수가 $n$이 되면 동전 던지기를 멈춘다고 하자.&lt;/li&gt;
  &lt;li&gt;$P(head)=p$&lt;/li&gt;
  &lt;li&gt;Random Variable $X$ : 동전을 던진 횟수&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;$X$는 Random Variable이 될 수 있는가?&lt;br /&gt;
X={1,2,3,…n}&lt;br /&gt;
x=1 =&amp;gt; 동전 던지기 한 번에 앞 면이 나옴&lt;br /&gt;
x=2 =&amp;gt; 동전 던지기 첫 번째에는 뒷 면이 나오고, 두 번째에는 앞 면이 나옴.&lt;br /&gt;
x=3 =&amp;gt; 동전 던지기 두 번째까지 뒷 면이 나오고, 세 번째에는 앞 면이 나옴.&lt;br /&gt;
x=n =&amp;gt; 동전 던지기 (n-1)까지 뒷 면이 나오고, n 번째에는 앞 면이든 상관없다.(어차피 멈춰야 함)&lt;br /&gt;
원소들을 실수로 바꿨으므로 확률변수라고 할 수 있다.&lt;/li&gt;
  &lt;li&gt;X의 원소를 확률에 대응시켜라&lt;br /&gt;
$P(X=1)=p(H)=p$&lt;br /&gt;
$P(X=2)=p(TH)=(1-p)p$&lt;br /&gt;
$P(X=3)=p(TTH)=(1-p)^2p$&lt;br /&gt;
$P(X=n-1)=(1-p)^{n-2}p$&lt;br /&gt;
$P(X=n)=(1-p)^{n-1}p+(1-p)^{n-1}(1-p)$   이 부분 주의해야 함&lt;/li&gt;
  &lt;li&gt;$\sum_{i=1}^{n}P(X=i)=1$을 만족하는지 확인하라&lt;br /&gt;
$\sum_{i=1}^{n-1}P(X=i)+P(X=n)$&lt;br /&gt;
$=\sum_{i=1}^{n-1}(1-p)^{i-1}p+(1-p)^{n-1}$&lt;br /&gt;
$=p{1-(1-p)^{n-1}\over 1-(1-p)}+(1-p)^{n-1}$&lt;br /&gt;
$=1$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;probability-mass-function-with-infinitely-many-possible-values&quot;&gt;Probability Mass Function with Infinitely Many Possible Values&lt;/h2&gt;
&lt;p&gt;PMF : $p(X=i)=c\lambda^i/i!, \;\; i=0,1,2,…, \;\; \lambda는 양수$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$c$를 구하라.&lt;br /&gt;
$\sum_{i=1}^{\infty}P(X=i)=c\sum_{i=1}^{\infty}{\lambda^i\over i!}=c(1+\lambda+{\lambda^2\over 2!}+{\lambda^3\over 3!}+…)=ce^\lambda=1$&lt;br /&gt;
$c=e^{-\lambda}$&lt;/li&gt;
  &lt;li&gt;$P(X=0)$&lt;br /&gt;
$P(X=0)=e^{-\lambda}\lambda^0/0!=e^{-\lambda}$&lt;/li&gt;
  &lt;li&gt;$P(X&amp;gt;2)$&lt;br /&gt;
$=1-P(X\le 2)$&lt;br /&gt;
$=1-(P(X=0)+P(X=1)+P(X=2))$&lt;br /&gt;
$=e^{-\lambda}+e^{-\lambda}\lambda^1/1!+e^{-\lambda}\lambda^2/2!$&lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">How to deal with Absence of Negative Feedback</title><link href="http://localhost:4000/posts/2020/05/17/absence_of_negative_feedback" rel="alternate" type="text/html" title="How to deal with Absence of Negative Feedback" /><published>2020-05-17T00:00:00-07:00</published><updated>2020-05-17T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/17/absence_of_negative_feedback</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/17/absence_of_negative_feedback">&lt;p&gt;Implicit Feedback을 활용한 추천 시스템은 아마 unobserved data를 어떻게든 해결해야 할 것이다. 이 문제는 Explicit Feedback과 비교하면 명백하게 알 수 있다.&lt;/p&gt;

&lt;p&gt;점수가 1~5점 사이에 분포해 있는 Explicit Feedback의 경우 1점은 “유저가 아이템을 좋아하지 않는다”는 것을 나타내며, 5점은 “유저가 아이템을 좋아한다”는 것을 나타낸다.&lt;/p&gt;

&lt;p&gt;하지만 Implicit Feedback의 경우 0점을 기록(ex. 유저가 해당 아이템을 구매한 적이 없음)하더라도 “유저가 해당 아이템을 좋아하지 않는다”고 말할 수 없다. 유저가 실제로 아이템을 좋아하지 않는 경우 이외에도 유저가 아이템을 인지하지 못했던 경우가 있기 때문이다. 이러한 문제를 부정적인 피드백의 부재(absence of negative feedback)라고 한다.&lt;/p&gt;

&lt;p&gt;만약 부정적인 피드백를 모두 포함시켜 학습을 시킨다면 부정적인 피드백의 수가 긍정적인 피드백보다 훨씬 많아 긍정적인 피드백 무시하는 방향으로 학습이 진행될 것이다. 반대로 부정적인 피드백를 모두 제외시킨다면 긍정적인 피드백만 학습이 진행되어 우리의 최종 목표인 binary classification을 수행하지 못할 것이다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위한 두 가지 방법을 제안한다. 하나는 Neural Collaborative Filtering에서 사용한 방법이고, 다른 하나는 Collaborative Filtering for Implicit Feedback Datasets에서 사용한 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;1-negative-sampling&quot;&gt;1. Negative Sampling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05031.pdf&quot;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;유저가 긍정적인 피드백을 준 것 이외에 유저가 아직 피드백을 주지 않은 k개의 아이템을 포함시켜 Training Set을 구성한다.(k는 하이퍼 파라미터이며, 이 논문에서는 $k=4$)&lt;/li&gt;
  &lt;li&gt;긍정적인 피드백이 $n$개라면 Training Set은 $n(k+1)$개가 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback의 일부만 샘플링해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-confidence-level&quot;&gt;2. Confidence Level&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://yifanhu.net/PUB/cf.pdf&quot;&gt;Collaborative Filtering for Implicit Feedback Datasets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Confidence Level이라는 개념을 도입한다. 유저가 아이템과 연관성이 있는지 정도를 수치적으로 표현하기 위한 것이다.&lt;/li&gt;
  &lt;li&gt;$c_{ui}=1+\alpha r_{ui}$로 계산할 수 있다. $c_ui$가 클수록 유저와 아이템은 연관성이 큰 것이다.&lt;/li&gt;
  &lt;li&gt;$r_{ui}$는 긍정적인 피드백의 수이며, $r_{ui}$가 증가할수록 $c_ui$도 커진다. (증가하는 정도를 나타낸 것이 $\alpha$다. $\alpha$는 하이퍼 파라미터이며, 이 논문에서는 $\alpha=40$)&lt;/li&gt;
  &lt;li&gt;$c_{ui}$는 모두 1 이상의 값을 갖게 된다.&lt;/li&gt;
  &lt;li&gt;즉, Negative Feedback도 모두 사용해 훈련에 사용하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">Implicit Feedback을 활용한 추천 시스템은 아마 unobserved data를 어떻게든 해결해야 할 것이다. 이 문제는 Explicit Feedback과 비교하면 명백하게 알 수 있다.</summary></entry><entry><title type="html">Independence</title><link href="http://localhost:4000/posts/2020/05/16/independence" rel="alternate" type="text/html" title="Independence" /><published>2020-05-16T00:00:00-07:00</published><updated>2020-05-16T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/16/independence</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/16/independence">&lt;iframe src=&quot;https://www.youtube.com/embed/dHTkIna_hFk&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;independence&quot;&gt;Independence&lt;/h2&gt;
&lt;p&gt;$A$, $B$ 사건이 다음과 같은 조건을 만족하면 독립이라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B|A)=P(B)&lt;/script&gt;

&lt;p&gt;이 수식을 해석하면, “사건 $B$가 발생할 확률은 $A$의 발생 여부에 영향을 받지 않는다”이다.&lt;/p&gt;

&lt;p&gt;식을 다시 써보면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(AB)=P(A)P(B|A)=P(A)P(B)&lt;/script&gt;

&lt;p&gt;즉, 조건부 확률에 해당하는 부분을 없앨 수 있다.&lt;/p&gt;

&lt;p&gt;이를 응용하면 &lt;a href=&quot;https://blanik00.github.io/posts/2020/05/14/conditional_probability&quot;&gt;Conditional Probability&lt;/a&gt;에서 봤던 Multiplicative Rules in Probability을 단순화시킬 수 있다.&lt;/p&gt;

&lt;p&gt;$P(A_1A_2…A_n)$&lt;br /&gt;
$=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)…P(A_n|A_1A_2…A_{n-1})$&lt;br /&gt;
$=P(A_1)P(A_2)…P(A_n)$&lt;/p&gt;

&lt;h2 id=&quot;independence-events---example-1&quot;&gt;Independence Events - Example 1&lt;/h2&gt;
&lt;p&gt;A마을에는 소방차 한 대와 구급차 한 대가 있다. 소방차가 필요할 때 소방차를 사용할 수 있는 확률은 0.98이고, 구급차가 필요할 때 구급차를 사용할 수 있는 확률은 0.92이다. 이 둘을 동시에 호출했을 때 둘 다 사용할 수 있는 확률은 얼마인가?(두 사건은 독립이다)&lt;/p&gt;

&lt;p&gt;A: 소방차&lt;br /&gt;
B: 앰뷸런스&lt;/p&gt;

&lt;p&gt;$P(AB)=P(A)P(B)=0.9016$&lt;/p&gt;

&lt;h2 id=&quot;independence-events---example-2&quot;&gt;Independence Events - Example 2&lt;/h2&gt;
&lt;p&gt;두 개의 주사위를 던진다.&lt;br /&gt;
$E_1= \{ sum\; is\; 6 \} = \{ (1,5),(2,4),(3,3),(4,2),(5,1) \} $&lt;br /&gt;
$E_2= \{ sum\; is\; 7 \} = \{ (1,6),(2,5),(3,4),(4,3),(5,2),(6,1) \} $&lt;br /&gt;
$E_3= \{ first\; dice\; is\;4 \} = \{ (4,1),(4,2),(4,3),(4,4),(4,5),(4,6) \} $&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$E_1$과 $E_2$는 독립인가?&lt;br /&gt;
$E_1$과 $E_2$는 mutually exclusive하기 때문에 $P(E_1E_2)=0$이다.&lt;br /&gt;
$P(E_1E_2)\neq P(E_1)P(E_2)$&lt;br /&gt;
mutually exclusive와 independence가 다르다는 점을 주의해야 한다.&lt;/li&gt;
  &lt;li&gt;$E_1$과 $E_3$는 독립인가? dependent&lt;/li&gt;
  &lt;li&gt;$E_2$과 $E_3$는 독립인가? independent&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;또한 사건 $E$, $F$가 독립이라면, $E$, $F^C$ 또한 독립이다.&lt;/p&gt;

&lt;p&gt;$P(E)=P(EF)+P(EF^C)$&lt;br /&gt;
$=P(E)P(F)+P(EF^C)$&lt;/p&gt;

&lt;p&gt;$P(EF^C)=P(E)(1-P(F))$&lt;br /&gt;
$=P(E)P(F^C)$&lt;/p&gt;

&lt;p&gt;즉, $E$, $F$가 독립이라면, $E$가 발생할 확률은 $F$의 발생 여부에 영향을 받지 않는다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;세 개의 사건 $E$, $F$, $G$가 독립이기 위해서는 다음 조건을 만족해야 한다.&lt;/p&gt;

&lt;p&gt;$P(EFG)=P(E)P(F)P(G)$&lt;br /&gt;
$P(EF)=P(E)P(F)$&lt;br /&gt;
$P(EG)=P(E)P(G)$&lt;br /&gt;
$P(FG)=P(F)P(G)$&lt;/p&gt;

&lt;p&gt;세 사건이 독립이라면, $E$는 $F$와 $G$를 어떻게 조합해도 그 조합한 것과 독립이다. 예를 들어, $E$는 $F\cup G$과 독립이다.&lt;/p&gt;

&lt;p&gt;$P[E(F\cup G)]=P(EF\cup EG)$&lt;br /&gt;
$=P(EF)+P(EG)-P(EFG)$&lt;br /&gt;
$=P(E)P(F)+P(E)P(G)-P(E)P(FG)$&lt;br /&gt;
$=P(E)[P(F)+P(G)-P(FG)]$&lt;br /&gt;
$=P(E)P(F\cup G)$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Conditional Probability</title><link href="http://localhost:4000/posts/2020/05/14/conditional_probability" rel="alternate" type="text/html" title="Conditional Probability" /><published>2020-05-15T00:00:00-07:00</published><updated>2020-05-15T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/14/conditional_probability</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/14/conditional_probability">&lt;iframe src=&quot;https://www.youtube.com/embed/Cj25K_leYZw&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;conditional-probability&quot;&gt;Conditional Probability&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$F$라는 이벤트가 일어났다는 조건 하에 이벤트 $E$가 일어날 확률&lt;/li&gt;
  &lt;li&gt;$P(E|F)={P(EF)\over P(F)}, \quad P(F) &amp;gt; 0$&lt;/li&gt;
  &lt;li&gt;“Probability of E given F” 혹은 “Probability of E conditional on F”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;if $E$ and $F$ are mutually exclusive&lt;br /&gt;
$P(E|F)={P(EF)\over P(F)}={0\over P(F)}=0$&lt;/p&gt;

&lt;p&gt;if $F\subset E$&lt;br /&gt;
$P(E|F)={P(EF)\over P(F)}={P(F)\over P(F)}=1$&lt;/p&gt;

&lt;h2 id=&quot;conditional-probability---example-1&quot;&gt;Conditional Probability - Example 1&lt;/h2&gt;
&lt;p&gt;한 마을에 있는 인구를 성별과 고용 상태에 따라 나눈 것이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Employed&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Unemployed&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Totals&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;460&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;F&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;140&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;260&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Totals&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;900&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$P(M|U)={P(MU)\over P(U)}={40 \over 300}$&lt;/p&gt;

&lt;h2 id=&quot;conditional-probability---example-2&quot;&gt;Conditional Probability - Example 2&lt;/h2&gt;
&lt;p&gt;$P(E|F)={P(EF)\over P(F)}$&lt;br /&gt;
$P(EF)=P(F)P(E|F)$&lt;br /&gt;
$P(E|FG)={P(EFG)\over P(FG)}$&lt;/p&gt;

&lt;p&gt;Celine는 French 혹은 Chemistry 수업 중 어떤 것을 들을지 고민하고 있다. 그녀가 French를 수강하고 A학점을 받을 확률은 $1/2$이며, Chemistry를 수강하고 A학점을 받을 확률은 $2/3$이다. 만약 그녀가 동전던지기를 통해 어떤 수업을 들을지 결정한다면, 그녀가 Chemistry 수업을 듣고, A학점을 받게될 확률은 무엇인가?&lt;/p&gt;

&lt;p&gt;event C : Chemistry 수업을 수강하는 사건&lt;br /&gt;
event A : A학점을 받는 사건&lt;/p&gt;

&lt;p&gt;구하고자 하는 확률은 $P(CA)$이다.&lt;/p&gt;

&lt;p&gt;$P(CA)=P(C)P(A|C)=({1\over 2})({2\over 3})={1\over3}$&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-rules-in-probability&quot;&gt;Multiplicative Rules in Probability&lt;/h2&gt;
&lt;p&gt;조건부 확률의 정의로부터 다음을 도출할 수 있다.&lt;br /&gt;
$P(AB)=P(A)P(B|A)=P(B)P(A|B)$&lt;br /&gt;
$P(ABC)=P(AB)P(C|AB)=P(A)P(B|A)P(C|AB)$&lt;br /&gt;
$P(ABCD)=P(ABC)P(D|ABC)=P(A)P(B|A)P(C|AB)P(D|ABC)$&lt;/p&gt;

&lt;p&gt;이것을 일반화하면 사건 $A_1$, $A_2$, … $A_k$에 대해 &lt;script type=&quot;math/tex&quot;&gt;P(A_1A_2...A_k)=P(A_1)P(A_2\|A_1)P(A_3\|A_1A_2)...P(A_k\|A_1A_2...A_{k-1})&lt;/script&gt;을 만족한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고&lt;/strong&gt; 각 사건이 독립이라는 조건이 있다면 위 연산이 간단해진다.&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule&quot;&gt;Bayes’ Rule&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82064341-9777d680-9707-11ea-9727-4353740824c2.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$A_1$, $A_2$, $A_3$은 표본 공간 S의 Partition이다(Mutually Exclusive &amp;amp; Collectively Exhaustive).&lt;/li&gt;
  &lt;li&gt;$B$는 event이다.&lt;br /&gt;
$P(B)=P(A_1B)+P(A_2B)+P(A_3B)$&lt;br /&gt;
$\qquad=P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)$&lt;br /&gt;
$\qquad=\sum_{i=1}^{3}P(A_i)P(B|A_i)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$P(B)$ 즉, 우리가 관심있는 이벤트에 대한 확률은 &lt;strong&gt;조건부 확률의 합&lt;/strong&gt;으로 표현할 수 있다.(Law of Total Probability) 이것이 Bayes’ Rule의 시작점이다.&lt;/p&gt;

&lt;p&gt;이제 본격적으로 Bayes’ Rule에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;다음과 같은 확률들을 알고있다고 가정하자.&lt;br /&gt;
$P(A_1)$, $P(A_2)$, $P(A_3)$&lt;br /&gt;
$P(B|A_1)$, $P(B|A_2)$, $P(B|A_3)$&lt;/p&gt;

&lt;p&gt;이 정보들을 가지고 다음과 같은 확률을 알아내야 한다고 하자.&lt;/p&gt;

&lt;p&gt;$P(A_1|B)$, $P(A_2|B)$, $P(A_3|B)$&lt;/p&gt;

&lt;p&gt;이 작업을 해내는 것이 Bayes’ Rule이다.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A_1|B)={P(A_1)P(B|A_1)\over P(B)}={P(A_1)P(B|A_1)\over P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A_2|B)={P(A_2)P(B|A_2)\over P(B)}={P(A_2)P(B|A_2)\over P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A_3|B)={P(A_3)P(B|A_3)\over P(B)}={P(A_3)P(B|A_3)\over P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;용어-정리&quot;&gt;용어 정리&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/82064328-947ce600-9707-11ea-9271-482fb3a038ac.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;정리&quot;&gt;정리&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Prior Probability와 Data Probability를 기반으로 Posterior Probability를 구하는 법칙&lt;/li&gt;
  &lt;li&gt;$P(A_1|B)$과 $P(A_1)$은 모두 $A_1$에 대한 확률이다. 다만 앞의 것은 $B$가 주어졌을 때의 확률이다. 무엇인가 주어졌다는 것은 정보가 주어졌다는 것이다. 즉, $B$는 정보다. $P(A_1|B)$는 $P(A_1)$이 정보 $B$에 의해 업데이트된 것이다. 그래서 $P(A_1)$은 정보를 받기 전이라서 Prior이고, $P(A_1|B)$은 정보를 받은 후라서 Posterior이다. 정리하자면 Posterior Probability는 Prior Probability의 업데이트된 버전이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bayes-rule---supplier-example&quot;&gt;Bayes’ Rule - Supplier Example&lt;/h2&gt;
&lt;p&gt;완성차 제조업체 A는 두 타이어 업체로부터 타이어를 공급받는다. 타이어 업체는 1, 2 두 가지가 있으며, 업체 1에서 공급받은 타이어의 10%가 불량품이며, 업체 2에서 공급받은 타이어의 10%가 불량품이다. 현재 A에서 보유하고 있는 타이어 재고의 40%가 1에서 공급받은 것이다. 이 때 불량 타이어가 발견되었을 때, 그것이 업체 1에서 공급받은 타이어일 확률을 구하라.&lt;/p&gt;

&lt;p&gt;$P(S_1 | D)$&lt;/p&gt;

&lt;p&gt;우선 Prior Probability와 Data Probability를 알아야 한다.
$P(S_1)=0.4, P(S_2)=0.6$
$P(D | S_1)=0.1, P(D| S_2)=0.05$&lt;/p&gt;

&lt;p&gt;$P(S_1 | D)={P(S_1 \cap D)\over P(D)}$&lt;br /&gt;
$\qquad \qquad={P(S_1 \cap D)\over P(S_1 \cap D) + P(S_2 \cap D)}$&lt;br /&gt;
$\qquad \qquad={P(S_1)P(D|S_1)\over P(S_1)P(D|S_1) + P(S_2)P(D|S_2)}$&lt;br /&gt;
$\qquad \qquad=0.57$&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule---criminal-investigation&quot;&gt;Bayes’ Rule - Criminal Investigation&lt;/h2&gt;
&lt;p&gt;경찰관이 수사를 통해 60%의 확신을 가지고 용의자를 추려냈는데, 이 상황에서 새로운 증거가 발견되었다. 그것은 범인이 대머리라는 사실이다. 만약 전체 인구의 20%가 대머리일 때, 대머리인 사람이 범인일 확률을 구하라.&lt;/p&gt;

&lt;p&gt;마찬가지로 Prior Probability와 Data Probability를 알아야 하는데, 조금 헷갈리는 부분이 있어 잘 생각해봐야 한다.&lt;/p&gt;

&lt;p&gt;$P(S)$ : 유죄일 확률&lt;br /&gt;
$P(\sim S)$ : 무죄일 확률&lt;br /&gt;
$P(B| S)$ : 유죄이면서 대머리일 확률&lt;br /&gt;
$P(B| \sim S)$ : 무죄이면서 대머리일 확률&lt;/p&gt;

&lt;p&gt;$P(S)=0.6, P(\sim S)=0.4$&lt;br /&gt;
$P(B| S)=1, P(B| \sim S)=0.2$&lt;/p&gt;

&lt;p&gt;처음에는 전체 인구의 20%가 대머리라고 해서 $P(B| S)$과 $P(B| \sim S)$의 확률이 모두 0.2이라고 생각했다. 하지만 문제에서 범인이 대머리라는 사실이 밝혀졌다고 명시했으므로 $P(B| S)=1$이다. 즉, 범인은 반드시 대머리다.&lt;/p&gt;

&lt;p&gt;$P(S | B)={P(S \cap B)\over P(B)}$&lt;br /&gt;
$\qquad \qquad={P(B|S)P(S)\over P(B|S)P(S) + P(B|\sim S)P(\sim S)}$&lt;br /&gt;
$\qquad \qquad=0.882$&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule---disease-example&quot;&gt;Bayes’ Rule - Disease Example&lt;/h2&gt;
&lt;p&gt;전체 인구의 1%가 희귀병에 앓고 있다. A사가 개발한 혈액 테스트는 97%의 확률로 이 희귀병을 진단할 수 있다(희귀병에 걸린 사람에게 희귀병에 걸렸다고 진단). 하지만 건강한 사람이 희귀병을 앓고 있다고 잘못 진단할 확률도 6% 있다.&lt;/p&gt;

&lt;p&gt;우선 Prior Probability와 Data Probability를 알아두자.&lt;/p&gt;

&lt;p&gt;$D$ : 희귀병&lt;br /&gt;
$P$ : 양성&lt;br /&gt;
$N$ : 음성&lt;/p&gt;

&lt;p&gt;$P(D)=0.01, P(\sim D)=0.99$&lt;br /&gt;
$P(P| D)=0.97, P(N| D)=0.03$&lt;br /&gt;
$P(P| \sim D)=0.06, P(N| \sim D)=0.94$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$P(P)$&lt;br /&gt;
$P(P)=P(D\cap P) + P(\sim D\cap P)$&lt;br /&gt;
$\qquad=P(D)P(P|D)+P(\sim D)P(P|\sim D)$&lt;br /&gt;
$\qquad=0.0691$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(D| P)$&lt;br /&gt;
$P(D| P)={P(D\cap P)\over P(P)}=0.14$  (위에서 다 구했던 것들임)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(\sim D| N)$&lt;br /&gt;
$P(\sim D| N)={P(\sim D\cap N)\over P(N)}={P(\sim D)P(N|\sim D)\over 1-P(P)}=0.999$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">Poisson Distribution</title><link href="http://localhost:4000/posts/2020/05/14/poisson_distribution" rel="alternate" type="text/html" title="Poisson Distribution" /><published>2020-05-14T00:00:00-07:00</published><updated>2020-05-14T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/14/poisson_distribution</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/14/poisson_distribution">&lt;iframe src=&quot;https://www.youtube.com/embed/HJZ5Ev_p8Uo&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;복습&quot;&gt;복습&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sample Space(표본 공간)&lt;/strong&gt; : 실험으로 나오는 모든 결과를 담고 있는 집합&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Random Variable(확률 변수)&lt;/strong&gt; : sample space의 원소들을 숫자로 바꿔주는 함수&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Probability Function(확률 함수)&lt;/strong&gt; : Random Variable을 확률로 변환하는 함수&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Probability Distribution(확률 분포)&lt;/strong&gt; : Probability Function으로부터 나온 확률들의 전반적인 패턴&lt;/p&gt;

&lt;p&gt;확률 분포는 확률 함수의 input에 따라 두 가지로 나뉜다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;확률 함수의 input&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;확률 함수&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;확률 분포&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;discrete&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PMF(확률질량함수)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Discrete Probability Distribution(이산형 확률분포)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;continous&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PDF(확률밀도함수)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Continous Probability Distribution(연속형 확률분포)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;확률질량함수에서 나온 확률 분포. 즉, 이산형 확률분포는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Bernoulli Distribution&lt;/li&gt;
  &lt;li&gt;Binomial Distribution&lt;/li&gt;
  &lt;li&gt;Poisson Distribution&lt;/li&gt;
  &lt;li&gt;Geometric Distribution&lt;/li&gt;
  &lt;li&gt;Negative Binomial Distribution&lt;/li&gt;
  &lt;li&gt;Hypergeometric Distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;지난 시간에 Bernoulli, Binomial Distribution에 대해 배웠으며, &lt;strong&gt;이번에는 Poisson Distribution에 대해 배운다.&lt;/strong&gt; 또한 다음 시간에는 나머지 이산형 확률분포에 대해 배운다.&lt;/p&gt;

&lt;p&gt;확률밀도함수에서 나온 확률 분포. 즉, 연속형 확률분포는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Uniform Distribution&lt;/li&gt;
  &lt;li&gt;Normal Distribution&lt;/li&gt;
  &lt;li&gt;Exponential Distribution&lt;/li&gt;
  &lt;li&gt;Gamma Distribution&lt;/li&gt;
  &lt;li&gt;Beta Distribution&lt;/li&gt;
  &lt;li&gt;Chi-square Distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연속형 확률 분포는 나중에 다룬다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;지난 시간에 배운 Bernoulli, Binomial Distribution을 복습하자.&lt;/p&gt;

&lt;h3 id=&quot;bernoulli-distribution&quot;&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : 0 혹은 1만을 가지는 단순한 확률 변수&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;0과 1을 확률로 변환하려면 확률 함수(Probability Function)가 필요하다. 여기서 확률 함수의 input은 이산형이므로 Bernoulli Distribution은 이산형 확률 분포가 된다.&lt;/p&gt;

&lt;p&gt;확률 함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(x;p)=p^x(1-p)^{1-x}, x=0 \; or \; 1&lt;/script&gt;

&lt;p&gt;즉, Bernoulli Distribution은 Bernoulli 확률 함수로부터 생성되는 확률들의 분포를 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;binomial-distribution&quot;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Binomial Distribution은 Bernoulli Distribution으로부터 나온다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : Bernoulli trials를 $n$번 했을 때, 성공 횟수
parameter는 $n$, $p$이다.(parameter는 확률 분포의 모양의 결정할 때 중요한 값)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;성공 횟수를 확률로 변환하려면 확률 함수(Probability Function)가 필요하다. 여기서 확률 함수의 input은 이산형이므로 Binomial Distribution은 이산형 확률 분포가 된다.&lt;/p&gt;

&lt;p&gt;확률 함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)={n\choose x}p^x(1-p)^{1-x}\quad for \;x=0,1,2,...,n&lt;/script&gt;

&lt;p&gt;즉, Binomial Distribution은 Binomial 확률 함수로부터 생성되는 확률들의 분포를 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;poisson-distribution&quot;&gt;Poisson Distribution&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : 단위 시간 혹은 단위 공간 안에 특정 사건이 몇 번 발생할 것인가&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;한 번도 발생 안할 수도 있고(0), 1번 발생할 수도 있고, 2번 발생할 수도 있다. 즉, discrete하다.&lt;/p&gt;

&lt;p&gt;발생 횟수를 확률로 변환하려면 확률 함수(Probability Function)가 필요하다. 여기서 확률 함수의 input은 이산형이므로 Poisson Distribution은 이산형 확률 분포가 된다.&lt;/p&gt;

&lt;p&gt;확률 함수는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X=i)={e^{-\lambda}\lambda^i\over i!}, \quad i=0,1,2,...&lt;/script&gt;

&lt;p&gt;이것이 확률 함수가 되기 위해서는 더해서 1이 되어야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=0}^{\infty}P(X=i)=e^{-\lambda}\sum_{i=0}^{\infty}{\lambda^i\over i!}=e^{-\lambda}e^{\lambda}=1\; (\sum_{i=0}^{\infty}{\lambda^i\over i!}=e^{\lambda} \; by \; taylor \; series)&lt;/script&gt;

&lt;p&gt;parameter는 $\lambda$이다.&lt;/p&gt;

&lt;h2 id=&quot;poisson-approximation-to-binomial&quot;&gt;Poisson Approximation to Binomial&lt;/h2&gt;
&lt;p&gt;Binomial Distribution에서 parameter는 $n$과 $p$이다. $n$이 굉장히 크고, $p$가 굉장히 작을 때, Binomial Distribution을 parameter $\lambda=np$인 Poisson Distribution으로 근사할 수 있다.&lt;/p&gt;

&lt;p&gt;즉, $Binomial(i;n,p)\approx Poisson(i;\lambda), \quad \lambda = np, p={\lambda \over n}$&lt;/p&gt;

&lt;p&gt;$P(X=i)={n!\over(n-i)!i!}p^i(1-p)^{n-i}\approx e^{-\lambda} {\lambda^i\over i!}$
$={n!\over(n-i)!i!}({\lambda \over n})^i(1-{\lambda \over n})^{n-i}$
$={n(n-1)…(n-i+1)\over n^i}{\lambda^i\over i!}{(1-{\lambda \over n})^{n}\over (1-{\lambda \over n})^{i}}$&lt;/p&gt;

&lt;p&gt;마지막 식을 세 부분으로 나누어 푼다. $n$이 아주 크다고 했으므로 $n$을 무한대로 보내보자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;${n(n-1)…(n-i+1)\over n^i}$&lt;br /&gt;
$\lim_{n \to \infty}{n(n-1)…(n-i+1)\over n^i}=1$&lt;/li&gt;
  &lt;li&gt;$(1-{\lambda \over n})^{i}$&lt;br /&gt;
$\lim_{n \to \infty}(1-{\lambda \over n})^{i}=1$&lt;/li&gt;
  &lt;li&gt;$(1-{\lambda \over n})^{n}$&lt;br /&gt;
$\lim_{n \to \infty}(1-{\lambda \over n})^{n}=\lim_{n \to \infty}((1-{\lambda \over n})^{n\over \lambda})^{\lambda}=\lim_{n \to \infty}(e^{-1})^{\lambda}=\lim_{n \to \infty}(e^{-\lambda})$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;결과를 대입해보면 ${e^{-\lambda}\lambda^i\over i!}$가 나오며, 이는 포아송 분포의 확률 함수와 같다.&lt;/p&gt;

&lt;h2 id=&quot;example-of-poisson-random-variable&quot;&gt;Example of Poisson Random Variable&lt;/h2&gt;
&lt;p&gt;Poisson 분포는 단위 시간 혹은 단위 공간 내에 특정 사건이 몇 번 발생했는지를 나타낸다고 했는데, 조금 더 추가하자면 특정 사건은 자주 발생하는 것이 아니라 드물게 발생해야 포아송 분포로 모델링하기 좋다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 마을에서 100세 이상까지 사는 사람의 수&lt;/li&gt;
  &lt;li&gt;책의 한 페이지에서 오타의 개수&lt;/li&gt;
  &lt;li&gt;하룻동안 잘못 전화를 건 횟수&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mean-and-variance-of-poisson-distribution&quot;&gt;Mean and Variance of Poisson Distribution&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;$E[X]=V[X]=\lambda$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$E[X]=\sum_{i=0}^{\infty}{ie^{-\lambda}\lambda^{i}\over i!}$&lt;br /&gt;
$=\lambda\sum_{i=1}^{\infty}{e^{-\lambda}\lambda^{i-1}\over (i-1)!}$&lt;br /&gt;
$Let \;\;k=i-1$&lt;br /&gt;
$=\lambda\sum_{k=0}^{\infty}{e^{-\lambda}\lambda^{k}\over k!}$&lt;br /&gt;
$=\lambda e^{-\lambda}\sum_{k=0}^{\infty}{\lambda^{k}\over k!}$&lt;br /&gt;
$=\lambda e^{-\lambda}e^{\lambda}$&lt;br /&gt;
$=\lambda$&lt;/p&gt;

&lt;p&gt;아래도 비슷하게 풀 수 있다.&lt;br /&gt;
$E[X^2]=\lambda(\lambda+1)$&lt;br /&gt;
$V[X]=E[X^2]-{E[X]}^2=\lambda^2 + \lambda - \lambda^2=\lambda$&lt;/p&gt;

&lt;h2 id=&quot;poisson-distribution---quality-example&quot;&gt;Poisson Distribution - Quality Example&lt;/h2&gt;
&lt;p&gt;유리 제조사의 품질 관리자가 유리에 결함이 있는지 검사하고 있다. 유리에 있는 결함의 수를 $\lambda=0.5$인 Poisson Distribution으로  가정했을 때, 유리에 결함이 하나도 없을 확률은 얼마인가?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : Number of imperfections in a glass&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;우리는 확률을 구해야 하니 확률 함수를 이용해야 한다. 즉, 이 문제는 $P(X=0)$을 구하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;$P(X=0)={e^{-0.5}0.5^0\over 0!}=e^{-0.5}=0.607$&lt;/p&gt;

&lt;h2 id=&quot;poisson-distribution---radioactive-particle-example&quot;&gt;Poisson Distribution - Radioactive Particle Example&lt;/h2&gt;
&lt;p&gt;방사선 입자는 ms(millisecond) 동안 4개가 나온다. 2.5ms 동안 나오는 방사선 입자의 개수가 15일 확률을 구하라.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Random Variable X : Number of Radioactive Particle over 2.5ms period&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;우리는 확률을 구해야 하니 확률 함수를 이용해야 한다. 즉, 이 문제는 $P(X=15)$을 구하고자 하는 것이다.&lt;/p&gt;

&lt;p&gt;우리가 알고 있는 것은 ms 동안 4개의 입자가 나온다는 것이다. 이를 2.5ms로 바꾸면 10개의 입자가 나온다는 것이 된다. 즉, $\lambda=\lambda’t=4\times 2.5=10$이다.(보통 Poisson Distribution 문제를 풀 때 단위 시간을 1로 주는데 이 문제는 1이 아니라는 점을 주의해야 한다)&lt;/p&gt;

&lt;p&gt;$P(X=15)=f(15;10)={e^{-10}10^{15}\over 15!}=0.0348$&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">YOLO, SSD, YOLO 9000</title><link href="http://localhost:4000/posts/2020/05/11/yolo" rel="alternate" type="text/html" title="YOLO, SSD, YOLO 9000" /><published>2020-05-11T00:00:00-07:00</published><updated>2020-05-11T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/11/yolo</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/11/yolo">&lt;h2 id=&quot;yolo&quot;&gt;YOLO&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;You only look once!&lt;/li&gt;
  &lt;li&gt;성능 자체는 Faster R-CNN과 비슷하나, 속도가 훨씬 빠름&lt;/li&gt;
  &lt;li&gt;기존의 방법에서는 bounding box를 만들어서 그것을 Neural Network에 넣어 분류를 하는 방법을 사용했다면 YOLO에서는 bounding box를 찾는 것과 classification을 동시에 수행한다. (detection 문제를 regression 문제로 바꿈)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;flow&quot;&gt;Flow&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;이미지를 입력받으면 $S\times S$로 줄인다.(Max Pooling을 통해서 줄임, 여기서 $S$=7) 각 칸을 그리드라고 부른다.&lt;/li&gt;
  &lt;li&gt;각 그리드마다  class probability를 C개씩, bounding box를 B개씩 부여한다. 각 bounding box는 [x, y, w, h, conf]로 이루어져 있다. 마지막에 있는 conf(confidence)는 이 bounding box가 쓸모있는 것인지 아닌지를 나타낸다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;만약 S=7, B=2, C=20이라면 마지막 layer는 $7\times 7\times 30$이다.($30=20+2\times 5$)&lt;/p&gt;

&lt;h3 id=&quot;yolo의-한계&quot;&gt;YOLO의 한계&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;각 그리드가 B개 만의 bounding box만을 만드므로 조그마한 물체들이 뭉쳐 있을 때는 좋은 성능을 발휘하지 못한다.&lt;/li&gt;
  &lt;li&gt;bounding box의 정보가 정확하지 않을 수 있다.&lt;/li&gt;
  &lt;li&gt;Loss Function이 작은 box와 큰 box를 동일하게 다뤄 에러를 측정하므로 점수를 메길 때 불리하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ssd&quot;&gt;SSD&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Single Shot MultiBox Detector&lt;/li&gt;
  &lt;li&gt;YOLO의 컨셉(Single Shot)과 Faster R-CNN의 Region Proposal(MultiBox)을 합친 것&lt;/li&gt;
  &lt;li&gt;Predicting category scores from each cell of multiple convolutional feature maps.&lt;/li&gt;
  &lt;li&gt;Predicting Box offsets for a fixed set of default boxes from each cell of multiple convolutional feature maps.&lt;/li&gt;
  &lt;li&gt;즉, 각 feature map의 셀 마다 k개의 default box(anchor box)를 가지며, 각 box는 c개의 class score와 4개의 offset을 가지고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;yolo-9000&quot;&gt;YOLO 9000&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;SSD보다 좋은 성능을 내기 위해 이것저것 많은 시도를 한 논문. 실용적인 내용이 많다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;yolo가-잘-안됐던-이유&quot;&gt;YOLO가 잘 안됐던 이유&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Localization Errors&lt;/li&gt;
  &lt;li&gt;Low recall compared region proposal based method
box를 적게 뽑았으니 recall 성능이 낮았다는 말임.
&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81780462-85443f80-9531-11ea-9085-b056bb24514d.jpg&quot; alt=&quot;1&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;개선-방법faster&quot;&gt;개선 방법(Faster)&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Batch Normalization
2% 성능 개선(웬만하면 Batch Normalization 쓰는 것이 좋음)&lt;/li&gt;
  &lt;li&gt;High Resolution
original YOLO는 $224\times 224$를 썼는데, 여기서는 $448\times 448$을 썼다.&lt;/li&gt;
  &lt;li&gt;Anchor Boxes
Region Proposal Network에서 사용된 anchor box와 비슷한 것을 사용
anchor box마다 class와 objectiveness를 예측&lt;/li&gt;
  &lt;li&gt;Dimension Cluster
기존에는 anchor box의 pre-defined 크기를 정할 때 실제 데이터셋의 box 크기르 고려하지 않고 사람이 임의로 정했다. YOLO 9000에서는 실제 데이터셋에서 bounding box를 클러스터링해서 거기에 많이 쓰이는 bounding box 크기를 사용&lt;/li&gt;
  &lt;li&gt;Location Prediction
YOLO 9000은 box offset과 scaling을 예측한다. 즉, 사이즈를 몇 배 키울지 혹은 줄일지를 예측&lt;/li&gt;
  &lt;li&gt;Fine-grained Features&lt;/li&gt;
  &lt;li&gt;Multi-scale Training&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;개선-방법stronger&quot;&gt;개선 방법(Stronger)&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Hierarchical classification
YOLO 9000은 9000개의 클래스를 구분할 수 있다는 말이다. 9000개 중 하나를 찾는 것은 매우 힘든 일이다. 그래서 여기서는 Hierarchical classification을 적용한다. 예를 들어, 요커셔 테리어를 생각해보면 “physical object” - “animal” - “mammal” - … - “hunting dog” - “terrior” - Yorkshire terriror”라는 식으로 parent에 해당하는 라벨도 생각해낼 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81780457-837a7c00-9531-11ea-8b3c-177b763d78ae.PNG&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림처럼  위계마다 하나의 클래스를 선택한다. 결과적으로는 한 이미지를 입력하면 여러 개의 클래스를 가지는 결과가 나온다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">YOLO You only look once! 성능 자체는 Faster R-CNN과 비슷하나, 속도가 훨씬 빠름 기존의 방법에서는 bounding box를 만들어서 그것을 Neural Network에 넣어 분류를 하는 방법을 사용했다면 YOLO에서는 bounding box를 찾는 것과 classification을 동시에 수행한다. (detection 문제를 regression 문제로 바꿈)</summary></entry><entry><title type="html">R-CNN, SPP, Fast R-CNN, Faster R-CNN</title><link href="http://localhost:4000/posts/2020/05/10/r-cnn" rel="alternate" type="text/html" title="R-CNN, SPP, Fast R-CNN, Faster R-CNN" /><published>2020-05-10T00:00:00-07:00</published><updated>2020-05-10T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/10/r-cnn</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/10/r-cnn">&lt;h2 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771298-92573380-951d-11ea-8aea-25f440c43aec.PNG&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최초로 딥러닝을 활용해 detection 문제에 접근&lt;/li&gt;
  &lt;li&gt;매우 간단&lt;/li&gt;
  &lt;li&gt;이미지 안에서 여러 개의 bounding box를 뽑아냄.(region proposal) 이 부분은 딥러닝을 사용하지 않았으며, 속도가 느리기 때문에 bottle neck이 된다. selective search 방법을 사용한다.&lt;/li&gt;
  &lt;li&gt;모든 bounding box를 같은 사이즈로 resize하고, resize된 box를 CNN에 넣어 feature를 추출한다. (모든 box에 대해 CNN을 수행하므로 속도가 느림)&lt;/li&gt;
  &lt;li&gt;추출된 feature를 SVM으로 분류한다.&lt;/li&gt;
  &lt;li&gt;분류 카테고리가 $n$개라면 실제로는 “background”까지 포함해 $n+1$개가 되어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;spp&quot;&gt;SPP&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771299-93886080-951d-11ea-81d7-9e532edc17ed.PNG&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RCNN은 한 이미지의 region proposals의 수만큼 CNN이 돌아야 했다. 이 부분 때문에 bottle neck이 생겼는데, SPP는 한 번만 돌면 되도록 개선시켰다.&lt;/li&gt;
  &lt;li&gt;SPP는 convolution 연산을 한 후에 나오는 convolutional feature map 위에서 원하는 정보를 찾는다. convolutional feature map에서 뽑히는 박스의 위치는 이미지에서 뽑힌 bounding box의 위치에서 뽑는다.(이 때도 딥러닝을 사용하지는 않는다) box를 뽑았다면 Spatial Pyramid Pooling을 사용한다. Spatial Pyramid Pooling의 목적은 fixed-length representation을 찾는 것이다.&lt;/li&gt;
  &lt;li&gt;Spatial Pyramid Pooling이란 무엇일까?
만약 우리가 뽑은 convolution feature map에서 뽑은 박스의 사이즈가 $2\times 4$라고 하자.&lt;br /&gt;
우리가 구하고자 하는 fixed-length가 $1\times 1$이라고 한다면 $2\times 4$의 값들을 평균내어 사용한다.&lt;br /&gt;
또한 우리가 구하고자 하는 fixed-length가 $2\times 2$라고 한다면 $2\times 4$를 네 부분으로 나누어 각각을 평균내어 사용한다. 즉, Pooling을 통해 같은 사이즈로 바꾸는 작업이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fast-r-cnn&quot;&gt;Fast R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771301-9420f700-951d-11ea-988e-1f6f110c6c02.PNG&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fast R-CNN과 SPP는 구조가 거의 같다.&lt;/li&gt;
  &lt;li&gt;굳이 차이점이 있다면 fixed-length representation을 찾는 방식에 차이가 있다. SPP는 Spatial Pyramid Pooling을 사용하였고, Fast R-CNN은 ROI Pooling을 통해 찾는다. 사실 이 둘도 비슷하긴 하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;flow&quot;&gt;Flow&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;입력 이미지를 받고, 입력 이미지로부터 bounding box를 뽑는다.(딥러닝을 사용하지 않는다.)&lt;/li&gt;
  &lt;li&gt;CNN을 통해 convolutional feature map을 생성한다.&lt;/li&gt;
  &lt;li&gt;ROI Pooling을 통해 각 bounding box의 fixed-length feature vector를 만든다.&lt;/li&gt;
  &lt;li&gt;결과는 두 가지가 되는데, 하나는 “background”를 고려한 (k+1) classification이고, 다른 하나는 bounding box regression이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;roi-pooling&quot;&gt;ROI Pooling&lt;/h3&gt;
&lt;p&gt;입력을 미리 정해둔 사이즈로 쪼갠 후, 해당 하는 부분을 평균내서 벡터로 만든다. 예를 들어, $3\times 3$으로 쪼개는 것으로 정해뒀다면 입력을 $3\times 3$으로 쪼개고, 각각을 평균낸 후 이어붙여 길이가 9인 벡터로 만든다.&lt;/p&gt;

&lt;h2 id=&quot;faster-r-cnn&quot;&gt;Faster R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81771304-94b98d80-951d-11ea-9505-6f5ac079ca4d.PNG&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;지금까지 배운 것은 나이브한 방법에 딥러닝을 바로 적용한 detection 방법론이었다. 하지만 여기부터는 딥러닝을 활용한 region proposal이 시작된다. (기존에는 이 부분이 bottleneck이었음)&lt;/li&gt;
  &lt;li&gt;Faster R-CNN = Region Proposal Network + Fast R-CNN&lt;/li&gt;
  &lt;li&gt;어떠한 이미지의 사이즈와 스케일이 다를 때 고려할 수 있는 방법은 세 가지가 있다. 첫 번째 방법은 Pyramids of images이며, Spatial Pyramid Pooling이 여기에 해당된다. 이미지 자체의 사이즈를 줄여서 줄어든 multiple scaled image를 한 번에 고려하는 것이다. 두 번째 방법은 Pyramids of filters이며, 이 방법은 필터의 사이즈를 바꾸는 것이다. 세 번째 방법이 &lt;strong&gt;Pyramids of anchors&lt;/strong&gt;이다. Anchor는 미리 정해져있는 bounding box의 크기를 의미한다. 미리 bounding box의 크기가 어느정도 될 것인지를 생각을 해두고, 이 box를 얼만큼 바꿔야지 target box가 되는지에 해당하는 offset을 학습한다.&lt;/li&gt;
  &lt;li&gt;Region Proposal Network는 이미지나 convolutional feature map을 입력으로 받는다. 각 픽셀마다 Anchor를 정의한다. 각 점을 중앙으로 하는 k개의 Anchor box를 만든다. 각 Anchor box마다 2개의 점수를 할당하는데(2k scores) 앞의 것이 크면 positive(쓸만한 Anchor box), 뒤의 것이 크면 negative(쓸모없는 Anchor box)이다. 또한 4k coordinates는 각 Anchor boxes를 어떻게 움직여야 target box에 가까워질 것인지에 대한 숫자가 들어있다.&lt;/li&gt;
  &lt;li&gt;즉, Region Proposal Network는 총 $k\times (4+2)$ 사이즈의 벡터를 출력한다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">R-CNN 최초로 딥러닝을 활용해 detection 문제에 접근 매우 간단 이미지 안에서 여러 개의 bounding box를 뽑아냄.(region proposal) 이 부분은 딥러닝을 사용하지 않았으며, 속도가 느리기 때문에 bottle neck이 된다. selective search 방법을 사용한다. 모든 bounding box를 같은 사이즈로 resize하고, resize된 box를 CNN에 넣어 feature를 추출한다. (모든 box에 대해 CNN을 수행하므로 속도가 느림) 추출된 feature를 SVM으로 분류한다. 분류 카테고리가 $n$개라면 실제로는 “background”까지 포함해 $n+1$개가 되어야 한다.</summary></entry><entry><title type="html">CNN</title><link href="http://localhost:4000/posts/2020/05/09/cnn" rel="alternate" type="text/html" title="CNN" /><published>2020-05-09T00:00:00-07:00</published><updated>2020-05-09T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/09/CNN</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/09/cnn">&lt;h2 id=&quot;cnn이란&quot;&gt;CNN이란?&lt;/h2&gt;
&lt;p&gt;CNN은 아래 세 가지 요소로 이루어져 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Convolution : Input Image에 Convolution 연산을 하면 “Convolutional Feature Map”이 나온다.&lt;/li&gt;
  &lt;li&gt;Subsampling : sampling을 통해 image 혹은 feature map의 크기(spatial한 정보)가 줄어듦.&lt;/li&gt;
  &lt;li&gt;Full Connection(Dense)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Convolution과 Subsampling은 &lt;strong&gt;특성 추출(feature extraction)&lt;/strong&gt;을 수행한다. 즉, CNN은 feature들의 조합으로 물체를 구분한다.&lt;/p&gt;

&lt;p&gt;또한 Full Connection은 feature들을 기반으로 분류를 수행한다.&lt;/p&gt;

&lt;p&gt;즉, CNN은 이미지로부터 특징을 추출해내고, 이를 기반으로 분류를 하는 모델이다.&lt;/p&gt;

&lt;h2 id=&quot;왜-cnn이-잘-될까&quot;&gt;왜 CNN이 잘 될까?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Local Invariance
동일한 사이즈의 filter가 이미지의 모든 부분을 돌아다니기 때문에 찾고자 하는 이미지가 어디에 있는지는 중요하지 않다.&lt;/li&gt;
  &lt;li&gt;Compositionality
CNN을 여러 층 쌓으면 계층 구조가 생겨 성능이 좋다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convolution이란&quot;&gt;Convolution이란&lt;/h2&gt;
&lt;p&gt;이미지의 특정 부분과 filter가 얼마나 유사한지 연산하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81528300-572af800-9397-11ea-9833-d7abea9d8664.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 Image의 진한 파란 부분과 filter의 convolution 연산을 수행한다면 두 행렬을 element-wise product하고 sum하면 된다. (=51)&lt;/p&gt;

&lt;p&gt;해당 부분에 대한 convolution을 완료했다면, 다음 칸으로 옮겨 동일한 연산을 수행한다.&lt;/p&gt;

&lt;h2 id=&quot;zero-padding&quot;&gt;Zero-Padding&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81528296-54c89e00-9397-11ea-9864-7c430aa6c784.gif&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convolution을 해야 하는데, 이를 위해서는 원본 이미지의 사이즈를 좀 더 크게 만들어야 할 때 zero-padding을 한다. 위 그림에서 실선 부분이 zero-padding한 곳이다.&lt;/p&gt;

&lt;h2 id=&quot;stride&quot;&gt;Stride&lt;/h2&gt;
&lt;p&gt;Convolution 연산을 마치고 몇 칸을 뛰는지를 결정하는 것이 Stride이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/81528298-56926180-9397-11ea-9723-a2f49c6c2d26.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 왼쪽은 stride=1이고, 오른쪽은 stride=2이다.&lt;/p&gt;

&lt;p&gt;stride의 수와 filter의 Width, Height가 같다면 Overlapping없이 feature map을 생성할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;input-image&quot;&gt;Input Image&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;[batch, in_height, in_width, in_channels]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;batch : batch size
in_channels : RGB인 경우 3, Grey인 경우 1&lt;/p&gt;

&lt;h2 id=&quot;filter&quot;&gt;Filter&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;[filter_height, filter_width, in_channels, out_channels]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;input_ image의 in_channels와 filter의 in_channels는 항상 같아야 한다.
out_channels는 필터의 개수&lt;/p&gt;

&lt;h2 id=&quot;파라미터의-수&quot;&gt;파라미터의 수&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;모델 보면 파리미터의 수를 셀 줄 알아야 한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$4\times 4\times 3$ 이미지를 $3\times 3\times 3$ filter로 convolution해서 $4\times 4\times 7$인 feature map이 나왔다고 하자. 이 경우 filter_height=3, filter_width=3, in_channels=3, out_channels=7이므로, 파라미터의 수는 $3\times 3\times 3 \times 7=189$이다.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html">CNN이란? CNN은 아래 세 가지 요소로 이루어져 있다.</summary></entry><entry><title type="html">최대우도추정(MLE, Maximum Likelihood Estimation)</title><link href="http://localhost:4000/posts/2020/05/03/maximum_likelihood_estimation" rel="alternate" type="text/html" title="최대우도추정(MLE, Maximum Likelihood Estimation)" /><published>2020-05-03T00:00:00-07:00</published><updated>2020-05-03T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/03/maximum_likelihood_estimation</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/03/maximum_likelihood_estimation">&lt;iframe src=&quot;https://www.youtube.com/embed/XepXtl9YKwc&quot;&gt; &lt;/iframe&gt;

&lt;p&gt;최대우도추정 : 데이터를 가장 잘 설명하는 모델의 모수($\theta$)를 찾는 것&lt;/p&gt;

&lt;p&gt;관측된 데이터 $X_1=x_1$, $X_2=x_2$, …, $X_n=x_n$이 있다고 할 때 $\theta$의 가능도는 다음과 같다.&lt;/p&gt;

&lt;p&gt;$L(\theta)=P(x_1, x_2, …, x_n|\theta)$&lt;/p&gt;

&lt;p&gt;(참고로 여기서 나오는 $P(x_1, x_2, …, x_n|\theta)$에서 $|$는 조건부 확률을 나타내는 기호가 아니다. $|$ 뒤에 있는 기호들이 모델의 모수라는 것을 강조하기 위해 사용되는 기호에 불과하다. $P(x_1, x_2, …, x_n;\theta)$으로 표현하기도 한다.)&lt;/p&gt;

&lt;p&gt;이 때 확률변수가 iid(independent and identically distributed)라면 가능도는 다음과 같이 표현할 수도 있다.&lt;/p&gt;

&lt;p&gt;$L(\theta)=\prod_{i=1}^{n}P(x_i|\theta)$&lt;/p&gt;

&lt;p&gt;가능도를 최대로 만드는 $\theta_{MLE}$는 다음과 같다.&lt;/p&gt;

&lt;p&gt;$\theta_{MLE}=argmax_{\theta}\prod_{i=1}^nP(x_i|\theta)$&lt;/p&gt;

&lt;p&gt;이 때 확률값은 1보다 작기 때문에 계속 곱하면 그 값이 지나치게 작아져 언더플로우(underflow) 문제가 발생하므로 로그를 취한다. 이 때 로그를 취하더라도 가능도를 최대로 만드는 $\theta$가 전과 동일하다는 보장이 있어야 하는데, 로그는 단조증가(monotonically increasing)한다는 성질이 있기 때문에 로그를 취하더라도 가능도를 최대로 만드는 $\theta$는 변하지 않는다.&lt;/p&gt;

&lt;p&gt;$\theta_{MLE}=argmax_{\theta}\sum_{i=1}^nlogP(x_i|\theta)$&lt;/p&gt;

&lt;h2 id=&quot;예시&quot;&gt;예시&lt;/h2&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/Dn6b9fCIUpM&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XepXtl9YKwc&quot;&gt;StatQuest: Maximum Likelihood, clearly explained!!!&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=Dn6b9fCIUpM&quot;&gt;Maximum Likelihood For the Normal Distribution, step-by-step!&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://ratsgo.github.io/statistics/2017/09/23/MLE/&quot;&gt;최대우도추정(Maximum Likelihood Estimation)&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry><entry><title type="html">확률(Probability)과 가능도(Likelihood)의 차이</title><link href="http://localhost:4000/posts/2020/05/02/probability_likelihood" rel="alternate" type="text/html" title="확률(Probability)과 가능도(Likelihood)의 차이" /><published>2020-05-02T00:00:00-07:00</published><updated>2020-05-02T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/05/02/probability_likelihood</id><content type="html" xml:base="http://localhost:4000/posts/2020/05/02/probability_likelihood">&lt;iframe src=&quot;https://www.youtube.com/embed/pYxNSUDSFH4&quot;&gt; &lt;/iframe&gt;

&lt;p&gt;확률과 가능도의 차이를 가장 직관적으로 설명한 영상이라고 생각한다. 한번 보면 감이 올 것이다.&lt;/p&gt;

&lt;p&gt;영상에서는 확률과 가능도를 다음과 같이 요약한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26649034/80852722-b4230180-8c65-11ea-9d9f-922949fad27c.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;내-나름의-정리&quot;&gt;내 나름의 정리&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;확률 : 분포를 알고 있을 때, 전체 관측값에서 우리가 관심있는 관측값이 차지하는 비중&lt;/li&gt;
  &lt;li&gt;가능도 : 분포를 모를 때, 분포 $\theta$가 관측값을 얼마나 잘 설명하는지 수치화한 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw&quot;&gt;StatQuest with Josh Starmer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability&quot;&gt;What is the difference between “likelihood” and “probability”?
&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;employer&quot;=&gt;nil, &quot;pubmed&quot;=&gt;nil, &quot;googlescholar&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;blanik00@gmail.com&quot;, &quot;researchgate&quot;=&gt;nil, &quot;uri&quot;=&gt;nil, &quot;bitbucket&quot;=&gt;nil, &quot;codepen&quot;=&gt;nil, &quot;dribbble&quot;=&gt;nil, &quot;flickr&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;foursquare&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;google_plus&quot;=&gt;nil, &quot;keybase&quot;=&gt;nil, &quot;instagram&quot;=&gt;nil, &quot;impactstory&quot;=&gt;nil, &quot;lastfm&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;orcid&quot;=&gt;nil, &quot;pinterest&quot;=&gt;nil, &quot;soundcloud&quot;=&gt;nil, &quot;stackoverflow&quot;=&gt;nil, &quot;steam&quot;=&gt;nil, &quot;tumblr&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;vine&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;xing&quot;=&gt;nil, &quot;youtube&quot;=&gt;nil, &quot;wikipedia&quot;=&gt;nil}</name><email>blanik00@gmail.com</email></author><category term="[&quot;posts&quot;]" /><summary type="html"></summary></entry></feed>